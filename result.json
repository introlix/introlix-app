{
  "url": "https://arxiv.org/pdf/2506.12594",
  "text": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and\nApplications\nRENJUN XU∗ and JINGWEN PENG, Zhejiang University, China\nThissurveyexaminestherapidlyevolvingfieldofDeepResearchsystems—AI-poweredapplicationsthatautomate\ncomplexresearchworkflowsthroughtheintegrationoflargelanguagemodels,advancedinformationretrieval,and\nautonomousreasoningcapabilities.Weanalyzemorethan80commercialandnon-commercialimplementationsthat\nhave emerged since 2023, including OpenAI/DeepResearch, Gemini/DeepResearch, Perplexity/DeepResearch, and\nnumerousopen-sourcealternatives.Throughcomprehensiveexamination,weproposeanovelhierarchicaltaxonomy\nthat categorizes systems according to four fundamental technical dimensions: foundation models and reasoning\nengines,toolutilizationandenvironmentalinteraction,taskplanningandexecutioncontrol,andknowledgesynthesis\nand output generation. We explore the architectural patterns, implementation approaches, and domain-specific\nadaptationsthatcharacterizethesesystemsacrossacademic,scientific,business,andeducationalapplications.Our\nanalysisrevealsboththesignificantcapabilitiesofcurrentimplementationsandthetechnicalandethicalchallenges\ntheypresentregardinginformationaccuracy,privacy,intellectualproperty,andaccessibility.Thesurveyconcludes\nbyidentifyingpromisingresearchdirectionsinadvancedreasoningarchitectures,multimodalintegration,domain\nspecialization,human-AIcollaboration,andecosystemstandardizationthatwilllikelyshapethefutureevolutionof\nthistransformativetechnology.ByprovidingacomprehensiveframeworkforunderstandingDeepResearchsystems,\nthissurveycontributestoboththetheoreticalunderstandingofAI-augmentedknowledgeworkandthepractical\ndevelopmentofmorecapable,responsible,andaccessibleresearchtechnologies.Thepaperresourcescanbeviewedat\nhttps://github.com/scienceaix/deepresearch.\nCCSConcepts:•Computingmethodologies→Artificialintelligence;Natural language processing;•Computersystems\norganization→Embeddedandcyber-physicalsystems;•Informationsystems→Informationretrieval;•Human-centered\ncomputing→Collaborativeandsocialcomputing.\nAdditional Key Words and Phrases: Deep Research, Large Language Models, Autonomous Agents, AI Systems,\nResearchAutomation,InformationRetrieval,KnowledgeSynthesis,Human-AICollaboration,Multi-AgentSystems,\nTool-UsingAgents\n∗Correspondingauthor:rux@zju.edu.cn\nAuthors’ContactInformation:RenjunXu;JingwenPeng,ZhejiangUniversity,China.\n1\n5202\nnuJ\n41\n]IA.sc[\n1v49521.6052:viXra\n2 Xu et al.\nContents\nAbstract 1\nContents 2\n1 Introduction 4\n1.1 Definition and Scope of Deep Research 4\n1.2 Historical Context and Technical Evolution 5\n1.3 Significance and Practical Implications 6\n1.4 Research Questions and Contribution of this Survey 7\n2 The Evolution and Technical Framework of Deep Research 7\n2.1 Foundation Models and Reasoning Engines: Evolution and Advances 7\n2.2 Tool Utilization and Environmental Interaction: Evolution and Advances 10\n2.3 Task Planning and Execution Control: Evolution and Advances 11\n2.4 Knowledge Synthesis and Output Generation: Evolution and Advances 13\n3 Comparative Analysis and Evaluation of Deep Research Systems 14\n3.1 Cross-Dimensional Technical Comparison 14\n3.2 Application-Based System Suitability Analysis 16\n3.3 Performance Metrics and Benchmarking 18\n4 Implementation Technologies and Challenges 21\n4.1 Architectural Implementation Patterns 21\n4.2 Infrastructure and Computational Optimization 27\n4.3 System Integration and Interoperability 29\n4.4 Technical Challenges and Solutions 33\n5 Evaluation Methodologies and Benchmarks 35\n5.1 Functional Evaluation Frameworks 35\n5.2 Non-Functional Evaluation Metrics 37\n5.3 Cross-Domain Evaluation Benchmarks 39\n5.4 Emerging Evaluation Approaches 41\n5.5 Comparative Evaluation Methodology 42\n6 Applications and Use Cases 44\n6.1 Academic Research Applications 44\n6.2 Scientific Discovery Applications 46\n6.3 Business Intelligence Applications 49\n6.4 Financial Analysis Applications 51\n6.5 Educational Applications 52\n6.6 Personal Knowledge Management Applications 54\n7 Ethical Considerations and Limitations 56\n7.1 Information Accuracy and Hallucination Concerns 56\n7.2 Privacy and Data Security 59\n7.3 Source Attribution and Intellectual Property 61\n7.4 Accessibility and Digital Divide 63\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 3\n8 Future Research Directions 64\n8.1 Advanced Reasoning Architectures 64\n8.2 Multi-Modal Deep Research 68\n8.3 Domain-Specific Optimization 70\n8.4 Human-AI Collaboration and Standardization 72\n9 Conclusion 76\n9.1 Key Findings and Contributions 76\n9.2 Limitations and Outlook 78\n9.3 Broader Implications 79\n9.4 Final Thoughts 80\nReferences 81\n4 Xu et al.\n1 Introduction\nRapidadvancementofartificialintelligencehasprecipitatedaparadigmshiftinhowknowledgeisdiscovered,\nvalidated, and utilized across academic and industrial domains. Traditional research methodologies, reliant\non manual literature reviews, experimental design, and data analysis, are increasingly supplemented—and\ninsomecasessupplanted—byintelligentsystemscapableofautomatingend-to-endresearchworkflows.This\nevolutionhasgivenrisetoanoveldomainweterm“DeepResearch”,whichsignifiestheconvergenceoflarge\nlanguage models (LLMs), advanced information retrieval systems, and automated reasoning frameworks to\nredefine the boundaries of scholarly inquiry and practical problem-solving.\n1.1 Definition and Scope of Deep Research\nDeep Research refers to the systematic application of AI technologies to automate and enhance research\nprocesses through three core dimensions:\n(1) Intelligent Knowledge Discovery: Automating literature search, hypothesis generation, and pattern\nrecognition across heterogeneous data sources\n(2) End-to-EndWorkflowAutomation: Integrating experimental design, data collection, analysis, and\nresult interpretation into unified AI-driven pipelines\n(3) CollaborativeIntelligenceEnhancement: Facilitating human-AI collaboration through natural lan-\nguage interfaces, visualizations, and dynamic knowledge representation\nTo clearly delineate the boundaries of Deep Research, we distinguish it from adjacent AI systems as\nfollows:\n∙ Differentiating from General AI Assistants: While general AI assistants like ChatGPT can answer\nresearch questions, they lack the autonomous workflow capabilities, specialized research tools, and\nend-to-endresearchorchestrationthatdefineDeepResearchsystems.Recentsurveyshavehighlighted\nthis crucial distinction between specialized research systems and general AI capabilities [73, 76],\nwith particular emphasis on how domain-specific tools fundamentally transform research workflows\ncompared to general-purpose assistants [213, 318].\n∙ DifferentiatingfromSingle-FunctionResearchTools: Specialized tools like citation managers, liter-\nature search engines, or statistical analysis packages address isolated research functions but lack\nthe integrated reasoning and cross-functional orchestration of Deep Research systems. Tools like\nscispace [242] and You.com [313] represent earlier attempts at research assistance but lack the\nend-to-end capabilities that define true Deep Research systems.\n∙ Differentiating from Pure LLM Applications: Applications that simply wrap LLMs with research-\noriented prompts lack the environmental interaction, tool integration, and workflow automation\ncapabilities that characterize true Deep Research systems.\nThis survey specifically examines systems that exhibit at least two of the three core dimensions, with\na focus on those incorporating large language models as their foundational reasoning engine. Our scope\nencompasses commercial offerings such as OpenAI/DeepResearch [197], Google’s Gemini/DeepResearch\n[89], and Perplexity/DeepResearch [209], alongside open-source implementations including dzhng/deep-\nresearch[321],HKUDS/Auto-Deep-Research[112],andnumerousothersdetailedinsubsequentsections.We\nexcludepurelybibliometrictoolsorsingle-stageautomationsystemslackingintegratedcognitivecapabilities,\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 5\nsuch as research assistance tools like Elicit [74], ResearchRabbit [228], Consensus [63], or citation tools\nlike Scite [243]. Additional specialized tools like STORM [278], which focuses on scientific text retrieval and\norganization, are valuable but lack the end-to-end deep research capabilities central to our survey scope.\n1.2 Historical Context and Technical Evolution\nThe trajectory of Deep Research can be mapped through three evolutionary stages that reflect both\ntechnological advancements and implementation approaches:\n1.2.1 Origin and Early Exploration (2023 - February 2025). It should be noted that workflow automation\nframeworks like n8n [183], QwenLM/Qwen-Agent [224], etc. had already been in existence long before the\nboom of deep research. Their early establishment demonstrated the pre-existing groundwork in related\ntechnologicaldomains, highlightingthat the developmentlandscape wasnot solelyshaped bythe emergence\nof deep research, but had a more diverse and earlier-rooted origin. The concept of Deep Research emerged\nfrom the shift of AI assistants towards intelligent agents. In December 2024, Google Gemini pioneered\nthis functionality with its initial Deep Research implementation, focusing on basic multi-step reasoning\nand knowledge integration [60]. This phase laid the groundwork for subsequent advancements, setting the\nstage for more sophisticated AI-driven research tools. Many of these advances built upon earlier workflow\nautomation tools like n8n [183] and agent frameworks such as AutoGPT [250] and BabyAGI [311] that had\nalready established foundations for autonomous task execution. Other early contributions to this ecosystem\ninclude cline2024 [61], which pioneered integrated research workflows, and open_operator [36], which\ndeveloped foundational browser automation capabilities essential for web-based research.\n1.2.2 Technological Breakthrough and Competitive Rivalry (February - March 2025). The rise of DeepSeek’s\nopen-source models [68] revolutionized the market with efficient reasoning and cost-effective solutions. In\nFebruary2025,OpenAI’sreleaseofDeepResearch,markedasignificantleapforward[197].Poweredbytheo3\nmodel, it demonstrated advanced capabilities such as autonomous research planning, cross-domain analysis,\nandhigh-qualityreportgeneration,achievingaccuracyratesexceedingpreviousbenchmarksincomplextasks.\nConcurrently, Perplexity launched its free-to-use Deep Research in February 2025 [209], emphasizing rapid\nresponseandaccessibilitytocapturethemassmarket.Open-sourceprojectssuchasnickscamara/open-deep-\nresearch[42],mshumer/OpenDeepResearcher[249],btahir_open_deep_research[37],andGPT-researcher\n[16]emergedascommunity-drivenalternativestocommercialplatforms.Theecosystemcontinuedtoexpand\nwith lightweight implementations like Automated-AI-Web-Researcher-Ollama [267], designed for local\nexecution with limited resources, and modular frameworks such as Langchain-AI/Open_deep_research\n[131] that provided composable components for custom research workflows.\n1.2.3 Ecosystem Expansion and Multi-modal Integration (March 2025 - Present). The third stage is character-\nizedbythematurationofadiverseecosystem.Open-sourceprojectslikeJina-AI/node-DeepResearch[121]\nenable localized deployment and customization, while commercial closed-source versions from OpenAI and\nGoogle continue to push boundaries with multi-modal support and multi-agent collaboration capabilities.\nTheintegrationofadvancedsearchtechnologiesandreportgenerationframeworksfurtherenhancesthetool’s\nutility across academic research, financial analysis, and other fields. Meanwhile, platforms like Manus [164]\nand AutoGLM-Research [330], MGX [171], and Devin [62] are incorporating advanced AI research capabilities\n6 Xu et al.\ntoenhancetheirservices.Concurrently,AnthropiclaunchedClaude/Research[13]inApril2025,introducing\nagentic search capabilities that systematically explore multiple angles of queries and deliver comprehensive\nanswers with verifiable citations. Agent frameworks such as OpenManus [193], Camel-AI/OWL [43], and TARS\n[39] further expand the ecosystem with specialized capabilities and domain-specific optimizations.\nEvolution Timeline of Deep Research Systems (2024-2025)\nOrigin and Early Exploration Technological Breakthrough Ecosystem Expansion\nEarly prototypes and foundational approaches Commercial releases and competitive rivalry Multi-modal integration and diverse applications\n(2023 - February 2025) (February - March 2025) (March 2025 - Present)\nJan 2025 Feb 2025 Mar 2025 Mar 2025\nDeepSeek nickscamara/ Camel-AI/OWL TARS\nApr 2024 Model open-deep-research\nQwenLM/\nQwen-Agent Feb 2025 Feb 2025\nmshumer/ Jina AI/ Mar 2025\n2023 OpenDeepResearcher node-Deep Research OpenManus\nn8n\nGoogle Gemini Perplexity\nDeep Research Deep Research AutoGLM-Research\nFeb 2025 Mar 2025\nDec 2024\nOpenAI\nDeep Research Manus\nFeb 2025 Mar 2025\nCommercial Systems\nOpen-source Systems\nFig.1. EvolutionTimelineofDeepResearchSystems\n1.3 Significance and Practical Implications\nDeep Research demonstrates transformative potential across multiple domains:\n(1) Academic Innovation: Accelerating hypothesis validation through automated literature synthesis\n(e.g., HotpotQA [307] performance benchmarks) and enabling researchers to explore broader inter-\ndisciplinary connections that might otherwise remain undiscovered. The transformative potential of\nDeep Research extends beyond individual applications to fundamentally reshape scientific discovery\nprocesses. As Sourati and Evans [256] argue, human-aware artificial intelligence can significantly\naccelerate science by augmenting researchers’ capabilities while adapting to their conceptual frame-\nworks and methodological approaches. This human-AI synergy represents a fundamental shift from\ntraditional automation toward collaborative intelligence that respects and enhances human scientific\nintuition.ComplementaryworkbyKhaliliandBouchachia[128]furtherdemonstrateshowsystematic\napproachestobuildingsciencediscoverymachinescantransformhypothesisgeneration,experimental\ndesign, and theory refinement through integrated AI-driven research workflows.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 7\n(2) Enterprise Transformation: Enabling data-driven decision-making at scale through systems like\nAgent-RL/ReSearch[2]andsmolagents/open_deep_research[115]thatcananalyzemarkettrends,\ncompetitive landscapes, and strategic opportunities with unprecedented depth and efficiency.\n(3) DemocratizationofKnowledge:Reducingbarrierstoentrythroughopen-sourceimplementationslike\ngrapeot/deep_research_agent[263]andOpenManus[193],makingsophisticatedresearchcapabilities\naccessible to individuals and organizations regardless of technical expertise or resource constraints.\n1.4 Research Questions and Contribution of this Survey\nThis survey addresses three fundamental questions:\n(1) Howdoarchitecturalchoices(systemarchitecture,implementationapproach,functionalcapabilities)\nimpact Deep Research effectiveness?\n(2) What technical innovations have emerged in LLM fine-tuning, retrieval mechanisms, and workflow\norchestration across the spectrum of Deep Research implementations?\n(3) How do existing systems balance performance, usability, and ethical considerations, and what\npatterns emerge from comparing approaches like those of n8n [183] and OpenAI/AgentsSDK [199]?\nOur contributions manifest in three dimensions:\n(1) Methodological: Proposing a novel taxonomy categorizing systems by their technical architecture,\nfrom foundation models to knowledge synthesis capabilities\n(2) Analytical: Conducting comparative analysis of representative systems across evaluation metrics,\nhighlighting the strengths and limitations of different approaches\n(3) Practical:Identifyingkeychallengesandformulatingaroadmapforfuturedevelopment,withspecific\nattention to emerging architectures and integration opportunities\nThe remainder of this paper follows a structured exploration beginning with conceptual frameworks\n(Section 2), technical innovations and comparative analysis (Sections 3-4), implementation technologies\n(Section5),evaluationmethodologies(Section6),applicationsandusecases(Section7),ethicalconsiderations\n(Section 8), and future directions (Section 9).\n2 The Evolution and Technical Framework of Deep Research\nThis section presents a comprehensive technical taxonomy for understanding Deep Research systems,\norganized around four fundamental technological capabilities that define these systems. For each capabil-\nity, we examine the evolutionary trajectory and technical innovations while highlighting representative\nimplementations that exemplify each approach.\n2.1 Foundation Models and Reasoning Engines: Evolution and Advances\nThe foundation of Deep Research systems lies in their underlying AI models and reasoning capabilities,\nwhich have evolved from general-purpose language models to specialized research-oriented architectures.\n2.1.1 From General-Purpose LLMs to Specialized Research Models. The progression from general LLMs to\nresearch-specialized models represents a fundamental shift in deep research capabilities:\n8 Xu et al.\nHierarchical Technical Framework of Deep Research Systems\nFoundation Models and Tool Utilization and\nReasoning Engines Environmental Interaction\n• General-purpose to specialized models • Web navigation and interaction\n• Context handling and memory • Content processing technologies\n• Chain-of-thought reasoning • API integration patterns\n• Tree-of-thought architectures • Domain-specific tool usage\nGeneral LLMs Specialized Web Interaction API Integration\n(GPT-4, Gemini) (o3, DeepSeek-R1) (Nanobrowser) (n8n, Manus)\nDeep\nResearch\nPlanning Execution Evaluation Generation\n(OpenAI SDK) (Agent-RL) (grapeot) (mshumer)\nTask Planning and Knowledge Synthesis and\nExecution Control Output Generation\n• Research task decomposition • Information evaluation\n• Hierarchical planning methods • Source verification\n• Autonomous execution monitoring • Structured report generation\n• Multi-agent collaboration • Interactive presentation\nFig.2. HierarchicalTechnicalFrameworkofDeepResearchSystems\nTechnical Evolution Trajectory. Early implementations relied on general-purpose LLMs with minimal\ntask-specific optimization. Current systems feature models specifically enhanced for research tasks through\narchitectural modifications, specialized training corpora, and fine-tuning regimes focused on analytical and\nreasoning capabilities. The transition from models like GPT-4 to OpenAI’s o3 demonstrates significant\nimprovements in abstraction, multi-step reasoning, and knowledge integration capabilities essential for\ncomplex research tasks [198, 200].\nRepresentative Systems. OpenAI/DeepResearch [197] exemplifies this evolution with its o3-based model\noptimizedspecificallyforwebbrowsinganddataanalysis.Thesystemleverageschain-of-thoughtandtree-of-\nthought reasoning techniques to navigate complex information landscapes. Google’s Gemini/DeepResearch\n[60] similarly employs Gemini 2.5 Pro with enhanced reasoning capabilities and a million-token context\nwindow to process extensive information. These approaches build upon foundational work in reasoning\nenhancement techniques like chain-of-thought prompting [291], self-consistency [287], and human preference\nalignment [205] that have been adapted specifically for research-intensive tasks. In the open-source domain,\nAutoGLM-Research [330] demonstrates how specialized training regimes can optimize existing models like\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 9\nChatGLMforresearch-intensivetasks,achievingsignificantperformancegainsthroughtargetedenhancements\nto reasoning components.\n2.1.2 Context Understanding and Memory Mechanisms. The ability to process, retain, and utilize extensive\ncontextual information represents a crucial advancement in Deep Research systems:\nTechnical Evolution Trajectory. Early systems struggled with limited context windows, hampering their\nabilitytosynthesizeinformationfrommultiplesources.Contemporaryimplementationsemploysophisticated\nmemory management techniques including episodic buffers, hierarchical compression, and attention-based\nretrieval mechanisms that extend effective context far beyond model limitations. The million-token context\nwindows of models like Grok 3 [299] and Gemini 2.5 Pro [60], along with the context optimization in\nOpenAI’s o3 model [195], have dramatically expanded the information processing capabilities of these\nsystems. Advanced systems now distinguish between working memory (active reasoning context) and\nlong-term memory (knowledge repository), allowing for more human-like research processes.\nRepresentative Systems. Perplexity/DeepResearch [209] has pioneered efficient context processing by\nleveragingDeepSeek-R1’scapabilitieswhileimplementingproprietarymechanismsforstructuredinformation\nmanagement. The system can analyze hundreds of sources while maintaining coherent reasoning threads.\nSimilarly,Camel-AI/OWL[43]employsaninnovativeopen-weightapproachtomemorymanagement,allowing\nfor dynamic allocation of attention resources based on information relevance and task requirements. Both\nsystems demonstrate how effective memory architectures can significantly enhance research performance\neven with comparable base model capabilities.\n2.1.3 Enhancements in Reasoning Capabilities. Advanced reasoning mechanisms distinguish modern Deep\nResearch systems from conventional LLM applications:\nTechnicalEvolutionTrajectory. Earlyimplementationsreliedprimarilyonzero-shotorfew-shotprompting\nfor reasoning tasks. Current systems integrate explicit reasoning frameworks including chain-of-thought,\ntree-of-thought,andgraph-basedreasoningarchitectures.RecentworkbyLangetal.[132]demonstrateshow\ndebate-driven reasoning can facilitate weak-to-strong generalization, enabling more robust performance on\ncomplex research tasks through structured argumentative processes. These approaches implement reasoning\npatterns that more closely mirror human scientific discourse, with explicit representation of alternative\nviewpoints and structured evaluation of competing hypotheses. Advanced implementations like OpenAI’s\no3 incorporate self-critique, uncertainty estimation, and recursive reasoning refinement [198, 200]. This\nevolution enables increasingly sophisticated forms of evidence evaluation, hypothesis testing, and knowledge\nsynthesis essential for high-quality research outputs.\nRepresentative Systems. QwenLM/Qwen-Agent[224]exemplifiesadvancedreasoningcapabilitiesthroughits\nspecializedtoolkitintegrationandmodularreasoningframework.Thesystememploysamulti-stagereasoning\nprocess with explicit planning, information gathering, analysis, and synthesis phases optimized for research\nworkflows. Similar capabilities are evident in smolagents/open_deep_research [115], which implements a\nflexible reasoning architecture that can adapt to different research domains and methodologies. Systems like\nCycleResearcher [294] demonstrate how integrating automated review processes into research workflows can\nenhance accuracy through structured feedback loops. These approaches implement explicit verification steps\n10 Xu et al.\nthatidentifypotentialerrorsandinconsistenciesbeforegeneratingfinalresearchoutputs.Theapplicationof\nAI to complex domains like mathematics further illustrates this progress, where models are increasingly\nviewed from a cognitive science perspective to enhance their reasoning abilities [320], achieving notable\nmilestones such as silver-medal standards in solving International Mathematical Olympiad problems [7].\nThesesystemshighlighthowreasoningenhancementscandramaticallyimproveresearchqualityevenwithout\nrequiring the largest or most computationally intensive base models.\n2.2 Tool Utilization and Environmental Interaction: Evolution and Advances\nDeepResearchsystemsmusteffectivelyinteractwithexternalenvironmentstogatherandprocessinformation,\nrepresenting a fundamental capability beyond core language model functions[144].\n2.2.1 Web Interaction Technology Development. The ability to navigate and extract information from the\nweb represents a foundational capability for deep research:\nTechnical Evolution Trajectory. Initial implementations relied on simple API-based search queries with\nlimited interaction capabilities. Current systems employ sophisticated web navigation including dynamic\ncontenthandling,authenticationmanagement,andinteractiveelementmanipulation.Advancedimplementa-\ntions feature semantic understanding of web structures, allowing for adaptive information extraction and\nmulti-page navigation flows. This evolution has dramatically expanded access to web-based information\nsources and the ability to extract insights from complex web environments.\nRepresentative Systems. Nanobrowser [184] represents a purpose-built browser environment designed\nspecifically for AI agent use, offering optimized rendering and interaction capabilities for research tasks.\nIt enables fine-grained control of web navigation while maintaining security and performance. Similarly,\nAutoGLM[330]demonstratessophisticatedGUIinteractioncapabilitiesacrossbothwebandmobileinterfaces,\nallowing it to access information through interfaces designed for human use. These systems showcase how\nspecialized web interaction technologies can significantly expand the information gathering capabilities of\nDeep Research systems.\n2.2.2 Content Processing Technology Advancements. Beyond basic navigation, the ability to process diverse\ncontent formats is crucial for comprehensive research:\nTechnical Evolution Trajectory. Early systems were limited primarily to text extraction from HTML\nsources. Modern implementations support multi-modal content processing including structured data tables,\nembedded visualizations, PDF documents, and interactive applications. Advanced systems like those built\non OpenAI’s o3 can extract semantic structure from unstructured content, identify key information from\ndiverseformats,andintegrateinsightsacrossmodalities[201].Thisevolutionhasdramaticallyexpandedthe\nrange of information sources that can be incorporated into research processes.\nRepresentative Systems. Thedzhng/deep-research[321]projectexemplifiesadvancedcontentprocessing\nthrough its specialized modules for different document types and formats. It implements custom extraction\nlogic for academic papers, technical documentation, and structured data sources. Similarly, nickscamara/\nopen-deep-research [42] features sophisticated content normalization pipelines that transform diverse\nformats into consistent knowledge representations suitable for analysis. Both systems demonstrate how\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 11\nspecialized content processing can significantly enhance the quality and comprehensiveness of research\noutputs.\n2.2.3 Specialized Tool Integration Progress. Integration with domain-specific tools extends Deep Research\ncapabilities beyond general information processing:\nTechnical Evolution Trajectory. Initial systems relied on general-purpose web search and basic API\nintegrations. The integration of diverse tools has been dramatically advanced by frameworks like ToolLLM\n[222], which enables large language models to master over 16,000 real-world APIs, significantly expanding\nthe interaction capabilities of research systems. Similarly, AssistGPT [82] demonstrates how general multi-\nmodal assistants can plan, execute, inspect, and learn across diverse environments, creating unified research\nexperiences that seamlessly incorporate varied information sources and interaction modalities. LLaVA-Plus\n[152]furtherextendsthesecapabilitiesthroughexplicittoollearningmechanisms,enablingresearchassistants\nto adaptively incorporate specialized tools within multimodal workflows. Current implementations feature\ncomplex toolchains including specialized databases, analytical frameworks, and domain-specific services.\nAdvanced systems dynamically select and orchestrate tools based on research requirements, effectively\ncomposingcustomresearchworkflowsfromavailablecapabilities.Someimplementationslikethoseleveraging\nOpenAI’sCodex[194]canevengeneratecustomcodetoprocessresearchdataorimplementanalyticalmodels\non demand, further extending analytical capabilities. This evolution has enabled increasingly sophisticated\nanalysis and domain-specific research applications.\nRepresentativeSystems. Manus[164]exemplifiessophisticatedtoolorchestrationthroughitsextensiveAPI\nintegration framework and tool selection mechanisms. The system can incorporate domain-specific research\ntools and services into unified workflows, significantly expanding its analytical capabilities. Similarly, n8n\n[183]providesaflexibleworkflowautomationplatformthatcanbeconfiguredforresearchtasks,allowingfor\nintegrationwithspecializeddatasourcesandanalyticalservices.Stewardextendswebinteractioncapabilities\nby implementing natural language-driven navigation and operation across websites, overcoming scalability\nlimitations of traditional automation frameworks while maintaining low operational costs [261]. These\nsystems highlight how tool integration can extend Deep Research capabilities into specialized domains and\ncomplex analytical workflows.\n2.3 Task Planning and Execution Control: Evolution and Advances\nEffective research requires sophisticated planning and execution mechanisms to coordinate complex, multi-\nstage workflows.\n2.3.1 Research Task Planning Development. The ability to decompose research objectives into manageable\ntasks represents a fundamental advancement:\nTechnical Evolution Trajectory. Early approaches employed simple task decomposition with linear ex-\necution flows, similar to those found in early agent frameworks like MetaGPT [111] and AgentGPT [230].\nModernsystemsimplementhierarchicalplanningwithdynamicrefinementbasedonintermediateresultsand\ndiscoveries. Advanced planning approaches increasingly incorporate structured exploration methodologies\nto navigate complex solution spaces efficiently. AIDE [120] demonstrates how tree search algorithms can\n12 Xu et al.\neffectively explore the space of potential code solutions for machine learning engineering, trading computa-\ntional resources for enhanced performance through strategic reuse and refinement of promising pathways.\nAdvancedimplementationsincorporateresource-awareplanning,consideringtimeconstraints,computational\nlimitations, and information availability. However, incorporating AI tools for tasks like automated code\nreview has been observed to increase pull request closure durations despite benefits, as evidenced in studies\nsuchasCihanetal.[59],highlightingthecriticalneedtoaccountfortemporalimpactsinsuchresource-aware\nsystems. This evolution has enabled increasingly sophisticated research strategies adaptive to both task\nrequirements and available resources.\nRepresentative Systems. The OpenAI/AgentsSDK [199] provides a comprehensive framework for research\ntask planning, with explicit support for goal decomposition, execution tracking, and adaptive refinement.\nIt enables the development of applications with sophisticated planning capabilities for research workflows.\nSimilarly, Flowith/OracleMode [77] implements specialized planning mechanisms optimized for research\ntasks, with particular emphasis on information quality assessment and source prioritization. These systems\ndemonstratehowadvancedplanningcapabilitiescansignificantlyimproveresearchefficiencyandeffectiveness.\n2.3.2 AutonomousExecutionandMonitoringAdvances. Reliableexecutionofresearchplansrequiressophisti-\ncated control and monitoring mechanisms:\nTechnical Evolution Trajectory. Initial systems employed basic sequential execution with limited error\nhandling. Current implementations feature concurrent execution paths, comprehensive monitoring, and\ndynamicresponsetoexecutionchallenges.Advancedsystemsimplementself-supervisionwithexplicitsuccess\ncriteria, failure detection, and autonomous recovery strategies. This evolution has dramatically improved\nthe reliability and autonomy of Deep Research systems across complex tasks.\nRepresentative Systems. Agent-RL/ReSearch [2] exemplifies advanced execution control through its rein-\nforcement learning-based approach to research execution. The system learns effective execution strategies\nfromexperience,continuouslyimprovingitsabilitytonavigatecomplexresearchworkflows.Itsadaptiveexe-\ncutionmechanismscanrecoverfromfailuresandadjuststrategiesbasedonintermediateresults,highlighting\nhow sophisticated control mechanisms can enhance research reliability and effectiveness.\n2.3.3 Multi-Agent Collaboration Framework Development. Complex research often benefits from specialized\nagent roles and collaborative approaches:\nTechnical Evolution Trajectory. Early systems relied on monolithic agents with undifferentiated capa-\nbilities. Modern implementations employ specialized agent roles with explicit coordination mechanisms\nand information sharing protocols. Advanced systems feature dynamic role allocation, consensus-building\nmechanisms,andsophisticatedconflictresolutionstrategies.Thisevolutionhasenabledincreasinglycomplex\ncollaborative research workflows and improved performance on challenging tasks[49]. For instance, frame-\nworks employing multi-agent debate have been shown to improve evaluation consistency [48], while research\ninto generative AI voting demonstrates resilience to model biases in collective decision-making [162].\nRepresentative Systems. The smolagents/open_deep_research [115] framework demonstrates effective\nmulti-agent collaboration through its modular agent architecture and explicit coordination mechanisms. It\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 13\nenablesthecompositionofspecializedresearchteamswithcomplementarycapabilitiesandsharedobjectives.\nSimilarly,TARS[39]implementsasophisticatedagentcollaborationframeworkwithinitsdesktopenvironment,\nallowing multiple specialized agents to contribute to unified research workflows. These systems highlight\nhow multi-agent approaches can enhance research capabilities through specialization and collaboration.\n2.4 Knowledge Synthesis and Output Generation: Evolution and Advances\nThe ultimate value of Deep Research systems lies in their ability to synthesize disparate information into\ncoherent, actionable insights.\n2.4.1 Information Evaluation Technology Development. Critical assessment of information quality represents\na crucial capability for reliable research:\nTechnicalEvolutionTrajectory. Earlysystemsreliedprimarilyonsourcereputationheuristicswithlimited\ncontent-based assessment. Modern implementations employ sophisticated evaluation frameworks considering\nsource characteristics, content features, and consistency with established knowledge. Advanced systems\nimplement explicit uncertainty modeling, contradiction detection, and evidential reasoning approaches. This\nevolution has dramatically improved the reliability and trustworthiness of research outputs. Advances in\nknowledge retrieval based on generative AI enhance the ability to source and verify information [306].\nRepresentative Systems. The grapeot/deep_research_agent [263] implements sophisticated information\nevaluation mechanisms with explicit quality scoring for diverse source types. It can assess information\nreliability based on both intrinsic content features and extrinsic source characteristics, enabling more\ndiscerning information utilization. These capabilities highlight how advanced evaluation mechanisms can\nsignificantly enhance research quality and reliability.\n2.4.2 Report Generation Technology Advances. Effective communication of research findings requires sophis-\nticated content organization and presentation:\nTechnical Evolution Trajectory. Initial systems produced simple text summaries with limited structure or\ncoherence.Currentimplementationsgeneratecomprehensivereportswithhierarchicalorganization,evidence\nintegration, and coherent argumentation. Advanced systems produce adaptive outputs tailored to audience\nexpertise, information needs, and presentation contexts. This evolution has dramatically improved the\nusability and impact of Deep Research outputs.\nRepresentative Systems. The mshumer/OpenDeepResearcher [249] project exemplifies advanced report\ngeneration through its structured output framework and evidence integration mechanisms. It produces\ncomprehensive research reports with explicit attribution, structured arguments, and integrated supporting\nevidence. These capabilities demonstrate how sophisticated report generation can enhance the utility and\ntrustworthiness of Deep Research outputs. Additionally, the MegaWika dataset [22] offers a large-scale\nmultilingual resource consisting of millions of articles and referenced sources, enabling collaborative AI\nreport generation.\n2.4.3 Interactive Presentation Technology Development. Beyond static reports, interactive result exploration\nenhances insight discovery and utilization:\n14 Xu et al.\nTechnicalEvolutionTrajectory. Earlysystemsproducedfixedtextualoutputswithminimaluserinteraction.\nModern implementations support dynamic exploration including drill-down capabilities, source verification,\nand alternative viewpoint examination. Advanced systems enable collaborative refinement through iterative\nfeedback incorporation and adaptive response to user queries. This evolution has dramatically enhanced the\nutility and flexibility of Deep Research interfaces.\nRepresentative Systems. HKUDS/Auto-Deep-Research [112] implements sophisticated interactive presenta-\ntion capabilities,allowing usersto explore research findings through dynamicinterfaces, examinesupporting\nevidence, and refine analysis through iterative interaction. These features highlight how interactive presen-\ntation technologies can enhance the utility and accessibility of Deep Research outputs, facilitating more\neffective knowledge transfer and utilization.\nThis technical framework provides a comprehensive foundation for understanding the capabilities and\nevolution of Deep Research systems. The subsequent sections will build on this framework to analyze\nimplementation approaches, evaluate system performance, and explore applications across diverse domains.\n3 Comparative Analysis and Evaluation of Deep Research Systems\nBuilding upon the technical framework established in Section 2, this section provides a comprehensive\ncomparativeanalysisofexistingDeepResearchsystemsacrossmultipledimensions.Weexaminehowdifferent\nimplementations balance technical capabilities, application suitability, and performance characteristics to\naddress diverse research needs.\n3.1 Cross-Dimensional Technical Comparison\nDeep Research systems demonstrate varying strengths across the four key technical dimensions identified\nin our framework. This section analyzes how different implementations balance these capabilities and the\nresulting performance implications.\n3.1.1 Foundation Model and Reasoning Efficiency Comparison. The underlying reasoning capabilities of Deep\nResearch systems significantly impact their overall effectiveness:\nTable1. ComparisonofFoundationModelCharacteristics\nSystem BaseModel ContextLength ReasoningApproach\nOpenAI/DeepResearch [197] o3 may up to 200k tokens [195] Multi-step reasoning\nGemini/DeepResearch [60] Gemini 2.5 Pro 1M tokens [167] Chain-of-thought\nPerplexity/DeepResearch [209] DeepSeek-R1 128K tokens [210] Iterative reasoning\nGrok3Beta [299] Grok 3 1M tokens [299] Chain-of-thought\nAutoGLM-Research [330] ChatGLM DOM Step-by-step planning\nDOM:DependsOntheModel\nCommercialsystemsfromOpenAIandGoogleleverageproprietarymodelswithextensivecontextwindows\nandsophisticatedreasoningmechanisms,enablingthemtoprocesslargervolumesofinformationwithgreater\ncoherence.OpenAI’so3modeldemonstratesparticularstrengthincomplexreasoningtasks,whileGemini2.5\nProexcelsininformationintegrationacrossdiversesources.Incontrast,Perplexity/DeepResearchachieves\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 15\ncompetitive performance with the open-source DeepSeek-R1 model through optimized implementation and\nfocused use cases.\nOpen-source implementations like Camel-AI/OWL [43] and QwenLM/Qwen-Agent [224] demonstrate that\neffective deep research capabilities can be achieved with more accessible models through specialized opti-\nmization. The open-weight approach of Camel-AI/OWL [43] enables flexible deployment across computing\nenvironments, while QwenLM/Qwen-Agent [224] leverages modular reasoning to compensate for more limited\nbase model capabilities.\n3.1.2 Tool Integration and Environmental Adaptability Comparison. The ability to interact with diverse\ninformation environments varies significantly across implementations:\nTable2. EnvironmentalInteractionCapabilitiesofDeepResearchSystems\nSystem WebInteraction APIIntegration DocumentProcessing GUINavigation\nNanobrowser[184] Headlessbrowsing,JavaScriptexecution,dynamiccontentrendering RESTAPIconnectors BasicHTMLparsing Notimplemented\nAutoGLM[330] Fullbrowserautomation,forminteraction RESTfulandGraphQLsupport PDF,Officeformats,JSON Elementidentification,click/inputautomation\ndzhng/deep-research[321] Multi-pagenavigation,cookiehandling OAuthauthenticationsupport Academicpaperextraction,tableparsing Notimplemented\nManus[164] JavaScriptrendering,sessionmanagement 150+serviceintegrations,webhooksupport PDFwithlayoutpreservation,CSVprocessing Basicelementinteraction\nn8n[183] Limited,viaHTTPrequests 200+integrationnodes,customwebhookendpoints CSV/XMLprocessing Notimplemented\nTARS[39] Viewportmanagement,scrollhandling REST/SOAPsupport Standardformatsprocessing Desktopapplicationcontrol,UIelementrecognition\nNote:Capabilitiesdocumentedbasedonsystemrepositories,technicaldocumentation,andpublisheddemonstrationsasofApril2025.\nSpecialized tools like Nanobrowser [184] excel in web interaction capabilities, providing sophisticated\nnavigation and content extraction optimized for research workflows. Systems like dzhng/deep-research\n[321] and nickscamara/open-deep-research [42] complement these capabilities with advanced document\nprocessing features that can extract structured information from diverse formats.\nComprehensive platforms like Manus [164] and AutoGLM [330] offer broader environmental interaction\ncapabilities, balancing web browsing, API integration, and document processing. These systems can adapt\nto diverse research scenarios but may not match the specialized performance of more focused tools in\nspecific domains. The workflow automation capabilities of n8n [183] provide exceptional flexibility for API\nintegration but offer more limited direct interaction with web and document environments.\n3.1.3 Task Planning and Execution Stability Comparison. Effective research requires reliable task planning\nand execution capabilities:\nTable3. PlanningandExecutionCapabilitiesofDeepResearchSystems\nSystem TaskPlanningMechanisms ErrorHandlingFeatures CollaborationInfrastructure\nOpenAI/AgentsSDK[199] Hierarchicaltaskdecomposition,goal-orientedplanning Automatedretrylogic,exceptionhandling Supervisor-workerarchitecture\nFlowith/OracleMode[77] Constraint-basedplanning,informationqualityprioritization Checkpoint-basedrecovery Limitedrole-basedworkflow\nAgent-RL/ReSearch[2] Reinforcementlearningplanning,adaptivetaskordering Progressivefallbackstrategies,staterestoration Standardagentmessagingprotocol\nsmolagents/open_deep_research[115] Taskqueuemanagement,priority-basedscheduling Basicretrymechanisms Multi-agentconfiguration,specializedroledefinitions\nTARS[39] Processtemplatearchitecture,event-drivencoordination Statepersistence,interruptionhandling Team-basedagentorganization,sharedmemory\ngrapeot/deep_research_agent[263] Lineartaskexecution,sequentialprocessing Timeouthandling Single-agentarchitecture\nNote:Capabilitiesdocumentedbasedonsystemrepositories,technicaldocumentation,andpublishedimplementationsasofApril2025.\nThe OpenAI/AgentsSDK [199] demonstrates sophisticated planning capabilities with hierarchical task\ndecomposition and adaptive execution, enabling complex research workflows with reliable completion rates.\nSimilarly, Flowith/OracleMode [77] offers advanced planning mechanisms optimized for research tasks,\nthough with more limited error recovery capabilities.\n16 Xu et al.\nAgent-RL/ReSearch [2] employs reinforcement learning techniques to develop robust execution strategies,\nenablingexceptionalerrorrecoverycapabilitiesthatcanadapttounexpectedchallengesduringresearchwork-\nflows. In contrast, smolagents/open_deep_research [115] and TARS [39] focus on multi-agent collaboration,\ndistributing complex tasks across specialized agents to enhance overall research effectiveness.\nSimpler implementations like grapeot/deep_research_agent [263] offer more limited planning and\nexecution capabilities but may provide sufficient reliability for less complex research tasks, demonstrating\nthe range of complexity available across the ecosystem.\n3.1.4 Knowledge Synthesis and Output Quality Comparison. The ability to synthesize findings into coherent,\nreliable outputs varies significantly:\nTable4. KnowledgeSynthesisCapabilitiesofDeepResearchSystems\nSystem SourceEvaluationMechanisms OutputStructuring UserInteractionFeatures\nOpenAI/DeepResearch[197] Sourcecorroboration,authorityrankingalgorithms Hierarchicalreportgeneration,sectionorganization Queryclarificationdialogue,resultexpansion\nPerplexity/DeepResearch[209] Sourcediversitymetrics,publicationdatefiltering Citation-basedorganization,inlineattribution Sourceexplorationinterface,follow-upquestioning\nmshumer/OpenDeepResearcher[249] Publicationvenuefiltering,citationcounttracking Template-baseddocumentgeneration,sectiontemplating Minimalinteraction,batchprocessingfocus\nHKUDS/Auto-Deep-Research[112] Basicsourcecategorization,recencyfiltering Standardacademicformat,headingorganization Interactiveresultexploration,citationnavigation\ngrapeot/deep_research_agent[263] Evidenceclassificationalgorithms,contradictoryclaimdetection Minimalformatting,rawdatapresentation Command-lineinterface,non-interactive\nOpenManus[193] Sourcetypecategorization,basicmetadatafiltering Markdownformatting,hierarchy-basedorganization Basicqueryrefinement,resultbrowsing\nNote:Capabilitiesdocumentedbasedonsystemrepositories,technicaldocumentation,andpublishedimplementationsasofApril2025.\nCommercialplatformslikeOpenAI/DeepResearch[197]andPerplexity/DeepResearch[209]demonstrate\nsophisticatedinformationevaluationcapabilities,effectivelyassessingsourcecredibilityandcontentreliability\nto produce high-quality syntheses. OpenAI’s implementation excels in report structure and organization,\nwhile Perplexity offers particularly strong citation practices for source attribution and verification.\nOpen-source implementations like mshumer/OpenDeepResearcher [249] focus on report structure and\norganization, producing well-formatted outputs that effectively communicate research findings. HKUDS/Auto-\nDeep-Research [112] emphasizes interactive exploration, allowing users to examine evidence and refine\nanalyses through iterative interaction. Specialized tools like grapeot/deep_research_agent [263] prioritize\ninformation evaluation over presentation, focusing on reliable content assessment rather than sophisticated\noutput formatting.\n3.2 Application-Based System Suitability Analysis\nBeyondtechnicalcapabilities,DeepResearchsystemsdemonstratevaryingsuitabilityfordifferentapplication\ncontexts. This section examines how system characteristics align with key application domains.\n3.2.1 Academic Research Scenario Adaptability Assessment. Academic research requires particular empha-\nsis on comprehensive literature review, methodological rigor, and citation quality. Systems like OpenAI/\nDeepResearch[197]excelinthisdomainthroughtheirabilitytoaccessacademicdatabases,comprehensively\nanalyze research methodologies, and generate properly formatted citations. Other specialized academic\nresearch tools like PaperQA [80] and Scite [243] offer complementary capabilities focused specifically on\nscientific literature processing, while Google’s NotebookLm [95] provides structured knowledge workspaces\nfor academic exploration.\nOpenAI/DeepResearch [197] demonstrates exceptional suitability for academic research through its com-\nprehensive literature coverage, methodological rigor, and high-quality citation practices. The system can\neffectively navigate academic databases, understand research methodologies, and produce well-structured\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 17\nTable5. AcademicResearchApplicationFeaturesofDeepResearchSystems\nSystem AcademicDatabaseIntegration MethodologyAnalysisFeatures CitationManagement\nOpenAI/DeepResearch[197] ArXiv,IEEEXplore,PubMed,GoogleScholar Statisticalmethodidentification,studydesignclassification IEEE,APA,MLA,Chicagoformatsupport\nPerplexity/DeepResearch[209] ArXiv,PubMed,JSTOR,ACMDigitalLibrary Experimentaldesignanalysis,samplesizeassessment Automatedcitationgeneration,DOIresolution\ndzhng/deep-research[321] ArXiv,SemanticScholar,limiteddatabaseaccess Basicmethodologyextraction BibTeXexport,standardformatsupport\nCamel-AI/OWL[43] Customcorpusintegration,specializeddomaindatabases Researchdesignpatternrecognition,methodologycomparison Domain-specificcitationformatting\nmshumer/OpenDeepResearcher[249] Openaccessdatabases,PDFrepositoryprocessing Methodologysummaryextraction Standardcitationformatgeneration\nHKUDS/Auto-Deep-Research[112] Universitylibraryintegration,institutionalrepositoryaccess Researchapproachcategorization Referencemanagement,bibliographygeneration\nNote:Featuresdocumentedbasedonsystemrepositories,technicaldocumentation,andpublishedusecasesasofApril2025.\nliterature reviews with appropriate attribution. Perplexity/DeepResearch [209] offers similarly strong\nperformanceforliteraturecoverageandcitationquality,thoughwithsomewhatlessmethodologicalsophisti-\ncation.\nOpen-source alternatives like Camel-AI/OWL [43] provide competitive capabilities for specific academic\ndomains,particularstrengthinmethodologicalunderstandingforspecificdomains.Systemslikedzhng/deep-\nresearch [321], mshumer/OpenDeepResearcher [249], and HKUDS/Auto-Deep-Research [112] offer moderate\ncapabilities across all dimensions, making them suitable for less demanding academic research applications\nor preliminary literature exploration.\n3.2.2 Enterprise Decision-Making Scenario Adaptability Assessment. Business intelligence and strategic\ndecision-making emphasize information currency, analytical depth, and actionable insights:\nTable6. EnterpriseDecision-MakingApplicationFeaturesofDeepResearchSystems\nSystem MarketInformationSources AnalyticalFrameworks DecisionSupportFeatures\nGemini/DeepResearch[60] NewsAPIintegration,SECfilingsaccess,marketdatafeeds Competitoranalysistemplates,trenddetectionalgorithms Executivesummarygeneration,recommendationformatting\nManus[164] Financialdataintegrations,newsaggregation,industryreports Marketsizingframeworks,SWOTanalysistemplates Strategicoptionspresentation,decisionmatrixgeneration\nn8n[183] CRMintegration,marketingplatformconnectivity,customdatasources Customanalyticsworkflowcreation,datapipelineautomation Dashboardgeneration,notificationsystems\nAgent-RL/ReSearch[2] Configurableinformationsourceadapters,customdatainputs Patternrecognitionalgorithms,causalanalysisframeworks Scenarioplanningtools,impactassessmentmatrices\nFlowith/OracleMode[77] Real-timedatafeeds,specializedindustrysources Industry-specificanalyticaltemplates,frameworkapplication Strategicbriefinggeneration,insightprioritization\nTARS[39] Enterprisesystemintegration,desktopapplicationdataaccess Basicanalyticaltemplateapplication Standardizedreporting,datavisualization\nNote:Featuresdocumentedbasedonsystemrepositories,technicaldocumentation,andpublishedusecasesasofApril2025.\nGemini/DeepResearch[60]demonstratesexceptionalsuitabilityforenterprisedecision-makingthroughits\nstrong information currency, analytical capabilities, and actionable output formats. The system effectively\nnavigates business information sources, analyzes market trends, and produces insights directly relevant to\ndecision processes. Manus [164] offers similarly strong performance for information acquisition and analysis,\nthough with somewhat less emphasis on actionable recommendation formatting. Microsoft Copilot [173]\nempowers organizations with powerful generative AI, enterprise-grade security and privacy, and is trusted\nby companies around the world. Similarly, the Adobe Experience Platform AI Assistant [181] employs\nknowledge graph-enhanced retrieval-augmented generation to accurately respond over private enterprise\ndocuments, significantly enhancing response relevance while maintaining provenance tracking.\nWorkflow automation platforms like n8n [183] provide particular strengths in information currency\nand actionability through their integration with enterprise data sources and business intelligence tools.\nResearch-focused systems like Agent-RL/ReSearch [2] and Flowith/OracleMode [77] offer competitive an-\nalytical capabilities but may require additional processing to translate findings into actionable business\nrecommendations.\n3.2.3 Personal Knowledge Management Adaptability Assessment. Individual knowledge management empha-\nsizes accessibility, personalization, and integration with existing workflows:\n18 Xu et al.\nTable7. PersonalKnowledgeManagementFeaturesofDeepResearchSystems\nSystem UserInterfaceDesign CustomizationOptions ExistingToolIntegration\nPerplexity/DeepResearch[209] Web-basedinterface,mobileapplicationsupport Topicpreferencesettings,informationfilteringoptions Browserextension,sharingfunctionality\nnickscamara/open-deep-research[42] Command-lineinterface,webinterfaceoption Modularconfiguration,sourcepriorityadjustment Localfilesystemintegration,note-takingexports\nOpenManus[193] Desktopapplication,localwebinterface Templatecustomization,workflowconfiguration Noteapplicationexports,knowledgebaseconnections\nNanobrowser[184] Programmaticinterface,developer-focusedAPI Fullconfigurationaccess,component-levelcustomization Browserautomationframeworkcompatibility\nsmolagents/open_deep_research[115] Technicalinterface,Pythonlibraryintegration Architecture-levelcustomization,agentbehaviorconfiguration Pythonecosystemintegration,customadaptersupport\nJina-AI/node-DeepResearch[121] Node.jsintegration,API-driveninterface Component-levelconfiguration,pipelinecustomization Node.jsapplicationecosystem,JavaScriptframeworksupport\nNote:Featuresdocumentedbasedonsystemrepositories,technicaldocumentation,andpublishedimplementationsasofApril2025.\nPerplexity/DeepResearch [209] offers strong accessibility for personal knowledge management through\nits consumer-friendly interface and free access tier, though with more limited personalization capabilities.\nOpen-source implementations like nickscamara/open-deep-research [42] and OpenManus [193] provide\ngreater personalization possibilities through local deployment and customization, enabling adaptation to\nindividual information management preferences.\nInfrastructuretoolslikeNanobrowser[184]andJina-AI/node-DeepResearch[121]offerparticularstrengths\nin workflow integration, allowing seamless incorporation into existing personal knowledge management\nsystems and processes. More complex frameworks like smolagents/open_deep_research [115] provide\nsophisticated capabilities but may present accessibility challenges for non-technical users.\n3.3 Performance Metrics and Benchmarking\nBeyond qualitative comparisons, quantitative performance metrics provide objective assessment of Deep\nResearch capabilities across systems.\n3.3.1 Quantitative Evaluation Metrics. Standard benchmarks enable comparative evaluation of core research\ncapabilities:\nTable8. PerformanceonStandardEvaluationBenchmarks\nSystem HLEScore*[212] MMLU**Score[33] HotpotQAScore[307] GAIAScore(pass@1)***[172]\nOpenAI/DeepResearch[197] 26.6% - - 67.36%\nGemini-2.5[60,293] 18.8% - - -\nGemini-2.0-Flash[89,93] - 77.9% - -\nPerplexity/DeepResearch[209] 21.1% - - -\nGrok3Beta[299] - 79.9% - -\nManus[164] - - - 86.5%\nAgent-RL/ReSearch[2] - - 37.51% -\n*Humanity’sLastExam:Testsfrontierresearchcapabilities\n**MassiveMultitaskLanguageUnderstanding:Testsgeneralknowledge\n***GAIAScore(pass@1):Averagescore\nOpenAI/DeepResearch [30, 123, 197] demonstrates leading performance across various benchmark cate-\ngories, particularly excelling in Humanity’s Last Exam (HLE) [212] hich measures advanced research and\nreasoning capabilities. Gemini/DeepResearch [60] shows comparable performance. According to the intro-\nduction of Google Deep Research with Gemini 2.5 Pro Experimental [60, 126], the new model demonstrated\nsuperior user preference over OpenAI/DeepResearch across four key metrics: instruction following (60.6% vs.\n39.4%),Comprehensiveness(76.9%vs.23.1%),Completeness(73.3%vs.26.7%),andWritingquality(58.2%\nvs.41.8%).TheseresultssuggestGemini2.5Pro’senhancedcapabilityinsynthesizingstructured,high-fidelity\nresearchoutputs.Thiscapabilityisfurtheramplifiedinfullstackapplications,wheretheintegrationofGemini\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 19\nTable9. DocumentedPerformanceMetricsfromDeepResearchSystems\nSystem Benchmark ReportedScore EvaluationContext Source\nOpenAI/DeepResearch HLE 26.6% Humanity’sLastExam [197]\nOpenAI/DeepResearch GAIA(pass@1) 67.36% GeneralAIassistanttasks [197]\nPerplexity/DeepResearch HLE 21.1% Humanity’sLastExam [209]\nPerplexity/DeepResearch SimpleQA 93.9% Factualquestionanswering [209]\nGrok3Beta MMLU 92.7% Multitasklanguageunderstanding [299]\nManus GAIA(pass@1) 86.5% GeneralAIassistanttasks [164]\nAgent-RL/ReSearch HotpotQA 37.51% Multi-hopquestionanswering [2]\nAutoGLM WebArena-Lite 55.2%(59.1%retry) Webnavigationtasks [330]\nAutoGLM OpenTable 96.2% Restaurantbookingtasks [330]\nNote:Scoresreflectperformanceonspecificbenchmarksasreportedincitedpublications.Directcomparisonrequiresconsiderationof\nevaluationmethodologiesandtaskspecifications.\nmodels with frameworks like LangGraph facilitates research-augmented conversational AI for comprehen-\nsive query handling, as demonstrated in Google-Gemini/Gemini-Fullstack-Langgraph-Quickstart [94].\nPerplexity/DeepResearch [209] achieves competitive results despite utilizing the open-source DeepSeek-R1\nmodel, highlighting the importance of implementation quality beyond raw model capabilities.\nOpen-source implementations show progressively lower benchmark scores, though many still achieve\nrespectable performance suitable for practical applications. Systems like AutoGLM-Research [330], HKUDS/\nAuto-Deep-Research [112], and Camel-AI/OWL [43] demonstrate that effective research capabilities can be\nachieved with more accessible models and frameworks, though with some performance trade-offs compared\nto leading commercial implementations.\nRecent benchmark development has expanded evaluation to more specialized aspects of research assis-\ntance. The AAAR-1.0 benchmark [157] specifically evaluates AI’s potential to assist research through 150\nmulti-domain tasks designed to test both retrieval and reasoning capabilities. Domain-specific approaches\ninclude DSBench [122], which evaluates data science agent capabilities across 20 real-world tasks[182, 283],\nSciCode[268]forscientificcodegeneration,MASSW[323]forscientificworkflowassistance,andMMSci[147]\nfor multimodal scientific understanding across graduate-level materials. ScienceQA[160] offers a comprehen-\nsive multimodal science benchmark with chain-of-thought explanations for evaluating reasoning capabilities.\nDomain-specific benchmarks like TPBench [58] for theoretical physics and AAAR-1.0 [157] for research\nassistance capabilities offer additional targeted evaluation approaches for specialized research applications.\nMulti-domain code generation benchmark like DomainCodeBench[328] is designed to systematically assess\nlarge language models across 12 software application domains and 15 programming languages. Interactive\nevaluation frameworks like LatEval [114] specifically assess systems’ capabilities in handling incomplete\ninformation through lateral thinking puzzles, providing insight into research abilities under uncertainty and\nambiguity. Complementary approaches like Mask-DPO [100] focus on generalizable fine-grained factuality\nalignment, addressing a critical requirement for reliable research outputs. Domain-specific benchmarks such\nas GMAI-MMBench [51] provide comprehensive multimodal evaluation frameworks specifically designed for\nmedicalAIapplications,whileAutoBench[52]offersautomatedevaluationofscientificdiscoverycapabilities,\nproviding standardized assessment of core research functions. Other broad evaluation frameworks including\nHELM[149],BIG-bench[88],andAGIEval[331],providecomplementaryassessmentdimensions.Specialized\n20 Xu et al.\nmultimodal benchmarks like INQUIRE [279] extend this landscape to ecological challenges, rigorously\nevaluating expert-level text-to-image retrieval tasks critical for accelerating biodiversity research.\nTable10. SpecializedDeepResearchBenchmarks\nBenchmark Focus Area Evaluation Approach Key Metrics\nAAAR-1.0 [157] Research assistance 150 multi-domain tasks Retrieval and reasoning capability\nDSBench [122] Data science 20 real-world tasks End-to-end completion rate\nSciCode [268] Scientific coding Curated by scientists Code quality, task completion\nMASSW [323] Scientific workflows Benchmarking tasks Workflow orchestration quality\nMMSci [147] Multimodal science Graduate-level questions Cross-modal understanding\nTPBench [58] Theoretical physics Physics reasoning Problem-solving accuracy\nNote:Thesebenchmarksrepresentdomain-specificevaluationframeworksforspecializedresearchcapabilities.\n3.3.2 Qualitative Assessment Frameworks. Beyond numeric benchmarks, qualitative evaluation provides\ninsight into practical effectiveness:\nTable11. DocumentedOutputCharacteristicsofDeepResearchSystems\nSystem ContentOrganization InformationDiversity VerificationFeatures NovelConnectionMechanisms\nOpenAI/DeepResearch[197] Hierarchicalstructurewith5+sections,executivesummaries Cross-domainsourceintegration(reportedin[197]) Statement-levelcitationlinking,contradictionflagging Cross-domainconnectionidentification\nGemini/DeepResearch[60] Multi-levelheadingorganization,standardizedformatting Multi-perspectivesourceinclusion(documentedin[60]) Sourcecredibilitymetrics,confidenceindicators Thematicpatternidentification\nPerplexity/DeepResearch[209] Progressiveinformationdisclosure,expandablesections Real-timesourceaggregationacrossplatforms Directquoteattribution,inlinesourcelinking Timeline-basedrelationshipmapping\nmshumer/OpenDeepResearcher[249] Template-baseddocumentstructure,consistentformatting Topic-basedcategorizationofsources Basiccitationframework,referencelisting Topicclustervisualization\ngrapeot/deep_research_agent[263] Minimalformatting,content-focusedpresentation Sourcetypecategorization,domaintracking Sourcecredibilityscoringsystembasedonmetadata Notimplementedperrepositorydocumentation\nAgent-RL/ReSearch[2] Adaptivecontentorganizationbasedoninformationtypes Exploratorysearchpatternsdocumentedinrepository Contradictiondetectionalgorithms Pattern-basedinsightgenerationdocumentedin[2]\nNote:Characteristicsdocumentedbasedonsystemtechnicaldocumentation,publisheddemonstrations,repositoryanalysis,andofficial\ndescriptionsasofApril2025.Specificfeatureimplementationsmayvaryacrosssystemversions.\nCommercial systems generally demonstrate stronger qualitative performance, particularly in output\ncoherence and factual accuracy. OpenAI/DeepResearch [197] produces exceptionally well-structured reports\nwith reliable factual content, while also achieving moderate innovation in connecting disparate sources.\nGemini/DeepResearch [60] shows similar strengths in coherence and accuracy, with slightly less emphasis\non novel insights.\nSome open-source implementations show particular strengths in specific dimensions. Agent-RL/ReSearch\n[2]achievesnotableperformanceininsightnoveltythroughitsexploration-focusedapproach,whilegrapeot/\ndeep_research_agent [263] demonstrates strong factual accuracy through its emphasis on information\nverification. These specialized capabilities highlight the diversity of approaches within the Deep Research\necosystem.\n3.3.3 EfficiencyandResourceUtilizationMetrics. Practicaldeploymentconsiderationsincludecomputational\nrequirements and operational efficiency:\nCommercial cloud-based services offer optimized performance with moderate response times, though\nwith dependency on external infrastructure and associated costs. Perplexity/DeepResearch [209] achieves\nparticularly strong efficiency metrics, with relatively quick response times and high token efficiency despite\nits competitive output quality.\nOpen-source implementations present greater variability in efficiency metrics. Systems like AutoGLM-\nResearch [330] and QwenLM/Qwen-Agent [224] require substantial computational resources but can be\ndeployed in local environments, offering greater control and potential cost savings for high-volume usage.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 21\nTable12. EfficiencyandResourceUtilization\nSystem ResponseTime* ComputeRequirements TokenEfficiency**\nOpenAI/DeepResearch[197] 5-30min Cloud-only High(detailed,citation-rich)\nPerplexity/DeepResearch[209] 2m59s Cloud-only -\nGrok3Beta[299] - Cloud-only -\nNanobrowser[184] - User-definedviaLLMAPIkey -\nn8n[183] - Self-hostedorcloud-based;scalable -\n*Typicalresponsetimeformoderatelycomplexresearchtasks\n**Efficiencyoftokenutilizationrelativetooutputquality\nLighter-weight implementations like nickscamara/open-deep-research [42] can operate with more limited\nresources but typically demonstrate longer response times and lower token efficiency.\nThiscomparativeanalysishighlightsthediversityofapproachesandcapabilitiesacrosstheDeepResearch\necosystem. While commercial implementations currently demonstrate leading performance on standard\nbenchmarks, open-source alternatives offer competitive capabilities in specific domains and use cases, with\nparticular advantages in customization, control, and potential cost efficiency for specialized applications.\nThe subsequent sections will build on this analysis to examine implementation technologies, evaluation\nmethodologies, and application domains in greater detail.\n4 Implementation Technologies and Challenges\nThe practical realization of Deep Research systems involves numerous technical challenges spanning in-\nfrastructure design, system integration, and safeguard implementation. This section examines the key\nimplementation technologies that enable effective Deep Research capabilities and the challenges that must\nbe addressed for reliable, efficient operation.\n4.1 Architectural Implementation Patterns\nThe diverse systems analyzed in this survey reveal several distinct architectural patterns that represent\ndifferent approaches to implementing Deep Research capabilities. This section examines four fundamental\narchitecturalpatterns:monolithic,pipeline-based,multi-agent,andhybridimplementations.Foreachpattern,\nwe analyze the underlying structural principles, component interactions, information flow mechanisms, and\nrepresentative systems.\n4.1.1 Monolithic Architecture Pattern. Monolithic implementations integrate all Deep Research capabilities\nwithin a unified architectural framework centered around a core reasoning engine. As illustrated in Figure 4,\nthese systems employ a centralized control mechanism with direct integration of specialized modules.\nThe defining characteristics of this architecture include:\n∙ CentralizedControlFlow: All operations route through a primary reasoning engine that maintains\nglobal state and execution context\n∙ Tightly Coupled Integration: Specialized modules (web browsing, document processing, etc.) are\ndirectly integrated with the central controller\n∙ Shared Memory Architecture: Information state is maintained in a centralized memory system\naccessible to all components\n22 Xu et al.\nImplementation Architecture of Deep Research Systems\nFoundation Model Information Retrieval Orchestration KnowledgeSynthesis Core Systems\nMulti-Agent System Foundation Model\nMemory System\n(Distributed Reasoning) (LLM Reasoning Engine)\nsmolagents/ (Context Management)\no3, Gemini, DeepSeek-R1\nopen_deep_research\nWeb Crawling API Integration Document Analysis Specialized Tools\n(Dynamic Discovery) (Structured Data) (Content Processing) (Domain-Specific)\nNanobrowser, AutoGLM n8n, Manus dzhng/deep-research Camel-AI/OWL\nTask Planning Execution Control\n(Decomposition Scheduling) (Monitoring Recovery)\nOpenAI Agents SDK, Flowith Agent-RL/ReSearch, TARS\nInformation Evaluation Report Generation Interactive Presentation\n(Quality Assessment) (Structured Output) (User Exploration)\ngrapeot/deep_research_agent mshumer/OpenDeepResearcher HKUDS/Auto-Deep-Research\nUser Interface\nFig.3. ImplementationArchitectureofDeepResearchSystems\n∙ Sequential Reasoning Processes: Operations typically follow a structured sequence defined by the\ncentral controller\nThis architectural pattern offers strong coherence and reasoning consistency through its unified control\nstructure. However, it presents challenges for extensibility and can struggle with parallelization of com-\nplex operations. Representative implementations include OpenAI/DeepResearch [197] and grapeot/deep_\nresearch_agent [263], which demonstrate how this architecture enables coherent reasoning across diverse\ninformation sources while maintaining implementation simplicity.\n4.1.2 Pipeline-Based Architecture Pattern. Pipeline architectures implement Deep Research capabilities\nthrough a sequence of specialized processing stages connected through well-defined interfaces. As shown in\nFigure 5, these systems decompose research workflows into discrete processing components with explicit\ndata transformations between stages.\nThe key characteristics of pipeline implementations include:\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 23\nMonolithic Deep Research Architecture\nUser Interface\nWeb Browsing Document\nFoundation Model\nModule Processor\n(CentralizedReasoningEngine)\nAPI Integration Knowledge\nModule Synthesizer\nShared Memory System\nKey Features: Representative Systems:\n• Centralized control flow • OpenAI/Deep Research\n• Tightly coupled components • grapeot/deep_research_agent\n• Unified shared memory\nResearch Output\n• Sequential reasoning processes\nFig.4. MonolithicDeepResearchArchitecture\n∙ SequentialComponentOrganization:Researchtasksflowthroughapredefinedsequenceofspecialized\nprocessing modules\n∙ Standardized Interfaces: Clear data transformation specifications between pipeline stages enable\nmodular component replacement\n∙ Staged Processing Logic: Each component implements a specific transformation, with minimal\ndependence on global state\n∙ ConfigurableWorkflowPaths: Advanced implementations enable conditional routing between alter-\nnative processing paths based on intermediary results\nPipeline architectures excel in workflow customization and component reusability but may struggle\nwith complex reasoning tasks requiring iterative refinement across components. Systems like n8n [183]\nand dzhng/deep-research [321] exemplify this approach, demonstrating how explicit workflow sequencing\nenables sophisticated research automation through composition of specialized components.\n4.1.3 Multi-Agent Architecture Pattern. Multi-agent architectures implement Deep Research capabilities\nthroughecosystemsofspecializedautonomousagentscoordinatedthroughexplicitcommunicationprotocols.\nFigure 6 illustrates how these systems distribute research functionality across collaborating agents with\ndifferentiated roles and responsibilities.\n24 Xu et al.\nPipeline-Based Deep Research Architecture\nQuery Processing Information Content Analysis Output\nStage 1 Retrieval Stage 2 Stage 3 Stage 4\nIntentClassifier WebSearchManager ContentExtractor Report\nQueryExpander DatabaseConnector FactVerificator Visual\nDocumentFetcher KnowledgeIntegrator\nStandardized Data Transformation Interface\nKey Features: Representative Systems:\n• Sequential component organization • n8n\nConditional Routing Control • dzhng/deep-research\n• Standardized interfaces\n• Component reusability\nFig.5. Pipeline-BasedDeepResearchArchitecture\nThe defining elements of multi-agent implementations include:\n∙ DistributedFunctionalDecomposition: Research capabilities are distributed across specialized agents\nwith defined roles (searcher, analyst, critic, etc.)\n∙ Explicit Coordination Mechanisms: Standardized message passing and task delegation protocols\nenable inter-agent collaboration\n∙ AutonomousDecisionLogic: Individual agents maintain independent reasoning capabilities within\ntheir designated domains\n∙ DynamicTaskAllocation:Advancedimplementationsemployflexibletaskassignmentbasedonagent\ncapabilities and current workload\nMulti-agent architectures excel in complex research tasks requiring diverse specialized capabilities and\nparallel processing. Their distributed nature enables exceptional scaling for complex research workflows but\nintroduceschallengesinmaintainingoverallcoherenceandconsistentreasoningacrossagents.Representative\nimplementations include smolagents/open_deep_research [115] and TARS [39], which demonstrate how\nmulti-agent coordination enables sophisticated research workflows through specialized agent collaboration.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 25\nMulti-Agent Deep Research Architecture\nAgent Coordination Layer\nSearch Agent AnalystAgent CriticAgent WriterAgent\n(Information Retrieval) (DataProcessing) (Verification) (ContentGeneration)\nStandardized Message Bus\nWeb Crawler API Client Parser Analytics Fact Check Validator Template Formatter\nSearch Memory Analysis Memory Verification Memory Content Memory\nSharedKnowledgeRepository\nKey Features: Representative Systems:\n• Distributed functional decomposition • Explicit coordination mechanisms • smolagents/open_deep_research\n• Autonomous specialized agents • TARS\nFig.6. Multi-AgentDeepResearchArchitecture\n4.1.4 Hybrid Architecture Pattern. Hybrid architectures combine elements from multiple architectural\npatterns to balance their respective advantages within unified implementations. As shown in Figure 7, these\nsystems employ strategic integration of architectural approaches to address specific research requirements.\nKey characteristics of hybrid implementations include:\n∙ TieredArchitecturalOrganization: Different architectural patterns are employed at different system\nlevels based on functional requirements\n∙ Domain-SpecificOptimization: Architectural approaches are selected based on domain-specific pro-\ncessing requirements\n∙ FlexibleIntegrationMechanisms:Standardizedinterfacesenablecommunicationbetweencomponents\nemploying different architectural patterns\n∙ Adaptive Execution Frameworks: Control mechanisms dynamically adjust processing approaches\nbased on task characteristics\nHybrid architectures offer exceptional flexibility and optimization opportunities but introduce implemen-\ntation complexity and potential integration challenges. Systems like Perplexity/DeepResearch [209] and\nCamel-AI/OWL [43] exemplify this approach, combining centralized reasoning with distributed information\n26 Xu et al.\nHybrid Deep Research Architecture\nAdaptive Orchestration Layer\nMonolithic Core Pipeline Sub-system Multi-Agent Sub-system\n(Centralized Reasoning) (Sequential Processing) (Distributed Collaboration)\nDomain: Complex Reasoning Domain: Data Transformation Domain: Parallel Information Gathering\nFoundationModel ContentExtractor SearchAgents\nUnifiedMemory DataTransformPipeline MessageBus\nUnified Integration Layer\nExternal Tools, APIs, and Data Sources\nnoitatpadA\nniamoD\nKey Features: Representative Systems: Perplexity/Deep Research, camel-ai/owl\n• Domain-specific optimization • Tiered architectural organization • Flexible integration mechanisms • Adaptive execution\nFig.7. HybridDeepResearchArchitecture\ngathering and specialized processing pipelines to achieve sophisticated research capabilities with balanced\nperformance characteristics.\n4.1.5 Emerging Agent Framework Ecosystems. Beyond the core architectural patterns described above, the\nDeep Research ecosystem has been significantly enhanced by specialized agent frameworks that provide\nstandardizedcomponentsforagentdevelopment.Emergingsystemsincorporatespecializedagentframeworks\n[54, 142, 301] that structure reasoning in ways particularly suited to complex research tasks requiring both\ndepth and breadth of analysis. As detailed in comprehensive analyses of agent frameworks [133, 304], these\nsystems offer varying approaches to agent orchestration, execution control, and reasoning orchestration.\nKey frameworks include LangGraph [134], which provides graph-based control flow for language model\napplications, enabling complex reasoning patterns through explicit state management and transition logic.\nGoogle’s Agent Development Kit (ADK) [91] offers a comprehensive framework for agent development with\nstandardizedinterfacesfortoolintegration,planning,andexecutionmonitoring.CrewAI[64]implementsan\nagent collaboration framework designed specifically for multi-specialist workflows, enabling role-based task\ndistribution with explicit coordination mechanisms. More experimental frameworks like Agno [3] explore\nagentic autonomy through self-improvement and meta-reasoning capabilities.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 27\nThe TapeAgents framework [19] provides a particularly comprehensive approach to agent development\nand optimization, with explicit support for iterative refinement through systematic recording and analysis\nof agent behavior. These frameworks collectively demonstrate an ongoing shift toward standardized agent\ncomponents that enhance development efficiency while enabling more complex reasoning and execution\npatterns.\n4.1.6 Architectural Pattern Comparison. Table 13 provides a comparative analysis of these architectural\npatterns across key performance dimensions:\nTable13. ArchitecturalPatternCharacteristicsinDeepResearchSystems\nCharacteristic Monolithic Pipeline Multi-Agent Hybrid\nControlStructure Centralized Sequential Distributed Mixed\nComponentCoupling Tight Loose Moderate Variable\nFailurePropagation System-wide Stage-limited Agent-isolated Component-dependent\nDevelopmentComplexity Minimal Moderate Substantial Maximal\nDeploymentFlexibility Limited Moderate Moderate High\nRepresentativeSystems grapeot/deep_research_agent n8n,dzhng/deep-research smolagents,TARS Perplexity,Camel-AI/OWL\nNote:Characteristicsbasedonarchitecturalanalysisofsurveyedsystems.Quantitativeperformancecomparisonrequiresstandardized\nbenchmarkingacrossidenticaltasksandenvironments.\nEach architectural pattern presents distinct advantages and limitations that influence its suitability for\nspecific Deep Research applications. Monolithic architectures excel in reasoning coherence and implemen-\ntation simplicity, making them appropriate for focused research applications with well-defined workflows.\nPipelinearchitecturesofferexceptionalextensibilityandcomponentreusability,enablingcustomizedresearch\nworkflows through modular composition. Multi-agent architectures provide superior parallelization and fault\ntolerance, supporting complex research tasks requiring diverse specialized capabilities. Hybrid architectures\nbalance these characteristics through strategic integration, offering flexible optimization for diverse research\nrequirements.\nThearchitecturalpatternselectionsignificantlyinfluencessystemcapabilities,performancecharacteristics,\nand application suitability. As the Deep Research ecosystem continues to evolve, we anticipate further\narchitecturalinnovationcombiningelementsfromthesefoundationalpatternstoaddressemergingapplication\nrequirements and technical capabilities.\n4.2 Infrastructure and Computational Optimization\nDeep Research systems require sophisticated infrastructure to support their complex reasoning and informa-\ntion processing capabilities.\n4.2.1 Distributed Reasoning Architectures. Effective reasoning across expansive information landscapes\nrequires specialized architectural approaches. Frameworks like AutoChain [78] and AutoGen [298] have\npioneered distributed agent paradigms that can be applied to research workflows. Advanced systems\nemploy distributed reasoning architectures that decompose complex queries into parallel processing paths.\nOpenAI/DeepResearch[197]implementsahierarchicalreasoningframeworkthatdistributesanalyticaltasks\nacross multiple execution threads while maintaining coherent central coordination.\nImplementation approaches increasingly leverage specialized frameworks for efficient LLM serving, in-\ncluding LightLLM [177], Ollama [192], VLLM [281], and Web-LLM [176] for browser-based deployment.\n28 Xu et al.\nThese frameworks enable more efficient utilization of computational resources, particularly important for\nresource-intensive research workflows requiring extensive model inference. Such optimizations are especially\ncriticalforopen-sourceimplementationsoperatingwithmoreconstrainedcomputationalresourcescompared\nto commercial cloud-based alternatives.\nParallel Reasoning Pathways. Advanced systems employ distributed reasoning architectures that decom-\npose complex queries into parallel processing paths. OpenAI/DeepResearch [197] implements a hierarchical\nreasoning framework that distributes analytical tasks across multiple execution threads while maintaining\ncoherentcentralcoordination.SimilarapproachesareevidentinGemini/DeepResearch[60],whichleverages\nGoogle’s distributed computing infrastructure to parallelize information analysis while preserving reasoning\nconsistency.\nOpen-source implementations like HKUDS/Auto-Deep-Research [112] and Agent-RL/ReSearch [2] demon-\nstrate more accessible distributed reasoning approaches, utilizing task decomposition and asynchronous\nprocessing to enhance performance within more constrained computational environments. These systems\nshow that effective parallelization can be achieved even without the extensive infrastructure of commercial\nplatforms.\nMemory and State Management. Distributed reasoning introduces significant challenges in memory coher-\nenceandstatemanagement.Commercialsystemsimplementsophisticatedstatesynchronizationmechanisms\nthatmaintainconsistentreasoningcontextsacrossdistributedcomponents.OpenAI’simplementationutilizes\na hierarchical memory architecture with explicit coordination protocols [200], while Google’s approach\nleverages its existing distributed computing frameworks adapted for reasoning workflows.\nOpen-source alternatives like Camel-AI/OWL [43] employ simplified but effective memory management\napproaches, including centralized knowledge repositories with controlled access patterns. These implementa-\ntions demonstrate pragmatic solutions to state management challenges within more constrained technical\nenvironments.\n4.2.2 Parallel Search and Information Retrieval. Information acquisition represents a primary bottleneck in\nDeep Research performance:\nConcurrent Query Execution. Advanced systems implement sophisticated parallel search infrastructures\nto accelerate information gathering. Perplexity/DeepResearch [209] employs a multi-threaded search\narchitecture that dispatches dozens of concurrent queries across different information sources, significantly\naccelerating the research process. Similar capabilities are evident in dzhng/deep-research [321], which\nimplements a specialized scheduler for concurrent web queries with adaptive rate limiting to avoid service\nrestrictions.\nInfrastructure tools like Nanobrowser [184] provide optimized platforms for parallel browsing operations,\nenablingmultipleconcurrentpageloadswithsharedresourcemanagement.Thesespecializedcomponentsen-\nhancetheinformationgatheringcapabilitiesofintegratedsystemslikeManus[164]andFlowith/OracleMode\n[77], which leverage concurrent browsing to accelerate their research workflows.\nQuery Coordination and Deduplication. Effective parallel search requires sophisticated coordination to\navoid redundancy and ensure comprehensive coverage. Commercial systems implement advanced query\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 29\nplanning that dynamically adapts to intermediate results, adjusting search strategies based on discovered\ninformation. OpenAI’s implementation includes explicit deduplication mechanisms that identify and con-\nsolidate redundant sources, while Perplexity employs source diversification techniques to ensure broad\ncoverage.\nOpen-sourcetoolslikenickscamara/open-deep-research[42]implementpragmaticapproachestoquery\ncoordination,includingsimplebuteffectivecachingmechanismsandresultfingerprintingtoavoidredundant\nprocessing. These techniques demonstrate that effective coordination can be achieved with relatively\nstraightforward implementation approaches.\n4.2.3 Resource Allocation and Efficiency Optimization. Computational efficiency significantly impacts both\nperformance and operational economics:\nAdaptive Resource Allocation. Advanced systems implement dynamic resource allocation based on task\ncharacteristics and complexity. Gemini/DeepResearch [60] employs sophisticated workload prediction to\nprovision computational resources adaptively, allocating additional capacity for more complex research\ntasks.Similarapproachesareemerginginopen-sourceimplementationslikeQwenLM/Qwen-Agent[224],which\nincorporates task complexity estimation to guide resource allocation decisions.\nProgressive Processing Strategies. Efficiency-focused implementations employ progressive processing ap-\nproaches that incrementally refine results based on available information. Perplexity/DeepResearch [209]\nutilizesastagedanalysisapproachthatprovidespreliminaryfindingsquicklywhilecontinuingdeeperanalysis\nin the background. This strategy enhances perceived responsiveness while ensuring comprehensive results\nfor complex queries.\nOpen-source alternatives like mshumer/OpenDeepResearcher [249] implement simpler but effective pro-\ngressive strategies, including early result previews and incremental report generation. These approaches\ndemonstrate pragmatic solutions to efficiency challenges without requiring sophisticated infrastructure.\n4.3 System Integration and Interoperability\nDeep Research systems must effectively coordinate diverse components and external services to deliver\ncomprehensive capabilities.\n4.3.1 API Design and Standardization. Consistent interfaces enable modular development and component\ninteroperability:\nComponent Interface Standardization. Current Deep Research implementations employ largely incom-\npatible architectures and interfaces. Future research could build upon emerging standardization efforts\nlike Anthropic’s Model Context Protocol (MCP) [12] and Google’s Agent2Agent Protocol (A2A) [90, 92]\nto establish truly universal component interfaces. MCP provides a structured framework for model-tool\ninteraction, enabling consistent integration patterns across diverse LLM applications, while A2A focuses\non standardized agent-to-agent communication to facilitate multi-agent systems. These complementary\napproaches could form the foundation for comprehensive standardization enabling modular development\n30 Xu et al.\nand interchangeable components across implementations. Early steps in this direction appear in frame-\nworks like OpenAI/AgentsSDK [199], which provides standardized agent definitions, but more comprehensive\nstandardization would require broader industry adoption of common protocols.\nWorkflow Automation. Several workflow automation platforms like Dify [259], Coze [38], and Flowise\n[5] have emerged as low-code environments for building LLM-powered applications, potentially offering\nstandardized frameworks for Deep Research components. Advanced workflow orchestration platforms\nincluding Temporal [265], Restate [229], and Orkes [203] provide robust infrastructure for complex, stateful\nworkflows with explicit support for long-running processes and reliability patterns crucial for sophisticated\nresearchapplications.Implementationapproachesmightincludedefiningstandardmessagepassingprotocols\nbetweenresearchcomponents,establishingcommondatastructuresforresearchtasksandresults,developing\ncompatibility layers between competing standards, extending existing protocols with research-specific\ninteraction patterns, and establishing common evaluation frameworks for component interoperability. These\nadvancescouldaccelerateecosystemdevelopmentbyenablingspecializedcomponentsfromdiversedevelopers\nto work seamlessly within unified frameworks, significantly enhancing the pace of innovation through\ncomponentization and reuse.\nExternal Service Integration. Access to specialized external services significantly enhances research capa-\nbilities. Advanced retrieval frameworks like LlamaIndex [235] provide standardized interfaces for retrieval\naugmentation, enabling consistent integration patterns across diverse information sources and document\nformats. Systems like n8n [183] excel in external service integration through their comprehensive connector\nlibraryandstandardizedauthenticationmechanisms.Thiscapabilityenablesaccesstospecializedinformation\nsources and analytical services that extend beyond basic web search.\nOpen-source frameworks like Jina-AI/node-DeepResearch [121] implement simplified but effective API\nintegration patterns, providing standardized wrappers for common services while maintaining extensibility\nfor custom integrations. These approaches balance standardization with flexibility for diverse research\nrequirements.\n4.3.2 Tool Integration Frameworks. Effective orchestration of diverse tools enhances overall system capabili-\nties:\nTool Selection and Composition. Advanced systems implement sophisticated tool selection based on\ntask requirements and information context. Manus [164] features an adaptive tool selection framework\nthat identifies appropriate tools for specific research subtasks, dynamically composing workflows based on\navailable capabilities. Similar approaches are emerging in open-source implementations like grapeot/deep_\nresearch_agent [263], which includes basic tool selection heuristics based on task classification.\nTool Execution Monitoring. Reliabletoolusagerequireseffectiveexecutionmonitoringanderrorhandling.\nCommercial systems implement sophisticated monitoring frameworks that track tool execution, detect\nfailures, and implement recovery strategies. OpenAI’s implementation includes explicit success criteria\nverification and fallback mechanisms for tool failures, ensuring reliable operation even with unreliable\nexternal components.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 31\nOpen implementations like Agent-RL/ReSearch [2] demonstrate more accessible monitoring approaches,\nincluding simplified execution tracking and basic retry mechanisms for common failure modes. These imple-\nmentations show that effective monitoring can be achieved with relatively straightforward implementation\nstrategies.\nRecent advances in agent collaboration frameworks [145, 221] highlight significant challenges in agent\ncoordination [46], particularly for complex research tasks requiring diverse, specialized capabilities working\nin concert toward unified research objectives.\n4.3.3 Cross-Platform Compatibility. Deployment flexibility requires careful attention to environmental de-\npendencies:\nPlatform Abstraction Layers. Cross-platform implementations employ abstraction layers to isolate core\nlogic from environmental dependencies. TARS [39] implements a sophisticated abstraction architecture that\nseparates its core reasoning framework from platform-specific integration components, enabling deployment\nacrossdiverseenvironments.SimilarapproachesareevidentinNanobrowser[184],whichprovidesconsistent\nbrowsing capabilities across different operating systems.\nContainerization and Deployment Standardization. Modern implementations leverage containerization to\nensureconsistentdeploymentacrossenvironments.OpenManus[193]providesexplicitcontainerconfigurations\nthat encapsulate all dependencies, enabling reliable deployment across diverse infrastructures. Similar\napproachesareemployedbyAutoGLM-Research[330],whichprovidesstandardizeddeploymentconfigurations\nfor different environments. Alongside containerization, modern cloud platforms such as Vercel [280] offer\nstreamlined, standardized deployment workflows for the web-based interfaces of many research applications.\n4.3.4 Research-Oriented Coding Assistance Integration. The integration of AI-powered coding assistants\nrepresents an increasingly important dimension of Deep Research system capabilities, particularly for\ncomputational research workflows requiring custom analysis scripts, data processing pipelines[108], and\nresearch automation tools.\nCoding Assistant Integration Patterns. Modern research workflows increasingly depend on custom code\ndevelopment for data analysis, visualization, and automation tasks. AI coding assistants have emerged as\ncrucial tools for enhancing researcher productivity in these computational aspects. The landscape of coding\nassistance tools demonstrates varying approaches to integration with research workflows, from IDE-native\ncompletionsystemstoconversationalcodegenerationinterfaces.SystemslikeGitHubCopilot[20,86]provide\nseamless integration within development environments, enabling context-aware code completion for research\nscripts and analysis workflows. Complementary approaches like ChatGPT-based code generation [309] offer\nconversational interfaces that can translate research requirements into executable implementations. More\nspecialized frameworks like AutoDev [275], DSPy[257], and Pydantic-AI[216] enable end-to-end automated\ndevelopment workflows particularly suited for research prototype generation and experimental tool creation.\nAdditionally, tools like Bolt [32] allow researchers to create web applications directly from text descriptions,\nhandlingthecodingprocesswhiletheyfocusontheirvision.EvolutionarycodingagentslikeAlphaEvolve[190]\nfurther enhance capabilities by iteratively optimizing algorithms using autonomous pipelines of LLMs and\nevolutionaryfeedbackmechanisms.RecentresearchexploresthesynergybetweengenerativeAIandsoftware\n32 Xu et al.\nengineering, leveraging techniques like zero-shot prompting to enhance coding assistants and streamline\ndevelopment processes [41]. However, research has revealed limitations in these assistants’ capabilities, such\nas ambiguous beliefs regarding research claims and a lack of credible evidence to support their responses\n[35]. A large-scale survey demonstrates that developers frequently decline initial suggestions, citing unmet\nfunctional or non-functional requirements and challenges in controlling the tool to generate desired outputs\n[148]. User resistance behaviors documented in such surveys highlight the need for comprehensive adoption\nstrategies, including providing active support during initial use, clearly communicating system capabilities,\nand adhering to predefined collaboration rules to mitigate low acceptance rates[252]. This underscores the\nneedforadaptivehintsystems,whichcanprovidepersonalizedsupportforbugfindingandfixingbytailoring\nto user understanding levels and program representations to improve accuracy in debugging tasks [226].\nPioneeringstudiesemployphysiologicalmeasurementssuchasEEGandeyetrackingtoquantifydevelopers’\ncognitive load during AI-assisted programming tasks, addressing critical gaps in understanding actual usage\npatterns and productivity impacts [106]. Furthermore, tools like CodeScribe address challenges in AI-driven\ncodetranslationforscientificcomputingbycombiningpromptengineeringwithusersupervisiontoautomate\nconversion processes while ensuring correctness [69]. Similarly, CodeCompose’s multi-line suggestion feature\ndeployed at Meta demonstrates substantial productivity improvements, saving 17% of keystrokes through\noptimizedlatencysolutionsdespiteinitialusabilitychallenges[72].Moreover,fordebuggingtasks,ChatDBG\n[139] enhances debugging capabilities by enabling programmers to engage in collaborative dialogues for root\ncause analysis and bug resolution, leveraging LLMs to provide domain-specific reasoning. Intelligent QA\nassistants are also being developed to streamline bug resolution processes [308], and grey literature reviews\nindicateagrowingtrendinAI-assistedtestautomation[231].Additionally,benchmarkslikeCodeMMLU[163]\nevaluate code understanding and reasoning across diverse tasks, revealing significant comprehension gaps in\ncurrent models despite advanced generative capabilities. Empirical evaluations of ACATs through controlled\ndevelopment scenarios demonstrate nuanced variations in acceptance patterns, modification reasons, and\neffectivenessbasedontaskcharacteristicsanduserexpertise[260].GenerativeAItoolssignificantlyenhance\ndeveloper productivity by accelerating learning processes and altering collaborative team workflows through\nreduced repetitive tasks, fundamentally transforming development paradigms [277]. To realize the vision of\nnext-generation AI coding assistants, it is crucial to address integration gaps and establish robust design\nprinciples such as setting clear usage expectations and employing extendable backend architectures [186].\nTable14. QualitativeAssessmentofAICodingAssistantsforResearchApplications\nSystem DocumentedCapabilities IntegrationApproach EvaluationEvidence Research-SpecificFeatures\nGitHubCopilot[86,319] Codecompletion,documentation IDE-nativeintegration Userstudyonpractices[319] Limiteddomainspecialization\nAmazonCodeWhisperer[175] Security-focusedsuggestions AWSecosystemintegration Comparativeevaluation[309] Cloudresearchworkflows\nChatGPTCode[309] Conversationalcodegeneration API-basedinteraction Codequalityassessment[309] Naturallanguagespecification\nCursor[65] Context-awarecompletion Codebaseintegration Nopublishedevaluation Repository-levelunderstanding\nCodeium[206] Multi-languagesupport Editorextensions Comparativebenchmark[206] Analysisworkflowsupport\nAutoDev[275] Automateddevelopment Taskautomationpipeline Empiricalevaluation[275] End-to-endimplementation\nGPT-Pilot[217] Projectscaffolding Guideddevelopmentprocess Repositorydemonstrations Researchprototypegeneration\nNote:Capabilitiesandevaluationsbasedonpublishedstudiesanddocumentedfeatures.Comparativeperformancerequiresstandardized\nevaluationacrossidenticaltasks.\nThe diversity of coding assistance approaches highlights the importance of integration flexibility within\nDeep Research systems. While some implementations benefit from tightly integrated coding assistance\nthat understands research context, others require more flexible interfaces that can accommodate diverse\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 33\ndevelopment workflows and programming paradigms. This integration dimension becomes particularly\ncrucial as research increasingly requires custom computational tools and analysis pipelines that extend\nbeyond pre-existing software packages[75, 244, 295]. Recent work by Chen et al. [53] demonstrates that\nproactive programming assistants, which automatically provide suggestions to enhance productivity and\nuser experience, represent a key advancement in this domain. Additionally, ChatDev [220] exemplifies how\nlinguistic communication serves as a unifying bridge for multi-agent collaboration in software development,\nstreamlining the entire lifecycle from design to testing. Moreover, research on integrating AI assistants in\nAgile meetings reveals critical links to team collaboration dynamics and provides roadmaps for facilitating\ntheir adoption in development contexts [40]. As demonstrated by Talissa Dreossi[70], this hybrid approach\nbridges the gap between the high performance of deep learning models and the transparency of symbolic\nreasoning, advancing AI by providing interpretable and trustworthy applications.\nResearch Workflow Code Generation. Advanced coding assistants specifically optimized for research\ncontextsdemonstrateparticularvalueintranslatingresearchmethodologiesintoexecutableimplementations.\nSystems like GPT-Pilot [217] enable guided development of complete research applications, while domain-\nspecific tools can generate analysis scripts aligned with particular research methodologies or data types.\nThesecapabilitiesenhanceresearchefficiencybyreducingthetechnicalbarriersbetweenresearchdesignand\ncomputational implementation.\nImplementation patterns typically involve integration with research data management systems, version\ncontrolworkflows,andcollaborativedevelopmentenvironmentsthatsupportreproducibleresearchpractices.\nTheeffectivenessofsuchintegrationdependssignificantlyonthecodingassistant’sunderstandingofresearch-\nspecificrequirementsincludingdocumentationstandards,reproducibilityconsiderations,anddomain-specific\nlibraries and frameworks commonly used in particular research fields[124].\n4.4 Technical Challenges and Solutions\nDeep Research systems face numerous technical challenges that must be addressed for reliable, trustworthy\noperation.\n4.4.1 Hallucination Control and Factual Consistency. Maintaining factual accuracy represents a fundamental\nchallenge for LLM-based research systems:\nSource Grounding Techniques. Advanced implementations employ explicit source grounding to enhance\nfactual reliability. Perplexity/DeepResearch [209] implements strict attribution requirements that link all\ngenerated content to specific sources, reducing unsupported assertions. Similar approaches are evident in\nOpenAI/DeepResearch[197],whichmaintainsexplicitprovenancetrackingthroughoutthereasoningprocess.\nOpen-source implementations like grapeot/deep_research_agent [263] demonstrate more accessible\ngrounding approaches, including simple but effective citation tracking and verification mechanisms. These\ntechniques show that meaningful improvements in factual reliability can be achieved with straightforward\nimplementation strategies.\nContradiction Detection and Resolution. Effectiveresearchrequiresidentificationandresolutionofcontra-\ndictory information. Commercial systems implement sophisticated contradiction detection mechanisms that\nidentify inconsistencies between sources and implement resolution strategies [296]. Gemini/DeepResearch\n34 Xu et al.\n[60] includes explicit uncertainty modeling and conflicting evidence presentation, enhancing transparency\nwhen definitive conclusions cannot be reached.\nOpenimplementationslikeHKUDS/Auto-Deep-Research[112]employsimplerbutusefulcontradictioniden-\ntification approaches, flagging potential inconsistencies for user review. These implementations demonstrate\nthat even basic contradiction handling can significantly enhance research reliability.\n4.4.2 Privacy Protection and Security Design. Research systems must safeguard sensitive information and\nprotect against potential misuse:\nQuery and Result Isolation. Secure implementations employ strict isolation between user queries to\nprevent information leakage. Commercial platforms implement sophisticated tenant isolation that ensures\ncomplete separation between different users’ research activities. Similar concerns motivate open-source\nimplementations like OpenManus [193], which enables local deployment for sensitive research applications.\nSource Data Protection. Responsible implementation requires careful handling of source information.\nSystemslikeFlowith/OracleMode[77]implementcontrolleddataaccesspatternsthatrespectsourcerestric-\ntions including authentication requirements and access limitations. These approaches enhance compliance\nwith source terms of service while ensuring comprehensive information access. Recent advancements include\nbenchmarking frameworks such as CI-Bench [56], which evaluates how well systems adhere to contextual\nnorms and privacy expectations.\n4.4.3 Explainability and Transparency. The scientific context places particularly stringent requirements on\nexplanation quality. Mengaldo [170] argues that transparent explanation is not merely a feature but a\nfundamental requirement for scientific applications, emphasizing that black-box approaches fundamentally\ncontradict scientific methodology’s requirement for transparent reasoning and reproducible results. This\nperspectivesuggeststhatexplanationcapabilitiesmayrequiredifferentstandardsinscientificDeepResearch\napplications compared to general AI systems. Trustworthy research systems must provide insight into their\nreasoning processes and sources:\nReasoning Trail Documentation. Advanced implementations maintain explicit documentation of the\nreasoning process. OpenAI/DeepResearch [197] includes comprehensive reasoning traces that expose the\nanalytical steps leading to specific conclusions. Similar capabilities are emerging in open-source alternatives\nlike mshumer/OpenDeepResearcher [249], which includes basic reasoning documentation to enhance result\ninterpretability.\nSource Attribution and Verification. Transparent systems provide clear attribution for all information\nand enable verification. Perplexity/DeepResearch [209] implements comprehensive citation practices with\nexplicit links to original sources, enabling direct verification of all claims. Similar approaches are employed\nby dzhng/deep-research [321], which maintains rigorous source tracking throughout the research process.\nThese implementation technologies and challenges highlight the complex engineering considerations\ninvolved in creating effective Deep Research systems. While commercial platforms benefit from extensive\ninfrastructure and specialized components, open-source implementations demonstrate that effective research\ncapabilitiescanbeachievedthroughpragmaticapproachestothesamefundamentalchallenges.Thediversity\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 35\nofimplementationstrategiesacrosstheecosystemreflectsdifferentprioritiesinbalancingcapability,efficiency,\nreliability, and accessibility.\n5 Evaluation Methodologies and Benchmarks\nRigorous evaluation of Deep Research systems presents unique challenges due to their complex capabilities\nand diverse application contexts. This section examines established frameworks for assessment, identifies\nemerging evaluation standards, and analyzes the strengths and limitations of current approaches.\nMulti-dimensional Evaluation Framework for Deep Research Systems\nHuman Assessment\nExpert evaluation\nFunctional Evaluation Cross-Domain Evaluation\nUser experience studies\nSUS scores, interviews\nTask Completion Information Retrieval Academic Research Business Intelligence\nWebArena Precision/Recall LitReview benchmark MarketInsight\nMobileArena TREC benchmarks MethodEval FinEval\nDeep Research\nEvaluation\nComprehensive Assessment\nNon-Functional Evaluation Emerging Evaluation\nPerformance Reliability Interactive Evaluation Multimodal Research\nResponsetime Errorrates Multi-turnassessment Cross-modalintegration\nBenchmark Assessment\nResource utilization Stability metrics Feedback incorporation Visual information\nStandardized tests\nMMLU, HLE, HotpotQA, GAIA\nComparative scoring\nFig.8. Multi-dimensionalEvaluationFrameworkforDeepResearchSystems\n5.1 Functional Evaluation Frameworks\nFunctional evaluation assesses core capabilities essential to effective research performance.\n5.1.1 Task Completion Capability Assessment. The ability to successfully complete research tasks represents\na fundamental evaluation dimension:\nTask Success Rate Metrics. Quantitative assessment of task completion provides objective performance\nmeasures. Standardized evaluation suites like WebArena [332] measure successful completion of web-based\n36 Xu et al.\nresearchtasks.Forinstance,AutoGLM[330]achievesa55.2%successrateonVAB-WebArena-Lite(improving\nto 59.1% on a second attempt) and 96.2% on OpenTable evaluation tasks. Similarly, benchmarks like\nMobileArena evaluate successful completion of mobile interface tasks, where AutoGLM [330] demonstrates a\n36.2% success rate on AndroidLab and 89.7% on common tasks in popular Chinese apps [153]. Domain-\nspecificbenchmarks,suchasAutoPenBenchforgenerativeagentsinpenetrationtesting[85],providefurther\ntargeted assessments. These benchmarks provide meaningful comparative metrics, though with limitations\nin representing real-world research complexity.\nThese benchmarks provide meaningful comparative metrics, though with limitations in representing real-\nworldresearchcomplexity.Perplexity/DeepResearch[209]explicitlyhighlightsthisdistinction,notingthat\nwhile benchmark performance provides comparative indicators, practical effectiveness depends significantly\non task characteristics and domain specifics.\nMulti-Attempt Resolution Rates. Effective research often involves iterative refinement with multiple\nattempts. Advanced evaluation frameworks incorporate multi-attempt metrics that assess system resilience\nand adaptability. AutoGLM [154] demonstrates significant performance improvement with second attempts\n(55.2% to 59.1% on WebArena-Lite), highlighting the importance of error recovery and adaptive strategies\nin practical research contexts.\nOpen-source frameworks like Agent-RL/ReSearch [2] explicitly emphasize iterative improvement through\nreinforcementlearningapproaches,demonstratinghowevaluationmethodsthatconsideradaptabilityprovide\nmore comprehensive assessment than single-attempt metrics alone.\n5.1.2 Information Retrieval Quality Evaluation. Effective information gathering forms the foundation of\nsuccessful research:\nSearch Effectiveness Metrics. Information retrieval quality significantly impacts overall research per-\nformance. Evaluation frameworks employ metrics including precision (relevance of retrieved informa-\ntion), recall (comprehensiveness of coverage), and F1 scores (balanced measure of both). Systems like\nPerplexity/DeepResearch [209] demonstrate particular strength in recall metrics, effectively identifying\ncomprehensive information across diverse sources.\nSpecialized information retrieval benchmarks like TREC [214] provide standardized assessment of search\neffectiveness. However, to the best of our knowledge, there is no specific evidence that the Deep Research\nsystems from OpenAI, Google, Perplexity, or any of the open-source projects listed in this survey have\nbeen formally evaluated on TREC benchmarks [214] . This limitation motivates domain-specific evaluation\napproaches that better reflect particular research requirements.\nSource Diversity Assessment. Comprehensive research requires balanced information from diverse per-\nspectives and sources. Advanced evaluation frameworks incorporate explicit diversity metrics that assess the\nbreadthofsourceutilization.CommercialsystemslikeGemini/DeepResearch[60]emphasizesourcediversity\nas a key performance indicator, while open implementations like dzhng/deep-research [321] incorporate\nspecific mechanisms to ensure balanced source consideration.\nEmerging evaluation approaches include explicit source spectra analysis that examines distribution\nacross domains, perspectives, and publication types. These methods provide more nuanced assessment of\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 37\ninformation gathering quality beyond simple relevance metrics, addressing concerns about potential bias in\nautomated research processes.\n5.1.3 Knowledge Synthesis Accuracy Assessment. Transforming information into accurate, coherent insights\nrepresents a crucial capability:\nFactual Consistency Metrics. Reliable research requires accurate synthesis without introducing errors\nor misrepresentations. Evaluation frameworks employ fact verification techniques that compare generated\ncontent against source materials, identifying potential inaccuracies or unsupported claims. Systems like\ngrapeot/deep_research_agent[263]emphasizefactualverificationthroughexplicitsourcelinking,enabling\ndirectaccuracyassessment.BenchmarksuiteslikeTruthfulQA[151]assessthetruthfulnessoflanguagemodels\nunderchallengingconditions.WhilespecificaccuracyfiguresforOpenAI/DeepResearch[197]andPerplexity/\nDeepResearch[209]onTruthfulQA[151]arenotpubliclyavailable,thesesystemshavedemonstratednotable\nperformance on other rigorous benchmarks. For instance, OpenAI/DeepResearch [197] achieved a 26.6%\naccuracy[197]onHumanity’sLastExam(HLE)[212].Similarly,Perplexity/DeepResearch[209]attaineda\n21.1%accuracy[209]onthesamebenchmark.Thedevelopmentofunified,fine-grained,andmulti-dimensional\nevaluation frameworks for summarization further advances the ability to assess the quality of synthesized\ncontent from LLMs [137]. These metrics provide standardized comparison points, though with recognized\nlimitations in representing the complexity of real-world research synthesis.\nLogical Coherence Assessment. Effective research requires logically sound integration of information into\ncoherent analyses. Sophisticated evaluation approaches employ reasoning validity assessment that examines\nlogical structures and inference patterns in research outputs. This dimension proves particularly challenging\nfor automated assessment, often requiring expert human evaluation for reliable scoring.\nCommercial systems like OpenAI/DeepResearch [197] and Gemini/DeepResearch [60] emphasize logical\ncoherence in their evaluation frameworks, while open-source alternatives like mshumer/OpenDeepResearcher\n[249]incorporatesimplifiedbutusefullogicalconsistencychecks.Theseapproacheshighlighttheimportance\nof sound reasoning in effective research outputs beyond simple factual accuracy.\n5.2 Non-Functional Evaluation Metrics\nBeyondcorefunctionality,practicaleffectivenessdependsonoperationalcharacteristicsthatimpactusability\nand deployment.\n5.2.1 Performance and Efficiency Metrics. Operational efficiency significantly impacts practical utility:\nResponse Time Profiling. Timeliness represents a crucial dimension of research effectiveness. Evaluation\nframeworks incorporate response time metrics that measure completion duration across standardized tasks.\nCommercialsystemsdemonstratevaryingperformancecharacteristics,withPerplexity/DeepResearch[209]\nachievingrelativelyquickresponsetimes(2-5minutesformoderatetasks)whileOpenAI/DeepResearch[197]\ntypically requires longer processing (5-10 minutes) for similar complexity.\nOpen-source implementations generally demonstrate longer response times, though with significant\nvariation based on implementation approaches and deployment environments. Systems like nickscamara/\n38 Xu et al.\nopen-deep-research[42]emphasizeaccessibilityoverperformanceoptimization,whileQwenLM/Qwen-Agent\n[224] incorporates specific optimizations to enhance response times within resource constraints.\nResource Utilization Assessment. Computational efficiency enables broader deployment and accessibility.\nComprehensive evaluation includes resource profiling that measures memory consumption, computational\nrequirements, and energy utilization across standardized workloads. Specialized benchmarks like Minerva\nassess programmable memory capabilities of language models, offering insights into their efficiency in\nhandlinglong-contextinformation[300].Commercialcloud-basedsystemsobscuresomeofthesemetricsdue\nto their managed infrastructure, though with operational costs providing indirect resource indicators. Open\nimplementations like Camel-AI/OWL [43] and AutoGLM-Research [330] provide more transparent resource\nprofiles, enabling direct assessment of deployment requirements and operational economics. These metrics\nhighlight significant variation in efficiency across the ecosystem, with implications for practical deployment\nscenarios and accessibility.\n5.2.2 Reliability and Stability Metrics. Consistent performance under diverse conditions ensures practical\nusability:\nErrorRateAnalysis. Reliabilityunderchallengingconditionssignificantlyimpactsusertrustandadoption.\nRobust evaluation frameworks incorporate error rate metrics that measure failure frequency across diverse\nscenarios.Commercialsystemsgenerallydemonstratelowererrorratescomparedtoopen-sourcealternatives,\nthough with remaining challenges in complex or novel research contexts.\nSpecialized reliability testing employs adversarial scenarios designed to trigger failure modes, providing\ninsight into system robustness. Systems like OpenAI/DeepResearch [197] and Agent-RL/ReSearch [2] incor-\nporate explicit error recovery mechanisms that enhance reliability under challenging conditions, highlighting\nthe importance of resilience in practical research applications.\nLong-Term Stability Assessment. Consistentperformanceoverextendedoperationprovidescrucialdeploy-\nment confidence. Comprehensive evaluation includes stability metrics that measure performance consistency\nacrossextendedsessionsandrepeatedexecutions.Thisdimensionprovesparticularlyrelevantforopen-source\nimplementationsthatmustoperateindiversedeploymentenvironmentswithvaryinginfrastructurestability.\nSystems like Flowith/OracleMode [77] and TARS [39] emphasize operational stability through robust\nerror handling and recovery mechanisms, enabling reliable performance in production environments. These\ncapabilitieshighlighttheimportanceofengineeringqualitybeyondcorealgorithmicperformanceinpractical\nresearch applications.\n5.2.3 User Experience and Usability Metrics. Effective interaction significantly impacts practical utility:\nInterface Usability Assessment. Intuitiveinterfacesenhanceaccessibilityandeffectiveutilization.Usability\nevaluation frameworks employ standardized usability metrics including System Usability Scale (SUS) [140]\nscoresandtaskcompletiontimemeasurements.Commercialsystemstypicallydemonstratestrongerusability\ncharacteristics, with Perplexity/DeepResearch [209] particularly emphasizing intuitive interaction for non-\ntechnical users. Open-source alternatives show greater variability, with implementations like HKUDS/Auto-\nDeep-Research [112] incorporating specific interface enhancements to improve accessibility.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 39\nUser studies provide more nuanced usability assessment beyond standardized metrics. Evaluations\nof systems like Manus [164] and Flowith/OracleMode [77] incorporate explicit user feedback to identify\ninteractionchallengesandimprovementopportunities.Theseapproacheshighlighttheimportanceofhuman-\ncentered design in practical research applications beyond technical performance. Similarly, frameworks such\nasAdaptoML-UX[87]enableHCIresearcherstoemployautomatedMLpipelineswithoutspecializedexpertise,\nfacilitating robust model development and customization.\nLearning Curve Assessment. Approachability for new users significantly impacts adoption and effective\nutilization. Comprehensive evaluation includes learning curve metrics that measure time-to-proficiency\nacrossusersegmentswithvaryingtechnicalbackgrounds.Commercialsystemsgenerallydemonstrategentler\nlearning curves, with Perplexity/DeepResearch [209] explicitly designed for accessibility to non-technical\nusers.\nOpen implementations show greater variability, with systems like n8n [183] requiring more technical\nexpertise for effective deployment and utilization. More accessible alternatives like nickscamara/open-\ndeep-research [42] incorporate simplified interfaces designed for broader accessibility, highlighting diverse\napproaches to the accessibility-sophistication balance across the ecosystem.\n5.3 Cross-Domain Evaluation Benchmarks\nStandardized benchmarks enable objective comparison across systems and domains.\n5.3.1 Academic Research Task Benchmarks. Specialized benchmarks assess capabilities relevant to scholarly\nresearch:\nLiterature Review Benchmarks. Comprehensive literature synthesis represents a fundamental academic\nresearch task requiring sophisticated information retrieval, critical analysis, and synthesis capabilities. To\nthebestofourknowledge,nobenchmarksuiteisspecificallydesignedtoevaluatesystems’abilitytoidentify\nrelevantliterature,synthesizekeyfindings,andhighlightresearchgapsacrossscientificdomains.Wepropose\nleveraging existing high-quality literature reviews published in Nature Reviews journals as gold standards.\nCitation networks from academic knowledge graphs—such as Microsoft Academic Graph, Semantic Scholar\nAcademic Graph, and Open Academic Graph—could provide complementary evaluation data by measuring\na system’s ability to traverse citation relationships and identify seminal works[1, 31].\nWhile direct literature review benchmarks remain underdeveloped, several indirect benchmarks offer\ninsight into related capabilities. OpenAI/DeepResearch [197] demonstrates leading performance, achieving\n26.6% accuracy on Humanity’s Last Exam (HLE) [212] and averaging 72.57% on the GAIA benchmark\n[172], reflecting strong performance in complex reasoning tasks essential for literature synthesis. Similarly,\nPerplexity/DeepResearch [209] achieves 21.1% accuracy on HLE [212] and 93.9% on SimpleQA [290],\nindicating robust factual retrieval capabilities.\nThesebenchmarksincludechallengingcasesrequiringintegrationacrossmultipledisciplines,identification\nof methodological limitations, and disambiguation of conflicting findings—all crucial for effective literature\nreview. Such tasks demonstrate the importance of sophisticated reasoning capabilities beyond simple\ninformationretrieval.WhilespecificperformancemetricsforsystemslikeCamel-AI/OWL[43]arenotpublicly\n40 Xu et al.\navailable, their specialized academic optimization suggests potential effectiveness in handling complex\nsynthesis tasks.\nMethodology Evaluation Benchmarks. Critical assessment of research methodology requires sophisticated\nanalytical capabilities. To the best of our knowledge, no benchmark is specifically designed for quantitative\nmethodology assessment of strengths and limitations. A comprehensive methodology evaluation benchmark\nwould need to assess a system’s ability to identify flaws in research design, statistical approaches, sampling\nmethods, and interpretive limitations across diverse disciplines. An effective benchmark might incorporate\nmulti-layeredevaluationcriteriaincluding:reproducibilityassessment,identificationofconfoundingvariables,\nappropriate statistical power analysis, and proper handling of uncertainty. Future benchmarks could utilize\nexpert-annotated corpora of research papers with methodological strengths and weaknesses clearly marked,\ncreating a gold standard against which systems’ analytical capabilities can be measured while minimizing\nbias through diverse evaluation metrics that reflect methodological best practices across different fields of\ninquiry.\nBeyond standard benchmarks, case study evaluations of complete AI scientist systems provide valuable\ninsights into current capabilities. Beel et al. [24] conduct a detailed assessment of Sakana’s AI Scientist\nfor autonomous research, examining whether current implementations represent genuine progress toward\n“ArtificialResearchIntelligence”orremainlimitedinfundamentalways,highlightingthegapbetweencurrent\nbenchmarks and comprehensive research capability evaluation.\n5.3.2 Business Analysis Task Benchmarks. Standardized evaluation for business intelligence applications:\nMarket Analysis Benchmarks. Strategic decision support necessitates a comprehensive understanding\nof market dynamics. Advanced AI systems, such as OpenAI/DeepResearch [197], are designed to analyze\ncompetitive landscapes, identify market trends, and generate strategic recommendations based on diverse\nbusiness information. OpenAI/DeepResearch has demonstrated significant capabilities in handling complex,\nmulti-domain data analysis tasks, providing detailed insights and personalized recommendations. Similarly,\nGoogle’s Gemini/DeepResearch [60] offers robust performance in processing extensive datasets, delivering\nconcise and factual reports efficiently.\nThese benchmarks include challenging scenarios requiring integration of quantitative financial data with\nqualitative market dynamics and regulatory considerations. Such tasks highlight the importance of both\nanalytical depth and domain knowledge, with systems like Manus [164] demonstrating strong performance\nthrough specialized business intelligence capabilities.\nFinancial Analysis Benchmarks. Economic assessment requires sophisticated quantitative reasoning\ncombined with contextual understanding of market dynamics. The FinEval benchmark [103] provides a\nstandardized framework for measuring systems’ capabilities in analyzing financial statements, evaluating\ninvestment opportunities, and assessing economic risk factors across diverse scenarios. To our knowledge, no\nDeep Research projects have yet published official FinEval benchmark results, though several commercial\ndemonstrations suggest strong performance in this domain. OpenAI/DeepResearch [197] has demonstrated\nparticular strength in quantitative financial analysis through its ability to process complex numerical\ndata while incorporating relevant market context. Meanwhile, open-source implementations show more\nvariableperformance,thoughspecializedsystemsliken8n[183]achievecompetitiveresultsthroughstrategic\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 41\nintegration with financial data sources and analytical tools. These patterns highlight the critical importance\nof domain-specific integrations and data accessibility in financial analysis applications, extending beyond\ncore language model capabilities to create truly effective analytical systems.\n5.3.3 General Knowledge Management Benchmarks. Broad applicability assessment across general research\ndomains:\nFactual Research Benchmarks. Accurate information gathering forms the foundation of effective research.\nThe SimpleQA benchmark [290] evaluates language models’ ability to answer short, fact-seeking questions\nwith a single, indisputable answer. Perplexity/DeepResearch [209] demonstrates exceptional performance\non this benchmark, achieving an accuracy of 93.9% [209]. OpenAI’s Deep Research tool, integrated into\nChatGPT,offerscomprehensiveresearchcapabilities,thoughspecificaccuracymetricsonSimpleQA[290]are\nnotpubliclydisclosed[197].Similarly,Google’sGemini/DeepResearchprovidesrobustinformationsynthesis\nfeatures, but detailed performance data on SimpleQA [290] is not available.\nThesemetricsprovideusefulbaselineperformanceindicators,thoughwithrecognizedlimitationsinrepre-\nsenting more complex research workflows. Comparative evaluation highlights the importance of information\nquality beyond simple factual recall, with sophisticated systems demonstrating more nuanced performance\nprofiles across complex tasks.\nHumanitiesandSocialSciencesBenchmarks. ComprehensiveevaluationrequiresassessmentbeyondSTEM\ndomains. The MMLU benchmark [33] evaluates systems’ performance across humanities and social science\nresearch tasks, including historical analysis, ethical evaluation, and social trend identification. Performance\nshows greater variability compared to STEM-focused tasks, with generally lower accuracy across all systems\nwhile maintaining similar relative performance patterns. These benchmarks highlight remaining challenges\nin domains requiring nuanced contextual understanding and interpretive reasoning. Commercial systems\nmaintain performance leads, though with open alternatives like smolagents/open_deep_research [115]\ndemonstratingcompetitivecapabilitiesinspecifichumanitiesdomainsthroughspecializedcomponentdesign.\n5.4 Emerging Evaluation Approaches\nBeyond established benchmarks, novel evaluation methods address unique aspects of Deep Research perfor-\nmance.\nInteractive Evaluation Frameworks. Traditional static benchmarks often fail to capture the dynamic and\ninteractive nature of real-world research workflows. To address this gap, interactive evaluation frameworks\nhave been developed to assess AI systems’ abilities to iteratively refine research strategies through multiple\ninteractionrounds.Notably,QuestBench[141]isanovelbenchmarkwhichspecificallyassessesanAIsystem’s\nabilitytoidentifymissinginformationandaskappropriateclarificationquestions,acrucialskillforreal-world\nresearch scenarios where problems are often underspecified. To the best of our knowledge, no deep research\nsysteminvestedinthissurveyhasyetbeenpubliclyevaluatedusingQuestBench.Nonetheless,thesesystems\nhave demonstrated strong performance in other interactive evaluations, highlighting their effectiveness in\nsupporting iterative research processes.\n42 Xu et al.\nMultimodal Research Evaluation. Comprehensive research increasingly involves diverse content modalities.\nAdvanced evaluation frameworks incorporate multimodal assessment that measures systems’ ability to\nintegrate information across text, images, data visualizations, and structured content. Commercial systems\ngenerallydemonstratestrongermultimodalcapabilities,withGemini/DeepResearch[60]particularlyexcelling\nin image-inclusive research tasks.\nOpenimplementationsshowemergingmultimodalcapabilities,withsystemslikeJina-AI/node-DeepResearch\n[121] incorporating specific components for multimodal content processing. These approaches highlight\nthe growing importance of cross-modal integration in practical research applications beyond text-centric\nevaluation.\nEthical and Bias Assessment. Responsible research requires careful attention to ethical considerations\nand potential biases. Comprehensive evaluation increasingly incorporates explicit assessment of ethical\nawareness, bias detection, and fairness in information processing. Commercial systems implement sophis-\nticated safeguards, with OpenAI/DeepResearch [197] incorporating explicit ethical guidelines and bias\nmitigation strategies. Open implementations show varied approaches to these considerations, with systems\nlike grapeot/deep_research_agent [263] emphasizing transparency in source selection and attribution.\nThese evaluation dimensions highlight the importance of responsibility beyond technical performance,\naddressing growing concerns about potential amplification of existing information biases through automated\nresearch systems. Ongoing development of standardized ethical evaluation frameworks represents an active\narea of research with significant implications for system design and deployment.\nThediverseevaluationapproachesoutlinedinthissectionhighlightboththecomplexityofcomprehensive\nassessment and the ongoing evolution of evaluation methodologies alongside system capabilities. While\nstandard benchmarks provide useful comparative metrics, practical effectiveness depends on alignment\nbetween system capabilities, evaluation criteria, and specific application requirements. This alignment\nrepresents a key consideration for both system developers and adopters seeking to integrate Deep Research\ncapabilities into practical workflows.\n5.5 Comparative Evaluation Methodology\nTo ensure systematic and consistent evaluation across diverse Deep Research systems, we have developed a\ncomprehensive evaluation framework. This section outlines our methodological approach, evaluation criteria\nselection, and application consistency across systems.\n5.5.1 Systems Selection Criteria. Our evaluation encompasses various Deep Research systems selected based\non the following criteria:\n∙ FunctionalCompleteness: Systems must implement at least two of the three core dimensions of Deep\nResearch as defined in Section 1.1\n∙ PublicDocumentation: Sufficient technical documentation must be available to enable meaningful\nanalysis\n∙ ActiveDevelopment: Systems must have demonstrated active development or usage within the past\n12 months\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 43\n∙ Representational Balance: Selection ensures balanced representation of commercial, open-source,\ngeneral-purpose, and domain-specialized implementations\n5.5.2 Evaluation Dimensions and Metrics Application. Our evaluation employs a consistent set of dimensions\nacross all systems, though the specific benchmarks within each dimension vary based on system focus and\navailable performance data. Table 15 presents the evaluation coverage across representative systems.\nTable15. EvaluationMetricsApplicationAcrossSystems\nSystem Functional Performance Efficiency Domain-Specific Usability\nBenchmarks Metrics Metrics Benchmarks Assessment\nOpenAI/DeepResearch HLE,GAIA Factualaccuracy Responsetime Academiccitation Userinterface\nGemini/DeepResearch MMLU Outputcoherence Cloudcompute Marketanalysis Mobilesupport\nPerplexity/DeepResearch HLE,SimpleQA Sourcediversity Responsetime Legalsearch Multi-device\nGrok3Beta MMLU Sourceverification Cloudefficiency Financialanalysis Voiceinterface\nManus GAIA Cross-domain APIlatency Businessanalysis Dashboard\nAgent-RL/ReSearch HotpotQA Planningefficiency Localcompute Scientificresearch CLIinterface\nAutoGLM-Research WebArena GUInavigation Mobileefficiency Domainadaptation Accessibility\nn8n Workflow APIintegration Self-hosted Enterpriseworkflow No-codedesign\n5.5.3 Data Collection Methods. Our evaluation data comes from four primary sources:\n(1) PublishedBenchmarks: Performance metrics reported in peer-reviewed literature or official system\ndocumentation\n(2) TechnicalDocumentationAnalysis: Capabilities and limitations outlined in official documentation,\nAPIs, and technical specifications\n(3) Repository Examination: Analysis of open-source code repositories for architectural patterns and\nimplementation approaches\n(4) Experimental Verification: Where inconsistencies exist, we conducted direct testing of publicly\navailable systems to verify capabilities\nWhen benchmark results are unavailable for specific systems, we indicate this gap explicitly rather than\nextrapolating performance. This approach ensures transparency regarding the limits of our comparative\nanalysis while maintaining the integrity of available evaluation data.\n5.5.4 Cross-System Comparison Challenges. Several methodological challenges exist in comparing Deep\nResearch systems:\n∙ BenchmarkDiversity: Different systems emphasize different benchmarks based on their focus areas\n∙ Implementation Transparency: Commercial systems often provide limited details about internal\narchitectures\n∙ RapidEvolution:Systemsundergofrequentupdates,potentiallyrenderingspecificbenchmarkresults\nobsolete\n∙ Domain Specialization: Domain-specific systems excel on targeted benchmarks but may perform\npoorly on general evaluations\n44 Xu et al.\nWe address these challenges through qualitative architectural analysis alongside quantitative benchmarks,\nenablingmeaningfulcomparisondespitedatalimitations.Section3.3presentstheresultingcomparativeanal-\nysis,highlightingbothperformancedifferentialsandthelimitationsofdirectcomparisonacrossheterogeneous\nimplementations.\n6 Applications and Use Cases\nThe technical capabilities of Deep Research systems enable transformative applications across diverse\ndomains. This section examines implementation patterns, domain-specific adaptations, and representative\nuse cases that demonstrate the practical impact of these technologies.\nDeep Research Application Domains and Use Cases\nAcademic Research Business Intelligence\nLiterature Review Synthesis\nMarket Research\nOpenAI DR, Perplexity DR\nGemini DR, n8n\nHypothesis Generation\nCamel-AI/OWL\nStrategic Decision Support\nManus, OpenAI DR\nInterdisciplinary Research\nsmolagents/open_deep_research\nDeep Research\nSystems Financial Analysis\nEducational Applications\nInvestment Research, Risk Assessment\nLearning Support, Content Development\nPerplexity DR, OpenAI DR\nHKUDS/Auto-Deep-Research\nScientific Discovery Personal Knowledge\nData Analysis Patterns Information Organization\nn8n, Gemini DR Perplexity DR, nickscamara\nExperiment Design Personal Learning\nAgent-RL/ReSearch OpenManus\nScientific Literature Integration Decision Support\njina-ai/node-DeepResearch Gemini DR, Agent-RL/ReSearch\nApplication Domains\nDomain Use Case\nFig.9. DeepResearchApplicationDomainsandUseCases\n6.1 Academic Research Applications\nDeep Research systems offer significant enhancements to scholarly research workflows.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 45\n6.1.1 Literature Review and Synthesis. Comprehensive literature analysis forms the foundation of effective\nresearch:\nSystematic Review Automation. Deep Research systems demonstrate particular effectiveness for system-\natic literature reviews requiring exhaustive coverage of existing research. Systems like Google’s Gemini/\nDeepResearch [60] can efficiently analyze thousands of research papers, a capability that has significant\nimplications for fields like biomedicine where the volume of literature makes comprehensive manual re-\nview increasingly challenging [289]. OpenAI/DeepResearch [197] has been successfully deployed for medical\nresearch reviews, analyzing thousands of publications to identify intervention efficacy patterns with sig-\nnificantly reduced human effort compared to traditional methods. Similar capabilities are evident in\nPerplexity/DeepResearch [209] and Gemini/DeepResearch [60], which enables rapid synthesis of research\nfindingsacrossdisciplinaryboundaries.GenerativeAIframeworksintegratingretrieval-augmentedgeneration\nfurther automate systematic reviews by expanding user queries to retrieve relevant scholarly articles and\nreduce time and resource burdens [234].\nOpen-source implementations like dzhng/deep-research [321] have found adoption in academic settings\nwhere local deployment and customization are prioritized. Specialized scientific implementations like AI-\nResearcher [109] extend these capabilities with domain-specific optimizations for academic literature\nprocessing and analysis. These systems enable literature review automation with greater control over search\nscopeandsynthesismethods,particularlyvaluableforspecializedresearchdomainswithuniquerequirements.\nImplementation patterns typically involve customization of search strategies, source weightings, and output\nformats to align with disciplinary conventions.\nResearch Gap Identification. Beyond simple synthesis, advanced systems effectively identify unexplored\nareas and research opportunities. Gemini/DeepResearch [60] has demonstrated this capability in inter-\ndisciplinary contexts, identifying connection opportunities between distinct research domains that might\notherwiseremainundiscovered.Thisapplicationleveragesthesystem’sabilitytoprocessextensiveliterature\nacross fields while identifying patterns and absences in existing research coverage.\nOpen implementations like HKUDS/Auto-Deep-Research [112] incorporate specific mechanisms for gap\nanalysis, including explicit detection of methodological limitations and underexplored variables across\nresearch corpora. These capabilities highlight the potential for automated systems to not only synthesize\nexisting knowledge but actively contribute to research direction through systematic gap identification.\n6.1.2 Hypothesis Generation and Testing. AI-assisted hypothesis development enhances research creativity\nand validation:\nHypothesis Formulation Support. Deep Research systems effectively generate testable hypotheses based\non existing literature and theoretical frameworks. OpenAI/DeepResearch [197] provides explicit hypothesis\ngeneration capabilities, identifying potential causal relationships and testable predictions derived from\nliterature synthesis. These features enable researchers to explore broader possibility spaces than might be\npractical through manual review alone.\nSpecialized frameworks like Camel-AI/OWL [43] implement domain-specific hypothesis generation for\nscientific applications, incorporating field-specific constraints and validation criteria. These approaches\nhighlight how domain adaptation enhances the practical utility of hypothesis generation capabilities beyond\n46 Xu et al.\ngeneric formulation. Implementation patterns typically involve iterative refinement with researcher feedback\nto align generated hypotheses with specific research objectives.\nPreliminary Validation Assessment. Advanced systems support hypothesis validation through evidence\nassessment and methodological planning. Gemini/DeepResearch [60] enables preliminary hypothesis testing\nthroughautomateddatasourceidentification,statisticalpoweranalysis,andpotentialconfoundidentification.\nThesecapabilitiesstreamlinethetransitionfromhypothesisformulationtoempiricaltesting,reducingmanual\neffort in research design.\nOpen implementations like Agent-RL/ReSearch [2] incorporate specific validation planning components,\nguiding researchers through experimental design considerations based on hypothesis characteristics. These\napproaches demonstrate how Deep Research capabilities extend beyond information gathering to actively\nsupport the complete research workflow from conception through validation planning.\n6.1.3 Interdisciplinary Research Support. Cross-domain integration represents a particular strength of auto-\nmated research systems:\nCross-Domain Knowledge Translation. Deep Research systems effectively bridge terminological and\nconceptual gaps between disciplines. Perplexity/DeepResearch [209] demonstrates this capability through\nexplicitconceptmappingbetweenfields,enablingresearchersfromdiversebackgroundstoexploreunfamiliar\ndomains with reduced onboarding barriers. This application leverages the system’s broad knowledge base to\nidentify conceptual parallels across disciplinary boundaries.\nOpenframeworkslikesmolagents/open_deep_research[115]implementspecializedagentsfordisciplinary\ntranslation,withexplicitfocusonterminologicalmappingandconceptalignment.Theseapproacheshighlight\nhow multi-agent architectures can effectively address the challenges of interdisciplinary communication\nthrough specialized component design[117].\nMethodology Transfer Facilitation. Advanced systems enable effective adaptation of research methods\nacross domains. OpenAI/DeepResearch [197] supports methodology transfer through explicit identification\nof adaptation requirements and implementation guidance when applying techniques from one field to\nanother. This capability accelerates methodological innovation by facilitating cross-pollination between\nresearch traditions. Implementation patterns typically involve specialized methodological components like\nthose in QwenLM/Qwen-Agent [224], which incorporates explicit methodology modeling to identify transfer\nopportunities and adaptation requirements. This is particularly relevant in fields like engineering, where AI\nis beginning to impact established design procedures for complex dynamical systems [67]. These approaches\ndemonstratehowDeepResearchsystemscanactivelycontributetomethodologicalinnovationbeyondsimple\ninformation retrieval and synthesis.\n6.2 Scientific Discovery Applications\nDeep Research technologies enable enhanced scientific investigation across disciplines.\n6.2.1 Data Analysis and Pattern Recognition. Automated analysis enhances insight extraction from complex\nscientific data:\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 47\nLarge-Scale Data Synthesis. DeepResearchsystemseffectivelyintegratefindingsacrossextensivedatasets\nto identify broader patterns. Gemini/DeepResearch [60] has been applied to climate science research,\nsynthesizing findings across hundreds of climate models and observational datasets to identify consistent\npatterns and outliers. This application leverages the system’s ability to process and integrate diverse\ndata formats while maintaining analytical coherence. Open implementations like n8n [183] enable similar\ncapabilities through workflow automation that coordinates specialized analytical tools across complex data\nprocessing pipelines. Furthermore, SqlCompose [161] enhances analytical workflows by automating SQL\nauthoring to reduce syntax barriers and improve efficiency in large-scale data operations, as demonstrated\nthroughenterprisedeploymentanduserfeedback.SystemslikeDataInquirerquantitativelymeasureworkflow\npatterns and task execution consistency, revealing significant variations across practitioners while also\nassessingAItoolimpactsonaligningnoviceapproacheswithexpertpractices[325].AIassistantsspecifically\ndesigned for data wrangling tasks can provide semi-automated support in transforming and cleaning data\nthrough interactive recommendations, thereby enhancing workflow efficiency [211]. Other systems assist\ndomain experts in making sense of multi-modal personal tracking data through visualization and human-\nin-the-loop LLM agents [143]. Additionally, no-code machine-readable documentation frameworks support\nresponsible dataset evaluation by facilitating quality assessment and accuracy verification during large-scale\ndata synthesis [233]. These approaches demonstrate how tool integration capabilities extend analytical\nreach beyond the core language model’s native capabilities, particularly valuable for quantitative scientific\napplications.\nAnomaly Detection and Investigation. Advanced systems effectively identify unexpected patterns and\nfacilitatetargetedinvestigation.OpenAI/DeepResearch[197]demonstratesthiscapabilityinpharmacological\ncontexts,identifyingunexpecteddruginteractionpatternsacrossclinicalliteratureandproposingmechanistic\nexplanations for further investigation. This application combines pattern recognition with explanatory\nhypothesis generation to enhance scientific discovery.\nSpecialized tools like grapeot/deep_research_agent [263] implement focused anomaly detection capa-\nbilities, with particular emphasis on statistical outlier identification and contextual explanation. These\napproaches highlight how targeted optimization can enhance specific scientific workflows beyond general-\npurpose research capabilities[125].\n6.2.2 Experiment Design and Simulation. AI assistance enhances experimental planning and virtual testing:\nExperimental Protocol Optimization. Deep Research systems support experimental design through com-\nprehensive protocol development and optimization. Gemini/DeepResearch [60] provides explicit protocol\ngeneration capabilities, incorporating existing methodological best practices while identifying potential\nconfounds and control strategies. These features streamline experimental planning while enhancing method-\nological rigor.\nOpen implementations like Agent-RL/ReSearch [2] incorporate specialized experimental design compo-\nnents with particular emphasis on statistical power optimization and confound control. These approaches\ndemonstrate how focused optimization can enhance specific scientific workflows through specialized compo-\nnent design targeting critical research phases.\n48 Xu et al.\nDespitethesecapabilities,significantgapsremainbetweencurrentsystemsandtrulyautonomousscientific\ndiscovery. Yu et al. [314] identify critical missing elements in current AI research systems, particularly\nhighlighting limitations in open-ended exploration, creative hypothesis generation, and experimental design\noptimization that constrain their effectiveness in leading scientific discovery processes.\nTheoretical Model Testing. Advanced systems enable accelerated testing of theoretical models through\nsimulation and virtual experimentation. OpenAI/DeepResearch [197] supports this application through\nintegration with computational modeling tools, enabling rapid assessment of theoretical predictions against\nexisting evidence. This capability accelerates theory refinement by identifying empirical constraints and\nvalidation opportunities more efficiently than manual methods.\nImplementation patterns typically involve specialized tool integration like that found in Manus [164],\nwhich provides sophisticated orchestration of computational modeling and simulation tools within research\nworkflows. Systems like AgentLaboratory [237] further enhance these capabilities through specialized\nexperimental design components that generate statistically rigorous protocols based on research objectives\nand methodological best practices. These approaches highlight how tool integration capabilities significantly\nenhance scientific applications beyond the language model’s native capabilities.\n6.2.3 Scientific Literature Integration. Comprehensive knowledge integration enhances scientific understand-\ning:\nCross-Modal Scientific Content Analysis. Deep Research systems effectively integrate information across\ntext, data, and visualizations prevalent in scientific literature. Gemini/DeepResearch [60] demonstrates\nparticularstrengthinthisapplication,extractingandsynthesizinginformationfromscientificfigures,tables,\nand text into cohesive analyses. This capability enables more comprehensive literature utilization than\ntext-only approaches.\nOpen implementations like Jina-AI/node-DeepResearch [121] incorporate specialized components for\nmultimodal scientific content processing, enabling similar capabilities in customizable frameworks. These\napproaches highlight the growing importance of multimodal processing in scientific applications, reflecting\nthe diverse information formats prevalent in scientific communication.\nConflicting Evidence Resolution. Advanced systems help navigate contradictory findings common in\nscientific literature. Perplexity/DeepResearch [209] provides explicit conflict identification and resolution\nguidance, identifying methodological differences, contextual factors, and potential reconciliation approaches\nwhen faced with contradictory evidence. This capability enhances scientific understanding by providing\nstructured approaches to evidence integration rather than simple aggregation.\nImplementationpatternstypicallyinvolvesophisticatedevidencemodelinglikethatfoundinHKUDS/Auto-\nDeep-Research [112], which implements explicit evidence weighting and confidence estimation mechanisms.\nThese approaches demonstrate how specialized components for scientific evidence handling enhance the\npractical utility of Deep Research systems in complex scientific contexts.\n6.2.4 Autonomous Scientific Discovery. Fully autonomous research systems represent an emerging direction\nthat extends current Deep Research capabilities toward greater autonomy. Recent work in this area includes\nthe AI Scientist system [159] that implements an automated discovery loop with hypothesis generation,\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 49\nexperimentation, and theory revision capacities. Similarly, the Dolphin system [316] demonstrates how\nclosed-loopauto-researchcanintegratethinking,practice,andfeedbackmechanismstoimplementsystematic\nscientific discovery processes.\nThis evolution toward more autonomous operation represents a significant advancement beyond tradi-\ntional tool-based approaches, enabling continuous research cycles with minimal human intervention while\nmaintaining scientific rigor through structured validation processes. Systems like CycleResearcher [294]\nfurther enhance this approach by incorporating automated peer review mechanisms [150] that improve\noutput quality through systematic feedback loops mimicking scientific review processes.\nPractical implementation of these concepts appears in systems like AgentLaboratory [240], which demon-\nstrates how LLM agents can function as effective research assistants within structured laboratory environ-\nments.Complementingtheseapproaches,theconceptofself-maintainability(SeM)addressescriticalgapsin\nlaboratoryautomationbyenablingsystemstoautonomouslyadapttodisturbancesandmaintainoperational\nreadiness [191]. In addition, strategies such as BOLAA [156] orchestrate multiple specialized agents by\nemploying a controller to manage communication among them, enhancing the resolution of complex tasks.\nMoreover, Automated Capability Discovery (ACD) [158] automates the evaluation of foundation models by\ndesignating one model as a scientist to propose open-ended tasks that systematically uncover unexpected\ncapabilitiesandfailures.Similarly,SeqMate[178]utilizeslargelanguagemodelstoautomateRNAsequencing\ndata preparation and analysis, enabling user-friendly one-click analytics and report generation for biologists.\nThe FutureHouse Platform [253] broadens accessibility by delivering the first publicly available superintelli-\ngent AI agents for scientific discovery through web interfaces and APIs. These implementations highlight\nboth the significant potential and current limitations of autonomous scientific discovery systems, suggesting\nanevolutionarypathtowardincreasinglycapableresearchautomationwhilemaintainingappropriatehuman\noversight and validation.\n6.3 Business Intelligence Applications\nDeep Research technologies enable enhanced strategic decision support in commercial contexts.\n6.3.1 Market Research and Competitive Analysis. Comprehensive market understanding supports strategic\nplanning:\nCompetitor Landscape Mapping. Deep Research systems effectively synthesize comprehensive competitive\nintelligence across diverse sources. Gemini/DeepResearch [60] enables detailed competitor analysis across\nfinancial disclosures, product announcements, market reception, and strategic positioning to identify com-\npetitive dynamics and market opportunities. This application leverages the system’s ability to integrate\ninformation across public and specialized business sources with current market context.\nOpen implementations like n8n [183] support similar capabilities through workflow automation that\nintegrates specialized business intelligence data sources. These approaches demonstrate how effective tool\nintegrationcancreatesophisticatedbusinessintelligenceapplicationsbycoordinatingspecializedcomponents\nwithin consistent analytical frameworks.\nEmerging Trend Identification. Advancedsystemseffectivelyidentifyearly-stagemarkettrendsandpoten-\ntialdisruptions.OpenAI/DeepResearch[197]demonstratesthiscapabilitythroughtemporalpatternanalysis\n50 Xu et al.\nacross industry publications, startup activity, and technology development indicators. This application\ncombines historical pattern recognition with current signal detection to anticipate market evolution with\ngreater lead time than manual methods alone.\nImplementation patterns typically involve specialized analytical components like those in Flowith/\nOracleMode [77], which incorporates explicit trend modeling and weak signal amplification techniques.\nThese approaches highlight how specialized optimization enhances business intelligence applications through\ncomponents targeting specific analytical requirements.\n6.3.2 Strategic Decision Support. AI-enhanced analysis informs high-stakes business decisions:\nInvestment Opportunity Assessment. Deep Research systems support investment analysis through com-\nprehensive opportunity evaluation. Perplexity/DeepResearch [209] enables detailed investment analysis\nincorporating financial metrics, market positioning, competitive dynamics, and growth indicators within\nunified analytical frameworks. This application integrates quantitative financial assessment with qualitative\nmarket understanding to support more comprehensive investment evaluation.\nOpen frameworks like mshumer/OpenDeepResearcher [249] implement investment analysis components\nwith particular emphasis on structured evaluation frameworks and comprehensive source integration. These\napproaches demonstrate how domain-specific optimization enhances practical utility for specialized business\napplications beyond generic research capabilities.\nRisk Factor Identification. Advanced systems support risk management through comprehensive threat\nidentification and assessment. Gemini/DeepResearch [60] provides explicit risk analysis capabilities, identi-\nfying potential threats across regulatory, competitive, technological, and market dimensions with associated\nimpact and likelihood estimation. These features enable more comprehensive risk management than might\nbe practical through manual analysis alone.\nImplementationpatternstypicallyinvolvespecializedriskmodelingcomponentslikethosefoundinManus\n[164], which incorporates explicit risk categorization and prioritization mechanisms. These approaches\nhighlight how targeted optimization enhances specific business workflows through specialized components\naddressing critical decision support requirements.\n6.3.3 Business Process Optimization. Research-driven insights enhance operational effectiveness:\nBestPracticeIdentification. DeepResearchsystemseffectivelysynthesizeoperationalbestpracticesacross\nindustriesandapplications.OpenAI/DeepResearch[197]enablescomprehensiveprocessbenchmarkingagainst\nindustry standards and innovative approaches from adjacent sectors, identifying optimization opportunities\nthat might otherwise remain undiscovered. This application leverages the system’s broad knowledge base to\nfacilitate cross-industry learning and adaptation.\nOpenimplementationslikeTARS[39]supportsimilarcapabilitiesthroughworkflowanalysisandrecommen-\ndation components designed for business process optimization. These approaches demonstrate how domain\nadaptation enhances practical utility for specific business applications beyond general research capabilities.\nImplementation Planning Support. Advanced systems support process change through comprehensive\nimplementation guidance. Gemini/DeepResearch [60] provides detailed implementation planning incorporat-\ning change management considerations, resource requirements, and risk mitigation strategies derived from\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 51\nsimilar initiatives across industries. This capability accelerates organizational learning by leveraging broader\nimplementation experience than typically available within single organizations.\nImplementation patterns typically involve specialized planning components like those in QwenLM/Qwen-\nAgent [224], HuggingGPT[246], XAgent[202], Mastra[168],Letta[138] and SemanticKernel[174] which incor-\nporates explicit process modeling and change management frameworks. These approaches highlight how\ntargeted optimization enhances specific business workflows through specialized components addressing\ncritical implementation challenges.\n6.4 Financial Analysis Applications\nDeep Research technologies enable enhanced financial assessment and decision support.\n6.4.1 Investment Research and Due Diligence. AI-enhanced analysis supports investment decisions across\nasset classes:\nComprehensiveAssetEvaluation. DeepResearchsystemsenabledetailedassetanalysisacrossfinancialand\ncontextualdimensions.Perplexity/DeepResearch[209]supportsinvestmentresearchthroughintegrationof\nfinancial metrics, market positioning, competitive dynamics, and growth indicators within unified analytical\nframeworks.Thisapplicationenhancesinvestmentdecisionqualitythroughmorecomprehensiveinformation\nintegration than typically practical through manual methods alone.\nOpen implementations like n8n [183] enable similar capabilities through workflow automation that\nintegratesspecializedfinancialdatasourcesandanalyticaltools.Theseapproachesdemonstratehoweffective\ntool orchestration creates sophisticated financial applications by coordinating specialized components within\nconsistent analytical frameworks.\nManagementQualityAssessment. Advancedsystemssupportleadershipevaluationthroughcomprehensive\nbackground analysis. OpenAI/DeepResearch [197] enables detailed management assessment incorporating\nhistoricalperformance,leadershipapproach,strategicconsistency,andreputationacrossdiversesources.This\ncapability enhances investment evaluation by providing deeper leadership insights than typically available\nthrough standard financial analysis.\nImplementation patterns typically involve specialized entity analysis components like those found in\nManus [164], which incorporates explicit leadership evaluation frameworks. These approaches highlight how\ntargeted optimization enhances specific financial workflows through specialized components addressing\ncritical evaluation dimensions.\n6.4.2 Financial Trend Analysis. Pattern recognition across financial data informs strategic positioning:\nMulti-Factor Trend Identification. Deep Research systems effectively identify complex patterns across\nfinancial indicators and contextual factors. Gemini/DeepResearch [60] demonstrates this capability through\nintegratedanalysisofmarketmetrics,macroeconomicindicators,sector-specificfactors,andrelevantexternal\ntrends. This application enhances trend identification through more comprehensive factor integration than\ntypically practical through manual analysis alone.\nOpen frameworks like grapeot/deep_research_agent [263] implement specialized trend analysis compo-\nnents with particular emphasis on statistical pattern detection and causal factor identification. However,\n52 Xu et al.\nresearch indicates that the effectiveness of such AI systems may be limited in tasks requiring deep domain\nunderstanding, as their generated outputs can exhibit redundancy or inaccuracies [254]. These approaches\ndemonstratehowdomain-specificoptimizationenhancespracticalutilityforspecializedfinancialapplications\nbeyond generic analytical capabilities.\nScenario Development and Testing. Advanced systems support financial planning through structured\nscenario analysis. OpenAI/DeepResearch [197] enables detailed scenario development incorporating varied\nassumptions, historical precedents, and system dependencies with coherent projection across financial\nimpacts.Thiscapabilityenhancesstrategicplanningbyfacilitatingmorecomprehensivescenarioexploration\nthan typically practical through manual methods.\nImplementation patterns typically involve specialized scenario modeling components like those in\nAgent-RL/ReSearch [2], which incorporates explicit dependency modeling and consistency verification\nmechanisms. These approaches highlight how targeted optimization enhances specific financial workflows\nthrough specialized components addressing critical planning requirements.\n6.4.3 Risk Assessment and Modeling. Comprehensive risk analysis informs financial decisions:\nMulti-Dimensional Risk Analysis. Deep Research systems enable integrated risk assessment across diverse\nrisk categories. Perplexity/DeepResearch [209] supports comprehensive risk evaluation incorporating\nmarket, credit, operational, regulatory, and systemic risk factors within unified analytical frameworks.\nThis application enhances risk management through more comprehensive factor integration than typically\npractical through compartmentalized analysis.\nOpen implementations like nickscamara/open-deep-research [42] implement risk analysis components\nwith particular emphasis on integrated factor assessment and interaction modeling. These approaches\ndemonstrate how domain adaptation enhances practical utility for specific financial applications beyond\ngeneral analytical capabilities. Evaluations such as RedCode-Exec[101] show that agents are less likely to\nreject executing technically buggy code, indicating high risks, which highlights the need for stringent safety\nevaluations for diverse code agents.\nStress Testing and Resilience Assessment. Advanced systems support financial stability through so-\nphisticated stress scenario analysis. Gemini/DeepResearch [60] provides detailed stress testing capabilities\nincorporating historical crisis patterns, theoretical risk models, and system dependency analysis to identify\npotential vulnerabilities. These features enable more comprehensive resilience assessment than might be\npractical through standardized stress testing alone.\nImplementation patterns typically involve specialized stress modeling components like those found in\nFlowith/OracleMode [77], which incorporates explicit extreme scenario generation and impact propagation\nmechanisms. These approaches highlight how targeted optimization enhances specific financial workflows\nthrough specialized components addressing critical stability assessment requirements.\n6.5 Educational Applications\nDeep Research technologies enable enhanced learning and knowledge development. Educational approaches\nto research automation have shown particular promise in scientific education [236] and data science peda-\ngogy[274],withsystemslikeDS-Agentautomatingmachinelearningworkflowsthroughcase-basedreasoning\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 53\nto reduce learners’ technical barriers [102], highlighting the dual role of these systems in both conducting\nresearch and developing research capabilities in human learners. Smart AI reading assistants are also being\ndevelopedtoenhancereadingcomprehensionthroughinteractivesupport[266].However,adoptionchallenges\nremainsignificantineducationalcontexts,whereuserresistanceandineffectivesystemutilizationcanimpede\nlearning progress, requiring strategies such as active support during initial use and clear communication\nof system capabilities [252]. Specifically in data science education, learners encounter challenges similar\nto those faced by data scientists when interacting with conversational AI systems, such as difficulties in\nformulating prompts for complex tasks and adapting generated code to local environments [57]. Structured\nempirical evaluations of LLMs for data science tasks, such as the work by Nathalia Nascimento et al. [185],\ndemonstrate their effectiveness in coding challenges and provide guidance for model selection in educational\ntools.\n6.5.1 Personalized Learning Support. AI-enhanced research supports individualized educational experiences:\nAdaptive Learning Path Development. Deep Research systems effectively generate customized learning\npathways based on individual interests and knowledge gaps. OpenAI/DeepResearch [197] enables detailed\nlearningplandevelopmentincorporatingknowledgestructuremapping,prerequisiterelationships,anddiverse\nlearningresourcestailoredtoindividuallearningstylesandobjectives.Thisapplicationenhanceseducational\neffectiveness through more personalized learning journeys than typically available through standardized\ncurricula.\nOpen implementations like OpenManus [193] implement personalized learning components with particular\nemphasis on interest-driven exploration and adaptive difficulty adjustment. These approaches demonstrate\nhow educational adaptation enhances practical utility beyond general research capabilities.\nComprehensive Question Answering. Advanced systems provide detailed explanations tailored to learner\ncontext and prior knowledge. Perplexity/DeepResearch [209] demonstrates this capability through multi-\nlevel explanations that adjust detail and terminology based on learner background, providing conceptual\nscaffolding appropriate to individual knowledge levels. This capability enhances learning effectiveness by\nproviding precisely targeted explanations rather than generic responses.\nImplementation patterns typically involve specialized educational components like those in HKUDS/\nAuto-Deep-Research [112], which incorporates explicit knowledge modeling and explanation generation\nmechanisms. These approaches highlight how targeted optimization enhances educational applications\nthrough specialized components addressing critical learning support requirements.\n6.5.2 Educational Content Development. Research-driven content creation enhances learning materials:\nCurriculum Development Support. DeepResearchsystemseffectivelysynthesizeeducationalbestpractices\nanddomainknowledgeintocoherentcurricula.Gemini/DeepResearch[60]enablescomprehensivecurriculum\ndevelopment incorporating learning science principles, domain structure mapping, and diverse resource\nintegration.Thisapplicationenhanceseducationaldesignthroughmorecomprehensiveknowledgeintegration\nthan typically practical for individual educators.\nOpenframeworkslikesmolagents/open_deep_research[115]implementcurriculumdevelopmentcompo-\nnents with particular emphasis on learning progression modeling and resource alignment. These approaches\n54 Xu et al.\ndemonstrate how specialized adaptation enhances practical utility for educational applications beyond\ngeneric content generation.\nMulti-Modal Learning Material Creation. Advanced systems generate diverse educational content formats\ntailoredtolearningobjectives.OpenAI/DeepResearch[197]supportscreationofintegratedlearningmaterials\nincorporating explanatory text, conceptual visualizations, practical examples, and assessment activities\naligned with specific learning outcomes. This capability enhances educational effectiveness through more\ncomprehensive content development than typically practical through manual methods alone.\nImplementation patterns typically involve specialized content generation components like those in\nQwenLM/Qwen-Agent [224], which incorporates explicit learning objective modeling and multi-format content\ngeneration.Theseapproacheshighlighthowtargetedoptimizationenhanceseducationalapplicationsthrough\nspecialized components addressing diverse learning modalities.\n6.5.3 Academic Research Training. AI-assisted research skill development supports scholarly advancement:\nResearch Methodology Instruction. Deep Research systems effectively teach research methods through\nguided practice and feedback. Perplexity/DeepResearch [209] provides explicit methodology training,\ndemonstrating effective research processes while explaining rationale and providing structured feedback on\nlearner attempts. This application enhances research skill development through more interactive guidance\nthan typically available through traditional instruction.\nOpen implementations like Jina-AI/node-DeepResearch [121] support similar capabilities through re-\nsearchpracticeenvironmentswithexplicitguidanceandfeedbackmechanisms.Theseapproachesdemonstrate\nhow educational adaptation enhances practical utility for research training beyond simple information provi-\nsion.\nCritical Evaluation Skill Development. Maintaining critical thinking skills while leveraging AI research\nassistance presents unique educational challenges. Drosos et al. [71] demonstrate that carefully designed\n“provocations” can help restore critical thinking in AI-assisted knowledge work, suggesting important\neducational approaches for developing research skills that complement rather than rely entirely on AI\ncapabilities. Advanced systems support critical thinking through guided source evaluation and analytical\npractice. OpenAI/DeepResearch [197] enables critical evaluation training, demonstrating source assessment,\nevidenceweighing,andanalyticalreasoningwhileguidinglearnersthroughsimilarprocesses.Thiscapability\nenhances critical thinking development through structured practice with sophisticated feedback.\nImplementation patterns typically involve specialized educational components like those in grapeot/\ndeep_research_agent [263], which incorporates explicit critical thinking modeling and guided practice\nmechanisms. These approaches highlight how targeted optimization enhances educational applications\nthrough specialized components addressing crucial scholarly skill development.\n6.6 Personal Knowledge Management Applications\nDeep Research technologies enable enhanced individual information organization and utilization.\n6.6.1 InformationOrganizationandCuration. AI-enhancedsystemssupportpersonalknowledgedevelopment:\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 55\nPersonalizedKnowledgeBaseDevelopment. DeepResearchsystemseffectivelyorganizediverseinformation\ninto coherent personal knowledge structures. Perplexity/DeepResearch [209] supports knowledge base\ndevelopment through automated information organization, connection identification, and gap highlighting\ntailored to individual interests and objectives. This application enhances personal knowledge management\nthrough more sophisticated organization than typically practical through manual methods alone.\nOpen implementations like nickscamara/open-deep-research [42] implement knowledge organization\ncomponents with particular emphasis on personalized taxonomy development and relationship mapping.\nTheseapproachesdemonstratehowindividualadaptationenhancespracticalutilityforpersonalapplications\nbeyond generic information management.\nContent Summarization and Abstraction. Advanced systems transform complex information into accessi-\nble personal knowledge. OpenAI/DeepResearch [197] provides multi-level content abstraction capabilities,\ngenerating overview summaries, detailed analyses, and conceptual maps from complex source materials\ntailored to individual comprehension preferences. This capability enhances information accessibility by\nproviding precisely targeted representations rather than generic summaries.\nImplementation patterns typically involve specialized content processing components like those in\nNanobrowser [184], which incorporates explicit knowledge distillation and representation generation mecha-\nnisms. These approaches highlight how targeted optimization enhances personal knowledge applications\nthrough specialized components addressing individual information processing needs.\n6.6.2 Personal Learning and Development. Research-driven insights support individual growth:\nInterest-Driven Exploration. Deep Research systems effectively support curiosity-driven learning through\nguided exploration. Gemini/DeepResearch [60] enables interest-based knowledge discovery, identifying\nconnections,extensions,andpracticalapplicationsrelatedtoindividualcuriosities.Thisapplicationenhances\npersonal learning through more sophisticated guidance than typically available through standard search\nalone.\nOpen frameworks like OpenManus [193] implement exploration components with particular emphasis on\ninterest mapping and discovery facilitation. These approaches demonstrate how personalization enhances\npractical utility for individual learning beyond generic information retrieval.\nSkill Development Planning. Advanced systems support personal growth through comprehensive develop-\nmentguidance.Perplexity/DeepResearch[209]providesdetailedskilldevelopmentplanning,incorporating\nlearning resource identification, progression mapping, and practice guidance tailored to individual objectives\nand constraints. This capability enhances personal development through more comprehensive planning\nsupport than typically available through generic guidance.\nImplementation patterns typically involve specialized planning components like those in TARS [39], which\nincorporates explicit skill modeling and development path generation. These approaches highlight how\ntargeted optimization enhances personal growth applications through specialized components addressing\nindividual development needs.\n6.6.3 DecisionSupportforIndividualUsers. Research-enhanceddecisionmakingimprovespersonaloutcomes:\n56 Xu et al.\nComplex Decision Analysis. Deep Research systems effectively support personal decisions through com-\nprehensive option evaluation. OpenAI/DeepResearch [197] enables detailed decision analysis, incorporating\nmultiple criteria, preference weighting, and consequence projection tailored to individual values and con-\nstraints. This application enhances decision quality through more sophisticated analysis than typically\npractical through manual methods alone.\nOpen implementations like Agent-RL/ReSearch [2] implement decision support components with par-\nticular emphasis on preference elicitation and consequence modeling. These approaches demonstrate how\npersonalizationenhancespracticalutilityforindividualdecisionmakingbeyondgenericinformationprovision.\nLife Planning and Optimization. Advanced systems support long-term planning through integrated life\ndomainanalysis.Gemini/DeepResearch[60]providescomprehensivelifeplanningsupport,integratingcareer,\nfinancial, health, and personal considerations within coherent planning frameworks tailored to individual\nvalues and objectives. This capability enhances life optimization through more integrated planning than\ntypically achievable through domain-specific approaches alone.\nImplementation patterns typically involve specialized planning components like those in Flowith/\nOracleMode [77], which incorporates explicit value modeling and multi-domain integration mechanisms.\nThese approaches highlight how targeted optimization enhances personal planning applications through\nspecialized components addressing holistic life considerations.\nThediverseapplicationsoutlinedinthissectiondemonstratethebroadpracticalimpactofDeepResearch\ntechnologies across domains. While specific implementation approaches vary across commercial and open-\nsource ecosystems, common patterns emerge in domain adaptation, specialized component design, and\nintegration with existing workflows. These patterns highlight how technical capabilities translate into\npractical value through thoughtful application design aligned with domain-specific requirements and user\nneeds.\n7 Ethical Considerations and Limitations\nThe integration of Deep Research systems into knowledge workflows introduces significant ethical considera-\ntions and technical limitations that must be addressed for responsible deployment. This section examines\nkey challenges across four fundamental dimensions (see Figure 10): information integrity, privacy protection,\nsource attribution and intellectual property, and accessibility.\n7.1 Information Accuracy and Hallucination Concerns\nDeep Research systems face fundamental challenges in maintaining factual reliability despite their sophisti-\ncated capabilities.\n7.1.1 Factual Verification Mechanisms. Recent studies have highlighted significant challenges in reliable\nuncertaintycommunication[55],withparticularconcernsforresearchcontextswhereuncertaintyboundaries\nmaybeunclearorcontested.SomeresearchershaveraisedconcernsaboutexcessiverelianceonAI-generated\ncontent in scholarly writing [27, 45, 104, 119, 146, 207, 282, 286, 324, 335], particularly when verification\nmechanisms are inadequate or bypassed. These limitations are further complicated by tendencies toward\nmisleadingresponsesinconversation[113],presentingparticularchallengesforinteractiveresearchworkflows\nwhere iterative refinement may inadvertently amplify initial inaccuracies. AI support systems designed for\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 57\nEthical Dimensions of Deep Research Systems\nInformation Accuracy and Hallucination Privacy and Data Security\nFactual Verification Hallucination Detection User Data Protection Sensitive Information\nSource validation Grounding requirements Query isolation PII management\nOpenAI DR, Perplexity DR Gemini DR, grapeot OpenAI DR, Gemini DR Perplexity DR, TARS\nQuality Control Compliance Frameworks\nConfidence estimation, feedback Regional and local deployment\nResponsible Deep\nResearch Development\nBalancing Capability, Safety, Equity\nSource Attribution and IP Issues Accessibility and Digital Divide\nCitation Generation Copyright Considerations Technology Access User Expertise\nAcademic formats Fair use boundaries Resource requirements Technical knowledge\nOpenAI DR, Perplexity DR jina-ai, mshumer Camel-AI/OWL OpenManus\nIntellectual Attribution Inclusive Design\nSourcetransparency,licensing Multilingual,disabilityaccommodation\nKey Ethical Considerations\nFig.10. EthicalDimensionsofDeepResearchSystems\nevidence-based expository writing tasks, such as literature reviews, offer frameworks to enhance verification\nthrough structured sensemaking over source documents [247]. Addressing these challenges requires technical\nadvancements in uncertainty representation, improvements in decision workflow design[107] and interface\ndesign improvements that effectively communicate confidence boundaries to research users[270].\nEnsuring information accuracy requires explicit verification strategies:\nSource VerificationApproaches. Leading implementations incorporate explicit source validation mecha-\nnisms to enhance factual reliability. OpenAI/DeepResearch [197] implements multi-level verification that\nconfirms information across multiple independent sources before incorporation into research outputs, with\ndetailedguidelinesoutlinedintheirsystemdocumentation[196].Similarly,Perplexity/DeepResearch[209]\nimplements automated fact-checking that independently verifies key claims against trusted reference sources\nbefore inclusion in final reports.\nOpen-source alternatives demonstrate varied approaches to verification. Systems like grapeot/deep_\nresearch_agent [263] emphasize explicit citation mechanisms that maintain direct links between claims\n58 Xu et al.\nand sources, enabling straightforward verification. More sophisticated implementations like HKUDS/Auto-\nDeep-Research [112] incorporate specialized verification modules that assess source credibility and content\nconsistency before information utilization.\nHallucination Detection and Prevention. Mitigating fabricated information represents a crucial challenge\nfor LLM-based research systems. Commercial implementations employ advanced hallucination reduction\ntechniques including strict grounding requirements and consistency verification. Gemini/DeepResearch [60]\nimplements explicit uncertainty modeling that distinguishes between confirmed information and speculative\nextensions, enhancing transparency when definitive answers cannot be provided. Emerging paradigms\nlike those proposed by Silver and Sutton [251] suggest a fundamental shift toward experience-driven\nlearning, potentially transforming how research systems acquire and refine capabilities through interaction\nwith information environments. Such approaches could enable more human-like research development\nthrough continuous improvement based on research experiences rather than static training alone, and could\nfundamentally mitigate hallucinations.\nOpen implementations demonstrate pragmatic approaches to hallucination reduction within more con-\nstrainedtechnicalenvironments.SystemslikeAgent-RL/ReSearch[2]employpreventativestrategiesincluding\nexplicit sourcing requirements and conservative synthesis guidelines that prioritize factual reliability over\ncomprehensivecoverage.ComplementaryapproacheslikeMask-DPO[100]focusongeneralizablefine-grained\nfactuality alignment, addressing a critical requirement for reliable research outputs. Recent work from\nthe GAIR NLP team on DeepResearcher [81] has advanced these capabilities through integrated neural\nverification and knowledge graph alignment techniques that significantly enhance factual reliability. These\napproaches highlight diverse strategies for addressing a fundamental challenge that impacts all LLM-based\nresearch systems.\n7.1.2 Uncertainty Communication Approaches. Transparent uncertainty representation enhances result inter-\npretation and appropriate utilization:\nConfidenceEstimationMethods. Advanced systems implement explicit confidence assessment for research\nfindings and recommendations. OpenAI/DeepResearch [197] incorporates graduated confidence scoring that\nreflectsevidencequality,consistencyacrosssources,andreasoningreliability.Thiscapabilityenhancesresult\ninterpretation by clearly distinguishing between well-supported conclusions and more speculative findings.\nOpen-source implementations demonstrate simplified but effective confidence communication approaches.\nSystems like mshumer/OpenDeepResearcher [249] incorporate basic confidence indicators that signal infor-\nmation reliability through explicit markers in research outputs. These approaches highlight the importance\nof transparent uncertainty communication regardless of implementation sophistication.\nEvidenceQualificationStandards. Responsible systems clearly communicate limitations and contextual\nfactors affecting result interpretation. Commercial implementations like Perplexity/DeepResearch [209]\nincorporate explicit evidence qualification that highlights contextual limitations, conflicting viewpoints, and\ntemporal constraints affecting research findings. This practice enhances appropriate utilization by providing\nnecessary context for result interpretation.\nOpen-source alternatives demonstrate varied approaches to evidence qualification. Systems like dzhng/\ndeep-research[321]implementexplicitlimitationstatementsthatidentifykeyconstraintsaffectingresearch\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 59\nreliability. More sophisticated implementations like Camel-AI/OWL [43] incorporate structured evidence\nmodels that represent both supporting and contradicting information within unified frameworks.\n7.1.3 Quality Control Frameworks. Systematic approaches to quality assurance enhance overall reliability:\nPre-ReleaseVerificationStandards. Leading implementations employ comprehensive validation processes\nbeforeresultdelivery.GeminiDeepResearchimplementsstructuredqualityverificationincludingautomated\nconsistency checking, source validation, and reasoning verification before providing research outputs. These\npractices enhance overall reliability through systematic error identification and correction.\nOpen-sourceimplementationsdemonstratemorevariedqualitycontrolapproaches.Systemslikenickscamara/\nopen-deep-research [42] incorporate simplified validation processes focusing on critical reliability factors\nincluding source verification and logical consistency. These approaches highlight how even basic quality\ncontrol mechanisms can significantly enhance research reliability.\nFeedback Integration Systems. Continuous improvement requires effective incorporation of accuracy\nfeedback. As Deep Research systems advance toward greater autonomy, broader safety considerations\nbecome increasingly important. Bengio et al. [26] highlight potential risks from superintelligent agents and\nproposeapproacheslike“ScientistAI”thatbalancecapabilitywithsaferdevelopmentpaths,emphasizingthe\nimportance of integrated safety mechanisms in advanced research systems. Commercial systems implement\nsophisticatedfeedbackintegrationincludingexplicitaccuracyreportingchannelsandsystematicerrorpattern\nanalysis. OpenAI/DeepResearch [197] includes dedicated correction mechanisms that incorporate verified\naccuracy feedback into system improvements, creating virtuous improvement cycles.\nOpenimplementationsdemonstratemorecommunity-orientedfeedbackapproaches.Systemslikesmolagents/\nopen_deep_research [115] incorporate collaborative improvement frameworks that enable distributed error\nidentificationandcorrectionthroughcommunitycontributions.Theseapproacheshighlightdiversestrategies\nfor enhancing reliability through user engagement across implementation contexts.\n7.2 Privacy and Data Security\nResearch systems must carefully protect sensitive information throughout the research process.\n7.2.1 User Data Protection Mechanisms. Safeguarding user information requires comprehensive protection\nstrategies:\nQueryIsolationPractices. Leading implementations employ strict isolation between user research sessions.\nCommercial systems like OpenAI/DeepResearch [197] and Gemini/DeepResearch [60] implement compre-\nhensive tenant isolation that prevents information leakage between distinct users or organizations. These\npractices are particularly crucial for sensitive research applications in corporate or governmental contexts.\nOpen-source implementations demonstrate varied isolation approaches depending on deployment models.\nSystems designed for local deployment like OpenManus [193] enable complete isolation within organizational\nboundaries, enhancing privacy for sensitive applications. Cloud-dependent implementations typically in-\ncorporate more limited isolation mechanisms, highlighting deployment considerations for privacy-sensitive\napplications.\nDataMinimizationStrategies.Responsiblesystemslimitsensitivedatacollectionandretention.Commercial\nimplementations increasingly emphasize data minimization, collecting only information necessary for service\n60 Xu et al.\nprovision and applying appropriate retention limitations. These practices enhance privacy protection by\nreducing potential exposure of sensitive information through either security incidents or authorized access.\nOpen implementations demonstrate diverse approaches to data management. Systems like Nanobrowser\n[184] enable complete local control of browsing data, preventing external exposure of research activities.\nInfrastructureframeworkslikeJina-AI/node-DeepResearch[121]provideflexibleconfigurationoptionsthat\nenable deployment-specific privacy controls aligned with organizational requirements.\n7.2.2 Sensitive Information Handling. Special safeguards are required for particularly sensitive content\ncategories:\nPersonalIdentifierManagement.Advancedsystemsimplementspecificprotectionsforpersonallyidentifiable\ninformation. Commercial implementations like Perplexity/DeepResearch [209] incorporate automatic\ndetection and redaction of personal identifiers from research outputs unless specifically relevant to research\nobjectives. These practices prevent inadvertent exposure of personal information through research activities.\nOpen implementations demonstrate more varied approaches to identifier management. Systems like TARS\n[39] incorporate basic identifier detection focused on common patterns like email addresses and phone\nnumbers.MoresophisticatedimplementationslikeQwenLM/Qwen-Agent[224]provideconfigurablesensitivity\ncontrols that enable context-appropriate protection aligned with specific deployment requirements.\nProtectedCategorySafeguards.Responsiblesystemsimplementenhancedprotectionsforspeciallyregulated\ninformation categories. Commercial implementations increasingly incorporate specialized handling for\ninformation categories including health data, financial records, and other regulated content types. These\npractices enhance compliance with domain-specific regulatory requirements governing sensitive information.\nOpen-source alternatives demonstrate more varied regulatory alignment. Systems like n8n [183] provide\nspecialized workflow components for handling regulated data categories, enabling compliance-oriented\nimplementations in sensitive domains. These approaches highlight how specialized components can address\ndomain-specific regulatory requirements within flexible implementation frameworks.\n7.2.3 Compliance with Regulatory Frameworks. Adherence to applicable regulations ensures legally appropri-\nate operation:\nJurisdictional Compliance Adaptation. Advanced systems implement regionally appropriate operational\nstandards. Commercial implementations increasingly incorporate jurisdiction-specific adaptations that align\nwith regional privacy regulations including GDPR, CCPA, and other frameworks. These practices enhance\nlegal compliance across diverse deployment environments with varying regulatory requirements.\nOpenimplementationsdemonstratemoredeployment-dependentcomplianceapproaches.Systemsdesigned\nfor flexible deployment like Flowith/OracleMode [77] provide configurable privacy controls that enable\nadaptation to specific regulatory environments. These approaches highlight the importance of adaptable\nprivacy frameworks that can address diverse compliance requirements across implementation contexts.\nTransparency and Control Mechanisms. Responsible systems provide appropriate visibility and user\nauthority over information processing. Emerging regulatory frameworks are increasingly focusing on AI\nagents with autonomous capabilities. Osogami [204] proposes that regulation of autonomous AI systems\nshould specifically consider action sequence patterns rather than individual actions in isolation, which has\nparticular implications for Deep Research systems that execute complex multi-step research workflows.\nCommercial implementations increasingly emphasize transparency through explicit processing disclosures\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 61\nand user control mechanisms aligned with regulatory requirements. These practices enhance both regulatory\ncompliance and user trust through appropriate information governance.\nOpen-source alternatives demonstrate varied transparency approaches. Systems like HKUDS/Auto-Deep-\nResearch[112]providedetailedloggingofinformationaccessandprocessingactivities,enablingappropriate\noversightandverification.Theseapproacheshighlighthowtransparentoperationcanenhancebothcompliance\nand trust across implementation contexts.\n7.3 Source Attribution and Intellectual Property\nProper acknowledgment of information sources and respect for intellectual property rights are essential for\nethical information utilization.\n7.3.1 CitationGenerationandVerification. Accuratesourceattributionrequiresreliablecitationmechanisms:\nAutomatedCitationSystems. Advanced implementations incorporate sophisticated citation generation\nfor research outputs. Commercial systems like OpenAI/DeepResearch [197] and Perplexity/DeepResearch\n[209] implement automatic citation generation in standard academic formats, enhancing attribution quality\nand consistency. These capabilities support appropriate source acknowledgment without manual effort.\nOpenimplementationsdemonstratevariedcitationapproaches.Systemslikemshumer/OpenDeepResearcher\n[249] incorporate basic citation generation focused on fundamental bibliographic information. More sophisti-\ncated alternatives like dzhng/deep-research [321] provide enhanced citation capabilities including format\ncustomization and citation verification against reference databases.\nCitationCompletenessVerification. Responsible systems ensure comprehensive attribution for all utilized\ninformation. Commercial implementations increasingly incorporate citation coverage verification that identi-\nfies unsupported claims requiring additional attribution. These practices enhance attribution reliability by\nensuring all significant claims maintain appropriate source connections.\nOpen-source alternatives demonstrate pragmatic approaches to attribution verification. Systems like\ngrapeot/deep_research_agent[263]implementexplicitsource-claimmappingthatmaintainsclearrelation-\nships between information and origins. These approaches highlight the importance of systematic attribution\nregardless of implementation sophistication.\n7.3.2 Intellectual Attribution Challenges. Special attribution considerations apply to complex intellectual\ncontributions:\nIdeaAttributionPractices. Research systems must appropriately acknowledge conceptual contributions\nbeyond factual information. Commercial implementations increasingly emphasize concept-level attribution\nthat acknowledges intellectual frameworks and theoretical approaches beyond simple facts. These practices\nenhance ethical information utilization by appropriately recognizing intellectual contributions.\nOpen implementations demonstrate varied idea attribution approaches. Systems like Camel-AI/OWL [43]\nincorporate explicit concept attribution that identifies theoretical frameworks and analytical approaches\nutilizedinresearchoutputs.Theseapproacheshighlighttheimportanceofcomprehensiveattributionbeyond\nbasic factual sources.\nSynthesizedKnowledgeAttribution. Attribution becomes particularly challenging for insights synthesized\nacrossmultiplesources.Advancedsystemsimplementspecializedattributionapproachesforsyntheticinsights\n62 Xu et al.\nthat acknowledge multiple contributing sources while clearly identifying novel connections. These practices\nenhance attribution accuracy for the increasingly common scenario of cross-source synthesis.\nOpen-source alternatives demonstrate pragmatic approaches to synthesis attribution. Systems like Agent-\nRL/ReSearch [2] implement explicit synthesis markers that distinguish between directly sourced information\nand system-generated connections. These approaches highlight the importance of transparent derivation\neven when direct attribution becomes challenging.\n7.3.3 Copyright and Fair Use Considerations. Research activities interact with copyright protections in\nmultiple dimensions:\nFairUseEvaluationMechanisms. Research systems must navigate appropriate utilization of copyrighted\nmaterials. Commercial implementations increasingly incorporate fair use evaluation that considers purpose,\nnature, amount, and market impact when utilizing copyrighted content. These practices enhance legal\ncompliance while enabling appropriate information utilization for legitimate research purposes.\nOpenimplementationsdemonstratevariedcopyrightapproaches.SystemslikeJina-AI/node-DeepResearch\n[121]incorporatebasiccopyrightacknowledgmentfocusingonproperattribution,whilemoresophisticatedal-\nternativeslikeManus[164]provideenhancedcopyrighthandlingincludingcontenttransformationassessment\nand restricted access mechanisms for sensitive materials.\nContent LicensingCompliance. Responsible systems respect diverse license terms applicable to utilized\ncontent.Advancedimplementationsincreasinglyincorporatelicense-awareprocessingthatadaptsinformation\nutilization based on specific terms governing particular sources. These practices enhance compliance with\nvaried license requirements across the information ecosystem.\nOpen implementations demonstrate more standardized licensing approaches. Systems like grapeot/deep_\nresearch_agent[263]incorporatesimplifiedlicensecategorizationfocusingoncommonframeworksincluding\ncreative commons and commercial restrictions. These approaches highlight pragmatic strategies for license\nnavigation within resource constraints.\n7.3.4 Output Intellectual Property Frameworks. Clear rights management for research outputs enhances\ndownstream utilization:\nOutputLicenseAssignment. Complex questions arise regarding intellectual property in research outputs.\nCommercial systems increasingly implement explicit license assignment for generated content, clarifying\nintellectual property status for downstream utilization. These practices enhance transparency regarding\nusage rights for research outputs created through automated systems.\nOpen-source alternatives demonstrate varied approaches to output rights. Systems like OpenManus [193]\nincorporate explicit license designation for research outputs aligned with organizational policies and source\nrestrictions. These approaches highlight the importance of clear intellectual property frameworks regardless\nof implementation context.\nDerivative Work Management. Research systems must address whether outputs constitute derivative\nworks of source materials. Commercial systems increasingly implement derivative assessment frameworks\nthat evaluate the nature and extent of source transformation in research outputs. These practices enhance\nappropriate categorization for downstream utilization aligned with source licenses.\nOpen-source alternatives demonstrate varied derivation approaches. Systems such as QwenLM/Qwen-Agent\n[224] incorporate a basic transformation assessment focusing on content reorganization and analytical\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 63\naddition. These approaches highlight the importance of thoughtful derivative consideration regardless of\nimplementation sophistication.\n7.4 Accessibility and Digital Divide\nEquitable access to research capabilities requires addressing systematic barriers.\n7.4.1 Technology Access Disparities. Recent work has highlighted both adoption barriers and opportunities\nfor making Deep Research systems more accessible. Bianchini et al. [29] and Tonghe Zhuang et al. [334]\nidentify specific organizational and individual factors affecting AI adoption in scientific research contexts,\nwith implications for Deep Research deployment. Accessibility-focused approaches like those presented by\nMowar et al. [179] demonstrate how AI coding assistants can be specifically designed to support accessible\ndevelopment practices, suggesting parallel opportunities for accessibility-centered Deep Research systems.\nExtending this, systems such as ResearchAgent [18] showcase how AI can lower barriers to scientific\ninnovation by enabling iterative refinement of research ideas through collaborative feedback mechanisms,\nthus democratizing access to complex ideation processes.\nResource requirements create potential exclusion for various user segments:\nComputationalRequirementConsiderations. Resource-intensive systems may exclude users without sub-\nstantial computing access. Commercial cloud-based implementations address this challenge through shared\ninfrastructurethatreduceslocalrequirements,thoughwithassociatedcostbarriers.Open-sourcealternatives\ndemonstrate varied resource profiles, with systems like Camel-AI/OWL [43] emphasizing efficiency to enable\nbroader deployment on limited hardware.\nCost Barrier Mitigation. Financial requirements create systematic access disparities across socioeco-\nnomic dimensions. Commercial implementations demonstrate varied pricing approaches, with systems like\nPerplexity/DeepResearch [209] offering limited free access alongside premium tiers. Open-source alter-\nnatives like HKUDS/Auto-Deep-Research [112] and nickscamara/open-deep-research [42] eliminate direct\ncost barriers while potentially introducing technical hurdles.\n7.4.2 User Expertise Requirements. Technical complexity creates additional access barriers beyond resource\nconsiderations:\nTechnicalExpertiseDependencies. Complex system deployment and operation may exclude users without\nspecialized knowledge. Commercial implementations address this challenge through managed services that\neliminate deployment complexity, though with reduced customization flexibility. Open-source alternatives\ndemonstrate varied usability profiles, with systems like OpenManus [193] emphasizing simplified deployment\nto enhance accessibility despite local operation.\nDomainKnowledgePrerequisites.Effectiveresearchstillrequirescontextualunderstandingforappropriate\nutilization. Both commercial and open-source implementations increasingly incorporate domain guidance\nthat assists users with limited background knowledge in specific research areas. These capabilities enhance\naccessibility by reducing domain expertise barriers to effective research utilization.\n7.4.3 Inclusivity and Universal Design Approaches. Deliberate inclusive design can address systematic access\nbarriers:\n64 Xu et al.\nLinguistic and Cultural Inclusivity. Language limitations create significant barriers for non-dominant\nlanguagecommunities.Commercialimplementationsincreasinglyoffermultilingualcapabilities,thoughwith\npersistentqualitydisparitiesacrosslanguages.Open-sourcealternativesdemonstratevariedlanguagesupport,\nwith systems like Flowith/OracleMode [77] emphasizing extensible design that enables community-driven\nlanguage expansion beyond dominant languages.\nDisabilityAccommodationApproaches. Accessible design ensures appropriate access for users with diverse\nabilities. Commercial implementations increasingly incorporate accessibility features including screen reader\ncompatibility,keyboardnavigation,andalternativeformatgeneration.Open-sourcealternativesdemonstrate\nmore varied accessibility profiles, highlighting an area for continued community development to ensure\nequitable access across implementation contexts.\nThe ethical considerations explored in this section highlight the complex responsibilities associated with\nDeep Research technologies beyond technical performance. While current implementations demonstrate\nvarying approaches to these challenges across commercial and open-source ecosystems, consistent patterns\nemergeintheimportanceoffactualverification,attributionquality,privacyprotection,intellectualproperty\nrespect, and accessible design. Addressing these considerations represents a critical priority for responsible\ndevelopment and deployment of these increasingly influential research technologies.\n8 Future Research Directions\nThe rapidly evolving field of Deep Research presents numerous opportunities for technical advancement\nand application expansion. Recent work by Zheng et al. [329] proposes scaling deep research capabilities\nvia reinforcement learning in real-world environments, while Wu et al. [297] explore enhancing reasoning\ncapabilities of LLMs with tools specifically for deep research applications. The comprehensive framework for\nbuilding effective agents outlined by Anthropic [11] provides additional design principles that could inform\nfutureDeepResearchsystems.Thissectionexaminespromisingresearchdirections(illustratedinFigure11)\nthatcouldsignificantlyenhancecapabilities,addresscurrentlimitations,andexpandpracticalimpactacross\ndomains, focusing on four key areas: advanced reasoning architectures, multimodal integration, domain\nspecialization, and human-AI collaboration with standardization.\n8.1 Advanced Reasoning Architectures\nEnhanced reasoning capabilities represent a fundamental advancement opportunity for next-generation\nsystems.\n8.1.1 Context Window Optimization and Management. The information-intensive nature of deep research\ntasks presents fundamental challenges for context window utilization:\nInformationCompressionandPrioritization.Currentsystemsstrugglewithcontextwindowexhaustionwhen\nprocessing extensive research materials. Future architectures could incorporate sophisticated compression\nmechanisms that maintain semantic content while reducing token consumption. Early steps in this direction\nappear in systems like OpenAI/DeepResearch [197], which implements basic summarization for lengthy\nsources. Recent work on academic paper review systems demonstrates how hierarchical processing of\nextended research content can maintain coherence while managing context limitations [333]. Semantic\nnavigation techniques offer complementary approaches by enabling efficient exploration of problem-solution\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 65\nResearch Directions for Deep Research Systems\nAdvanced Reasoning Architectures Multi-Modal Deep Research\nHybrid Symbolic-Neural Approaches Visual Information Integration\nNeuro-symbolic integration, knowledge graphs Scientific imagery, visual evidence analysis\nCausal Reasoning Enhancement Multimodal Source Analysis\nCausal inference, intervention modeling Video content, audio content processing\nUncertainty Representation Cross-Modal Reasoning\nMulti-dimensional uncertainty, Bayesian reasoning Consistency verification, multimodal explanations\nFuture of\nDeep Research\nDomain-Specific Optimization Human-AI Collaboration Standards\nScientific Domain Adaptation Interactive Research Workflows\nField-specific models, scientific workflow integration Query refinement, exploration interfaces\nLegal Regulatory Specialization Framework Standardization\nLegal reasoning, regulatory compliance Common APIs, research protocols, interoperability\nMedical Healthcare Research Joint Knowledge Creation\nClinical evidence synthesis, personalization Collaborative writing, mixed-initiative research\nFig.11. ResearchDirectionsforDeepResearchSystems\nspaces within constrained domains, optimizing context usage through input filtering while enhancing\ngeneration quality [238]. More advanced approaches could develop adaptive compression that preserves\ncrucial details while condencing secondary information based on query relevance.\nImplementation opportunities include developing hierarchical summarization techniques that maintain\nmulti-level representations of sources, implementing information relevance scoring that prioritizes context\nallocation to critical content, and designing dynamic context management that continuously optimizes\nwindow utilization throughout research workflows. These advances could significantly enhance information\nprocessing capabilities without requiring proportional increases in context length.\nExternal Memory Architectures. Beyond compression, architectural innovations could fundamentally\ntransform context window utilization. Future systems could implement sophisticated external memory\n66 Xu et al.\nframeworks that maintain rich information representations outside the primary context window, accessing\nthem through efficient retrieval mechanisms when needed. Systems like Camel-AI/OWL [43] demonstrate\nearly steps with basic retrieval-augmented generation, but more comprehensive approaches could enable\neffectively unlimited knowledge integration.\nResearch directions include developing differentiable retrieval mechanisms that seamlessly integrate\nexternal knowledge within reasoning flows, implementing structured memory hierarchies that organize\ninformation for efficient access, and designing memory-aware reasoning processes that explicitly consider\ninformation availability when planning analytical approaches. These architectures could fundamentally\naddress context limitations while enhancing reasoning transparency and reliability.\n8.1.2 Hybrid Symbolic-Neural Approaches. Integration of complementary reasoning paradigms offers signifi-\ncant potential:\nNeuro-SymbolicIntegration. Current Deep Research systems rely primarily on neural approaches with\nlimitedexplicitreasoningstructures.Futuresystemscouldintegratesymbolicreasoningcomponentsthatpro-\nvideformallogicalcapabilitiesalongsideneuralflexibility,enhancingbothreliabilityandexplainability.Early\nexamplesofthisdirectionappearinsystemslikeCamel-AI/OWL[43],whichincorporatesstructuredknowledge\nrepresentation within primarily neural architectures. Future research could develop more sophisticated\nintegration approaches that leverage the complementary strengths of both paradigms.\nImplementationapproachesmightincludeexplicitlogicalverificationlayersthatvalidateneural-generated\nreasoning, hybrid architectures that select appropriate reasoning mechanisms based on task characteristics,\nor integrated systems that translate between symbolic and neural representations as needed throughout\ncomplex workflows. These approaches could address current challenges in reliability and consistency while\nmaintaining the flexibility and generalization capabilities of neural foundations.\nAdvancedKnowledgeGraphIntegration.Whilecurrentsystemsalreadyincorporatebasicknowledgegraph\ncapabilities, future approaches could implement more sophisticated integration with dynamic, contextually-\nawareknowledgestructures.BeyondtheentityrelationshipmodelingseeninsystemslikeHKUDS/Auto-Deep-\nResearch [112], next-generation implementations could enable bidirectional updates where research findings\nautomaticallyrefineandexpandknowledgegraphswhilesimultaneouslyleveragingthemforreasoning.Such\napproaches could incorporate uncertainty representation within graph structures, probabilistic reasoning\nacross knowledge networks, and adaptive abstraction hierarchies that transform between detailed and\nhigh-level conceptual representations based on reasoning requirements. Research opportunities include\ndevelopingdynamicknowledgegraphconstructiontechniquesthatautomaticallybuildandrefinestructured\nrepresentationsfromunstructuredsources,implementinggraph-awareattentionmechanismsthatincorporate\nrelationship structures into neural reasoning, and designing hybrid querying approaches that combine graph\ntraversal with neural generation. These advances could enhance precision for complex reasoning tasks\nrequiring structured relationship understanding.\n8.1.3 Causal Reasoning Enhancement. Moving beyond correlation to causal understanding represents a\ncrucial capability advancement:\nCausalInferenceMechanisms. Current systems excel at identifying correlations but struggle with robust\ncausal analysis. Future research could develop specialized causal reasoning components that systematically\nidentify potential causal relationships, evaluate evidence quality, and assess alternative explanations. Recent\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 67\nwork in healthcare research by Schuemie et al. [241] demonstrates the challenges of establishing confident\nobservational findings, highlighting the need for more sophisticated causal reasoning in research systems.\nEarly steps in this direction appear in systems like OpenAI/DeepResearch [197], which incorporates basic\ncausal language in relationship descriptions. Other research explores the use of AI to assist in mining\ncausality,forinstance,bysearchingforinstrumentalvariablesineconomicanalysis[105].Moresophisticated\napproaches could enable reliable causal analysis across domains. Implementation opportunities include\ndevelopingcausalgraphconstructiontechniquesthatexplicitlymodelinterventioneffectsandcounterfactuals,\nimplementingcausaluncertaintyquantificationthatrepresentsconfidenceincausalassertions,anddesigning\nspecialized prompt structures that guide causal reasoning through structured analytical patterns. These\nadvances could enhance research quality for domains where causal understanding is particularly crucial,\nincluding medicine, social sciences, and policy analysis.\nInterventionModelingTechniques. Advanced causal understanding requires sophisticated intervention and\ncounterfactual reasoning capabilities. Future systems could incorporate explicit intervention modeling that\nsimulates potential actions and outcomes based on causal understanding, enhancing both explanatory and\npredictivecapabilities.EarlyexamplesofthisdirectionappearinsystemslikeAgent-RL/ReSearch[2],which\nimplements basic intervention simulation within reinforcement learning frameworks. More comprehensive\napproaches could enable sophisticated what-if analysis across domains.\nResearch directions include developing counterfactual generation techniques that systematically explore\nalternative scenarios based on causal models, implementing intervention optimization algorithms that\nidentifyhigh-leverageactionopportunities,anddesigningdomain-specificinterventiontemplatesthatembed\nfield-specific causal knowledge for common analysis patterns. These advances could enhance practical utility\nfor decision support applications requiring sophisticated action planning and outcome prediction.\n8.1.4 Uncertainty Representation and Reasoning. Sophisticated uncertainty handling enhances both accuracy\nand trustworthiness:\nMulti-DimensionalUncertaintyModeling.Currentsystemsemployrelativelysimplisticuncertaintyrepresen-\ntationsthatinadequatelycapturedifferentuncertaintytypes.Futureresearchcoulddevelopmulti-dimensional\nuncertainty frameworks that separately represent epistemic uncertainty (knowledge limitations), aleatoric\nuncertainty (inherent randomness), and model uncertainty (representation limitations). Early steps in\nthis direction appear in systems like Perplexity/DeepResearch [209], which distinguishes between source\nuncertainty and integration uncertainty. More comprehensive approaches could enable more nuanced and\nreliable uncertainty communication.\nImplementation opportunities include developing uncertainty propagation mechanisms that track distinct\nuncertainty types throughout reasoning chains, implementing uncertainty visualization techniques that\neffectively communicate multi-dimensional uncertainty to users, and designing uncertainty-aware planning\nalgorithms that appropriately balance different uncertainty types in decision contexts. These advances could\nenhance both system reliability and appropriate user trust calibration.\nBayesian Reasoning Integration. Probabilistic reasoning frameworks offer principled approaches to un-\ncertainty handling and knowledge integration. Future systems could incorporate explicit Bayesian rea-\nsoning components that systematically update beliefs based on evidence strength and prior knowledge,\nenhancing both accuracy and explainability. Early examples of this direction appear in systems like\n68 Xu et al.\ngrapeot/deep_research_agent[263],whichimplementsbasicevidenceweightingwithinresearchworkflows.\nMore sophisticated integration could enable principled uncertainty handling across domains.\nResearch directions include developing scalable Bayesian inference techniques compatible with large-scale\nlanguage models, implementing belief update explanation mechanisms that communicate reasoning in\nunderstandable terms, anddesigning domain-specific prior models that incorporatefield-specific background\nknowledge for common analysis patterns. These advances could enhance reasoning quality for domains with\ninherent uncertainty or limited evidence.\n8.2 Multi-Modal Deep Research\nExpanding beyond text to incorporate diverse information modalities represents a significant advancement\nopportunity.\n8.2.1 Visual Information Integration. Image understanding dramatically expands information access and\nanalysis capabilities:\nScientificImageAnalysis.Currentsystemsdemonstratelimitedcapabilitiesforextractingandinterpreting\nvisual scientific content. Future research could develop specialized visual understanding components for\nscientific images including graphs, diagrams, experimental images, and visualizations across domains. Early\nsteps in this direction appear in systems like Gemini/DeepResearch [60], which incorporates basic chart\nextraction capabilities. Frameworks such as ChartCitor [96] provide fine-grained bounding box citations to\nenhance explainability for complex chart understanding, improving user trust and productivity. Specialized\nmodels like LHRS-Bot [180] demonstrate sophisticated reasoning capabilities for remote sensing imagery by\nleveraging geographic information and multimodal learning. The development of large-scale, domain-specific\nmultimodal datasets for areas like entomology [272] and seafloor geology [188] is crucial for training more\ncapable models. More comprehensive approaches could enable sophisticated analysis of visual scientific\ncommunication. Implementation opportunities include developing specialized scientific visualization parsers\nthat extract quantitative data from diverse chart types, implementing diagram understanding systems\nthat interpret complex scientific illustrations across domains, and designing domain-specific visual analysis\ncomponents optimized for field-specific imagery like medical scans or astronomical observations. These\nadvances could dramatically expand information access beyond text-centric sources.\nVisualEvidenceIntegration.Effectiveresearchincreasinglyrequiresintegrationofvisualevidencealongside\ntextual sources. Future systems could implement sophisticated multimodal reasoning that incorporates\nvisual evidence within comprehensive analytical frameworks, enabling true multimodal research synthesis.\nRecent analyses have identified multi-modal integration as a key missing capability in current AI research\nsystems [315], highlighting the critical importance of cross-modal reasoning for scientific applications. Early\nexamplesofthisdirectionappearinsystemslikeGemini/DeepResearch[60],whichprovidesbasicintegration\nof image-derived information. More sophisticated approaches could enable balanced evidence integration\nacross modalities.\nResearch directions include developing evidence alignment techniques that match textual and visual\ninformation addressing common questions, implementing cross-modal consistency verification that identifies\nconflicts between textual claims and visual evidence, and designing multimodal synthesis mechanisms that\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 69\ngenerate integrated understanding across information types. These advances could enhance research quality\nfor domains with significant visual information components.\n8.2.2 Multimodal Source Analysis. Comprehensive understanding requires integrated analysis across diverse\ninformation formats:\nVideoContentProcessing. Video represents an increasingly important but currently underutilized infor-\nmation source. Future research could develop specialized video understanding components that extract and\ninterpret temporal visual information, including presentations, interviews, demonstrations, and dynamic\nprocesses. Initial steps in this direction are emerging in systems like OpenAI’s DALL-E 3, though not yet\nintegrated into Deep Research workflows. Comprehensive integration could enable access to the extensive\nknowledge embedded in video content.\nImplementation opportunities include developing lecture understanding systems that extract structured\nknowledgefromeducationalvideos,implementingprocessanalysiscomponentsthatinterpretdemonstrations\nandprocedures,anddesigningintegratedaudio-visualanalysisthatcombinesvisualinformationwithspoken\ncontent for comprehensive understanding. These advances could expand information access to the rapidly\ngrowing corpus of video knowledge.\nAudioContentIntegration.Spokeninformationinpodcasts,lectures,interviews,anddiscussionsrepresents\na valuable knowledge source. Future systems could incorporate sophisticated audio processing that extracts,\ninterprets, and integrates spoken information within research workflows. Early examples of speech process-\ning appear in transcription services, but comprehensive research integration remains limited. Advanced\napproaches could enable seamless incorporation of spoken knowledge alongside traditional text sources.\nResearch directions include developing speaker identification and attribution systems that maintain\nappropriate source tracking for spoken content, implementing domain-specific terminology extraction that\naccurately captures specialized vocabulary in varied acoustic conditions, and designing temporal alignment\ntechniques that connect spoken information with related textual or visual content. These advances could\nexpand information access while maintaining appropriate attribution and context.\n8.2.3 Cross-Modal Reasoning Techniques. Effective multimodal research requires specialized reasoning\napproaches across information types:\nMulti-ModalChainofThoughtReasoning. Current reasoning processes typically operate primarily within\nsingle modalities despite handling diverse information types. Future systems could implement true multi-\nmodal reasoning chains that explicitly incorporate diverse information types throughout the analytical\nprocess, not just in final outputs. Early steps appear in systems like Gemini/DeepResearch [60], which\ndemonstrates basic visual incorporation in reasoning steps. More sophisticated approaches could enable\nreasoningflowsthatseamlesslytransitionbetweentextualanalysis,visualprocessing,numericalcomputation,\nand spatial reasoning based on task requirements.\nResearch opportunities include developing explicit multi-modal reasoning protocols that formalize in-\nformation transfer between modalities, implementing cross-modal verification techniques that leverage\ncomplementary information types throughout reasoning chains, and designing unified representation frame-\nworks that enable coherent reasoning across diverse information formats. These advances could significantly\nenhance reasoning quality for complex research tasks requiring integrated understanding across modalities,\n70 Xu et al.\nmoving beyond the current text-centric reasoning paradigms to more human-like analytical processes that\nnaturally leverage the most appropriate modality for each reasoning component.\nCross-ModalConsistencyVerification.Integratingdiverseinformationmodalitiesintroducesnewconsistency\nchallenges. Future research could develop specialized verification mechanisms that assess consistency across\ntextual,visual,numerical,andtemporalinformation,enhancingoverallreliability.Earlystepsinthisdirection\nappear in systems like Gemini/DeepResearch [60], which implements basic cross-format validation. More\nsophisticated approaches could enable reliable integration of increasingly diverse information types.\nImplementation opportunities include developing cross-modal contradiction detection algorithms that\nidentify conflicts between information expressed in different formats, implementing uncertainty alignment\ntechniques that reconcile confidence estimates across modalities, and designing multimodal fact verification\nsystems that leverage complementary evidence types for enhanced reliability. These advances could address\nemerging challenges in multimodal information integration.\nMultimodalExplanationGeneration.Effectivecommunicationoftenrequirescoordinatedexplanationacross\nmodalities. Future systems could generate truly multimodal research outputs that combine textual, visual,\nand interactive components to enhance understanding and persuasiveness. Early examples of this direction\nappear in systems like mshumer/OpenDeepResearcher [249], which implements basic report visualization.\nMore comprehensive approaches could enable sophisticated multimodal communication tailored to content\nrequirements.\nResearch directions include developing coordinated generation architectures that produce aligned content\nacross modalities, implementing adaptive format selection algorithms that identify optimal representation\nformats for different content types, and designing multimodal narrative structures that effectively combine\ndiverse formats within coherent explanatory frameworks. These advances could enhance communication\neffectiveness across application domains.\n8.3 Domain-Specific Optimization\nTailored enhancement for particular fields offers significant performance improvements for specialized\napplications.\n8.3.1 Scientific Domain Adaptation. Scientific research presents unique requirements and opportunities for\nspecialization:\nField-Specific ModelAdaptation. Current systems employ relatively general architectures across scientific\ndomains. Future research could develop specialized adaptation techniques that optimize performance for\nparticularscientificfieldsincludingphysics,chemistry,biology,andotherswithdistinctknowledgestructures\nand reasoning patterns. Early steps in this direction appear in systems like AutoGLM-Research [330], which\nimplements domain-specific prompting. Domain-specialized research agents have demonstrated particular\npromise in physics [305], chemistry [6, 34, 50, 326], materials science [189], oceanography [28], geospatial\nanalysis [165], patent research [227, 285], and broader scientific discovery workflows [84]. These specialized\nimplementations highlight the value of domain adaptation beyond general research capabilities. More\ncomprehensive adaptation could enable significant performance improvements for scientific applications.\nImplementationapproachesmightincludedomain-specificfine-tuningregimesthatemphasizefield-relevant\nreasoningpatterns,specializedarchitecturalmodificationsthatenhanceperformancefordomain-characteristic\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 71\ntasks, or hybrid systems that incorporate symbolic components for domain-specific formal reasoning. These\napproachescouldaddresscurrentlimitationsinscientificreasoningwhilemaintaininggeneralcapabilitiesfor\ncross-domain research.\nScientific Workflow Integration. Effective scientific application requires integration with existing research\nmethodologies and tools. Future systems could implement specialized interfaces for scientific workflows\nincludingexperimentaldesign,dataanalysis,literatureintegration,andtheorydevelopment.Earlyexamples\nof this direction appear in systems like n8n [183], which provides workflow automation for data processing.\nPlatforms designed to support machine learning development in fundamental science also illustrate this\ntrend, enabling research in federated cloud environments [9]. More comprehensive integration could enable\nseamless incorporation within scientific research processes. Research assistant tools employing prompt-based\ntemplates demonstrate domain-agnostic support for tasks such as enhanced literature search queries and\npreliminarypeerreview,facilitatingstandardizedassistanceacrossdiversescientificfields[245].Userstudies\nhighlight varying automation needs across DS/ML workflows, suggesting targeted rather than complete\nend-to-end automation aligns with researcher preferences [284]. Research opportunities include developing\nexperimentaldesignassistantsthatgenerateandrefineresearchprotocolsbasedonliteratureandobjectives,\nimplementing integrated analysis pipelines that combine automated and human analytical components, and\ndesigning theory development frameworks that link empirical findings with formal theoretical structures.\nThese advances could enhance practical scientific impact beyond general information access [44, 288].\n8.3.2 Legal and Regulatory Domain Specialization. Legal applications present distinct challenges requiring\nspecialized adaptation:\nLegalReasoningEnhancement. Current systems struggle with the precision and structure of legal analysis.\nFuture research could develop specialized legal reasoning components that incorporate case-based reasoning,\nstatutoryinterpretation,anddoctrinalanalysiswithincoherentlegalframeworks.Earlystepsinthisdirection\nappear in systems like OpenAI/DeepResearch [197], which incorporates basic legal language handling. More\ncomprehensive specialization could enable sophisticated legal applications across practice areas.\nImplementation opportunities include developing case analysis systems that extract and apply relevant\nprecedent principles, implementing statutory interpretation frameworks that apply established analytical\nmethodologies to legislative text, and designing multi-jurisdictional reasoning approaches that navigate\nconflictsoflawacrosslegalboundaries.Theseadvancescouldenhancepracticalutilityforlegalresearchand\nanalysis applications.\nRegulatory Compliance Specialization. Compliance applications require comprehensive coverage with\nexceptional precision. Future systems could implement specialized compliance components that ensure\ncomplete regulatory coverage, systematic obligation identification, and reliable guidance across complex\nregulatory landscapes. Early examples of this direction appear in general information retrieval, but true\ncomplianceoptimizationremainslimited.Advancedapproachescouldenablereliableautomationofcurrently\nlabor-intensive compliance processes.\nResearch directions include developing regulatory change tracking systems that monitor and interpret\nevolving requirements, implementing obligation extraction techniques that identify and classify compliance\nrequirementsacrossregulatorytexts,anddesigningresponsibilitymappingapproachesthatconnectregulatory\n72 Xu et al.\nobligations with organizational functions and processes. These advances could enhance practical utility for\ncompliance-intensive industries facing complex regulatory environments.\n8.3.3 Medical and Healthcare Research Support. Healthcare applications present unique requirements and\nethical considerations:\nClinicalEvidenceSynthesis.Medicalapplicationsrequireexceptionalprecisionandcomprehensiveevidence\nintegration. Future research could develop specialized medical components that synthesize clinical evidence\nacrossstudies,guidelines,andpracticeobservationswhilemaintainingrigorousevaluationstandards.Recent\neffortssuchasGoogle’sco-scientistproject[97]demonstratethepotentialforAItoassistinscientificresearch\nincluding medical domains. Early steps in this direction appear in systems like Perplexity/DeepResearch\n[209], which implements enhanced citation for medical claims. More comprehensive specialization could\nenable reliable clinical decision support.\nImplementationapproachesmightincludeevidencegradingsystemsthatapplyestablishedframeworkslike\nGRADE[21]toclinicalresearch,meta-analysiscomponentsthatsystematicallyintegratequantitativefindings\nacrossstudies,andguidelinealignmenttechniquesthatmapevidencetoestablishedclinicalrecommendations.\nThese advances could enhance practical utility for evidence-based medicine while maintaining appropriate\ncaution for this high-stakes domain.\nPatient-Specific Research Adaptation. Personalized medicine requires adapting general knowledge to\nindividual patient contexts. Future systems could implement specialized personalization components that\nadapt research findings based on patient characteristics, comorbidities, preferences, and other individual\nfactors. Early examples of this direction appear in basic filtering of contraindications, but comprehensive\npersonalizationremainslimited.Advancedapproachescouldenabletrulypersonalizedevidencesynthesisfor\nclinical applications.\nResearch opportunities include developing comorbidity reasoning systems that adjust recommendations\nbased on condition interactions, implementing preference integration frameworks that incorporate patient\nvalues in evidence synthesis, and designing personalized risk-benefit analysis approaches that quantify\nindividual trade-offs for treatment options. These advances could enhance clinical utility while respecting\nthe complexity of individual patient contexts.\n8.4 Human-AI Collaboration and Standardization\nEnhancing human-AI partnership and establishing common standards represent crucial directions for\npractical research impact and ecosystem development.\n8.4.1 Interactive Research Workflows. Effective collaboration requires sophisticated interaction throughout\nthe research process:\nAdaptive Query Refinement. Current systems offer limited interaction during query formulation and\nrefinement. Future research could develop sophisticated refinement interfaces that collaboratively develop\nresearch questions through iterative clarification, expansion, and focusing based on initial results and user\nfeedback. Early steps in this direction appear in systems like HKUDS/Auto-Deep-Research [112], which\nimplements basic clarification dialogues, and benchmarks such as QuestBench [141], which evaluates\nAI systems’ ability to identify missing information and formulate appropriate clarification questions in\nunderspecified reasoning tasks. More comprehensive approaches could enable truly collaborative question\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 73\ndevelopment. Frameworks like AutoAgent [262] demonstrate how zero-code interfaces can enable non-\ntechnicaluserstoeffectivelyguidedeepresearchprocessesthroughintuitiveinteractionpatterns,whileother\nsystems are exploring methods that go beyond standard retrieval-augmented generation to better handle\nquestionidentificationinreal-timeconversations[4].Implementationopportunitiesincludedevelopingintent\nclarification systems thatidentify potential ambiguities andalternatives in research questions,implementing\nscope adjustment interfaces that dynamically expand or narrow research focus based on initial findings, and\ndesigning perspective diversification tools that suggest alternative viewpoints relevant to research objectives.\nThese advances could enhance research quality by improving question formulation through human-AI\ncollaboration.\nInteractive Exploration Interfaces. Current systems typically present relatively static research outputs.\nFuture research could develop sophisticated exploration interfaces that enable dynamic navigation, drilling\ndown, and expansion across research findings based on evolving interests. Early examples of this direction\nappear in systems like OpenManus [193], which provides basic exploration capabilities. Advanced approaches\ncould enable truly interactive research experiences tailored to discovery patterns.\nResearch directions include developing information visualization techniques specifically designed for\nresearch navigation, implementing adaptive detail management that expands or collapses content areas\nbased on user interest signals, and designing seamless source transition mechanisms that enable smooth\nmovementbetweensynthesisandoriginalsources.Theseadvancescouldenhancediscoverybyenablingmore\nexploratory and serendipitous research experiences.\n8.4.2 Expertise Augmentation Models. Effective augmentation requires adaptation to user expertise and\nobjectives:\nExpertise-Adaptive Interaction. Current systems offer limited adaptation to user knowledge levels and\nexpertise.Futureresearchcoulddevelopsophisticatedadaptationmechanismsthattailorresearchapproaches,\nexplanations, and outputs based on user domain knowledge and research sophistication. Early steps in\nthis direction appear in systems like Perplexity/DeepResearch [209], which implements basic terminology\nadjustment. More comprehensive adaptation could enable truly personalized research assistance aligned\nwith individual expertise.\nImplementation approaches might include expertise inference systems that dynamically assess user knowl-\nedge through interaction patterns, explanation adaptation mechanisms that adjust detail and terminology\nbased on expertise models, and knowledge gap identification tools that highlight potentially unfamiliar\nconcepts within research contexts. Furthermore, mechanisms that learn to strategically request expert\nassistancewhenencounteringgapsexceedingautonomouscapability-asformalizedintheLearningtoYield\nand Request Control (YRC) coordination problem [66] - are crucial for optimizing intervention timing and\nresolution effectiveness. These advances could enhance research effectiveness across diverse user populations\nwith varying domain familiarity.\nComplementary Capability Design. Optimal augmentation leverages complementary human and AI\nstrengths.Futuresystemscouldimplementspecializedinterfacesdesignedaroundcapabilitycomplementarity,\nemphasizing AI contributions in information processing while prioritizing human judgment for subjective\nevaluation and contextual understanding. Early examples of this direction appear in systems like Agent-\n74 Xu et al.\nRL/ReSearch[2],whichimplementsbasicdivisionofanalyticalresponsibilities.Moresophisticatedapproaches\ncould enable truly synergistic human-AI research partnerships.\nResearch opportunities include developing explanation components specifically designed to facilitate\nhuman judgment rather than replace it, implementing confidence signaling mechanisms that highlight areas\nparticularly requiring human evaluation, and designing interactive critique frameworks that enable efficient\nhuman feedback on system reasoning. Feng Xiong et al. [303] redefine the collaborative dynamics between\nhuman researchers and AI systems. These advances could enhance collaborative effectiveness by optimizing\naround natural capability distributions.\n8.4.3 FrameworkStandardizationEfforts. Commonarchitecturesenablemodulardevelopmentandcomponent\ninteroperability:\nComponent Interface Standardization. Advanced implementations employ standardized interfaces be-\ntween major system components. The OpenAI/AgentsSDK [199] defines explicit interface standards for\nagent components, enabling modular development and component substitution. Emerging industry stan-\ndards like Anthropic’s Model Context Protocol (MCP) [12] provide standardized interaction frameworks\nfor large language models and tools, enabling consistent integration patterns across implementations.\nSimilarly, Google’s Agent2Agent Protocol (A2A) [90, 92] establishes standardized communication pat-\nterns between autonomous agents, facilitating reliable multi-agent coordination. Open-source alternatives\nlike smolagents/open_deep_research [115] implement comparable messaging protocols between agent\ncomponents, highlighting industry convergence toward standardized interaction patterns. Projects like\nOpen_deep_search [8] further demonstrate how standardized protocols enable effective collaboration be-\ntween specialized research agents. Integration of diverse API interactions, as explored in Toolllm [223],\nprovidesadditionalstandardizationopportunitiesformanagingexternaltoolusagewithinresearchworkflows.\nEvaluation Metric Standardization. Current evaluation practices vary widely across implementations.\nFuture research could establish standardized evaluation frameworks that enable consistent assessment and\ncomparison across systems and components. Early examples of this direction appear in benchmarks like\nHLE [212] and MMLU [33], but comprehensive standardization remains limited. Advanced standardization\ncould enable more efficient development through reliable quality signals and clear improvement metrics.\nResearch opportunities include developing standardized benchmark suites targeting specific research\ncapabilities, implementing common evaluation methodologies across research domains and applications,\nand designing multi-dimensional assessment frameworks that provide nuanced performance profiles beyond\nsimple accuracy metrics. These advances could enhance ecosystem quality by establishing clear standards\nand highlighting genuine improvements.\n8.4.4 Cross-PlatformResearchProtocols. Interoperabilityacrossdiversesystemsenhancescollectivecapabili-\nties:\nResearchResultExchangeFormats. Current systems typically produce outputs in incompatible formats.\nFuture research could develop standardized exchange formats that enable seamless sharing of research\nresults across platforms and systems, enhancing collective capabilities. Early steps in this direction appear\nin basic document formats, but true research-specific standardization remains limited. Comprehensive\nstandardization could enable research workflows spanning multiple specialized systems.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 75\nImplementation opportunities include defining standard structures for research findings with appropriate\nattributionandconfidencemetadata,establishingcommonformatsforevidencerepresentationacrosssystems,\nand developing shared schemas for research questions and objectives to enable distributed processing. These\nadvances could enhance capability through specialization and complementary system utilization.\nDistributedResearchCoordination.Advancedinteroperabilityenablescoordinatedresearchacrosssystems\nwith complementary capabilities. Future research could develop sophisticated coordination frameworks that\nenable multi-system research workflows with appropriate task allocation, result integration, and process\nmanagement. Early examples of this direction appear in workflows like those enabled by n8n [183], but\ncomprehensive research-specific coordination remains limited. Advanced approaches could enable truly\ndistributed research ecosystems with specialized components addressing distinct process elements.\nResearch directions include developing distributed search coordination protocols that efficiently leverage\nspecialized search capabilities, implementing cross-system result verification techniques that ensure consis-\ntencyacrossdistributedfindings,anddesigningefficientcoordinationprotocolsthatminimizecommunication\noverhead in distributed research workflows. These advances could enhance collective capability through\nspecialization and parallelization across the ecosystem.\n8.4.5 Joint Human-AI Knowledge Creation. Moving beyond information retrieval to collaborative insight\ngeneration:\nCollaborativeCreationEnvironments. Advanced collaboration requires sophisticated content co-creation\ncapabilities.Futureresearchcoulddevelopspecializedcollaborativeenvironmentsthatenablefluidtransition\nbetween human and AI contributions within unified document development. Early steps in this direction\nappearinsystemslikemshumer/OpenDeepResearcher,whichimplementsbasiccollaborativedocumentgener-\nation. Advanced interfaces like those explored in Self-Explanation in Social AI Agents [23] demonstrate how\nexplanation capabilities can enhance collaborative research through more transparent reasoning processes.\nSimilarly, innovative interaction paradigms like AI-Instruments [232] show how prompts can be embodied\nas instruments to abstract and reflect commands as general-purpose tools, suggesting novel approaches\nto research interface design that enhance collaborative capabilities through intuitive interaction patterns.\nApproacheswhereAIagentslearntoassistotheragentsbyobservingthemalsoshowpromisefordeveloping\nmore effective collaborative behaviors [127]. Effidit demonstrates comprehensive writing support through\nmultifunctional capabilities including text polishing and context-aware phrase refinement, extending collabo-\nrative editing beyond basic generation [248]. More comprehensive approaches could enable truly integrated\nco-creation experiences.\nImplementation opportunities include developing section suggestion systems that propose potential\ncontent expansions based on document context, implementing stylistic adaptation mechanisms that align\nAI-generated content with established document voice and approach, and incorporating implicit feedback\nmechanismsthatinterpretrejectedsuggestionsasnegativesignalstorefineoutputswhilepreservingoriginal\nintent [271], and designing seamless revision interfaces that enable efficient editing across human and AI\ncontributions, like iterative human-AI co-editing as demonstrated by REVISE [302] – a framework allowing\nwriters to dynamically modify summary segments through fill-in-the-middle generation. These advances\ncould enhance collaborative productivity by reducing friction in joint content development [116].\n76 Xu et al.\nMixed-InitiativeResearchDesign. Sophisticated collaboration includes shared determination of research\ndirection and approach. Future systems could implement mixed-initiative frameworks that dynamically\nbalancedirectionsettingbetweenhumanpreferencesandAI-identifiedopportunitiesthroughouttheresearch\nprocess. Early examples of this direction appear in systems like smolagents/open_deep_research [115],\nwhich implements basic suggestion mechanisms. Advanced approaches could enable truly collaborative\nresearch planning with balanced initiative distribution.\nResearch directions include developing opportunity identification systems that highlight promising but\nunexplored research directions, implementing trade-off visualization techniques that communicate potential\nresearch path alternatives and implications, and designing preference elicitation frameworks that efficiently\ncapture evolving research priorities throughout the process, and integrating explainable reward function\nmechanismstoenhancehumanunderstandingofAI’sdecisionlogic,therebyimprovingcollaborativeefficiency\nin value alignment contexts [239]. These advances could enhance discovery by combining human insight\nwith AI-identified opportunities in balanced partnerships.\nThe future research directions outlined in this section highlight both the significant potential for advance-\nmentandthemulti-facetednatureofDeepResearchdevelopment.Progresswilllikelyemergethroughcomple-\nmentary advances across reasoning architectures, multimodal capabilities, domain specialization, human-AI\ncollaboration,andecosystemstandardization.WhilecommercialimplementationslikeOpenAI/DeepResearch\n[197], Gemini/DeepResearch [60], and Perplexity/DeepResearch [209] will undoubtedly drive significant\ninnovation,open-sourcealternativesandacademicresearchwillplaycrucialrolesinexpandingtheboundaries\nof what’s possible and ensuring broad participation in this rapidly evolving field.\n9 Conclusion\nThis survey has examined the rapidly evolving domain of Deep Research systems, tracing their development\nfrom initial implementations in 2023 through the sophisticated ecosystem emerging in 2025. Through\ncomprehensive analysis of commercial offerings like OpenAI/DeepResearch [197], Gemini/DeepResearch\n[60], and Perplexity/DeepResearch [209], alongside open-source alternatives including HKUDS/Auto-Deep-\nResearch[112],dzhng/deep-research[321],andnumerousothers,wehaveidentifiedkeytechnicalpatterns,\nimplementation approaches, and application opportunities that characterize this transformative technology\ndomain.\n9.1 Key Findings and Contributions\nOur analysis reveals several fundamental insights about the current state and trajectory of Deep Research\nsystems:\nTechnicalArchitecturePatterns. EffectiveDeepResearchimplementationsdemonstrateconsistentarchitec-\ntural patterns across foundation models, environmental interaction, task planning, and knowledge synthesis\ndimensions. Commercial implementations like OpenAI/DeepResearch [197] and Gemini/DeepResearch [60]\ntypically leverage proprietary foundation models with extensive context lengths and sophisticated reasoning\ncapabilities, while open-source alternatives like Camel-AI/OWL [43] and QwenLM/Qwen-Agent [224] demon-\nstrate how effective research capabilities can be achieved with more accessible models through specialized\noptimization.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 77\nEnvironmental interaction capabilities show greater diversity, with specialized tools like Nanobrowser\n[184]anddzhng/deep-research[321]demonstratingexceptionaleffectivenessinwebnavigationandcontent\nextraction,whilecomprehensiveplatformslikeManus[164]andAutoGLM-Search[330]offerbroaderinteraction\ncapabilities across multiple environments. These patterns highlight both the value of specialization and the\nimportance of comprehensive environmental access for effective research.\nTask planning and execution approaches reveal similar diversity, with frameworks like OpenAI/AgentsSDK\n[199] and Flowith/OracleMode [77] providing sophisticated planning capabilities, while systems like Agent-\nRL/ReSearch [2] and smolagents/open_deep_research [115] emphasize execution reliability and collabora-\ntiveapproachesrespectively.Knowledgesynthesiscapabilitiesdemonstrateconsistentemphasisoninformation\nevaluation, though with varied approaches to presentation and interactivity across implementations like\nHKUDS/Auto-Deep-Research [112] and mshumer/OpenDeepResearcher [249].\nImplementation Approach Distinctions. Our analysis highlights meaningful distinctions between commer-\ncialandopen-sourceimplementationapproaches.Commercialplatformstypicallyofferoptimizedperformance,\nsophisticated interfaces, and comprehensive capabilities, though with associated costs and customization\nlimitations. Systems like OpenAI/DeepResearch [197] and Perplexity/DeepResearch [209] demonstrate\nexceptionalperformanceonstandardbenchmarks,thoughwithsignificantvariationinapplicationfocusand\ninteraction models.\nOpen-source implementations demonstrate greater architectural diversity and customization flexibil-\nity, though typically with increased deployment complexity and more limited performance on stan-\ndard benchmarks. Projects like dzhng/deep-research [321], nickscamara/open-deep-research [42], and\nHKUDS/Auto-Deep-Research [112] offer complete research pipelines with varied architectural approaches,\nwhile specialized components like Jina-AI/node-DeepResearch [121] and Nanobrowser [184] enable cus-\ntomized workflows addressing specific requirements. Frameworks such as AutoChain [78] provide lightweight\ntools to simplify the creation and evaluation of custom generative agents, enabling rapid iteration for\nspecialized applications.\nThesedistinctionshighlightcomplementaryroleswithintheecosystem,withcommercialimplementations\noffering accessibility and performance for general users, while open-source alternatives enable customization,\ncontrol, and potentially lower operational costs for specialized applications and high-volume usage. This\ndiversity enhances overall ecosystem health through competition, specialization, and diverse innovation\npaths.\nApplication Domain Adaptations. Ourexaminationofapplicationpatternsrevealsmeaningfuladaptations\nacross domains including academic research[118, 273, 276], scientific discovery[6, 10, 25, 47, 79, 83, 98, 99,\n110, 129, 130, 135, 155, 166, 169, 218, 255, 258, 264, 269, 310, 312, 322, 327], business intelligence[187],\nfinancial analysis, education[14, 215, 219, 317], and personal knowledge management[136, 336]. Academic\napplications exemplified by systems like OpenAI/DeepResearch [197] and Camel-AI/OWL [43] demonstrate\nparticular emphasis on comprehensive literature coverage, methodological understanding, and citation\nquality. Scientific implementations like Gemini/DeepResearch [60] and Agent-RL/ReSearch [2] emphasize\nexperimental design, data analysis, and theory development capabilities.\nBusinessapplicationsleveragingsystemslikeManus[164]andn8n[183]showstrongerfocusoninformation\ncurrency, competitive analysis, and actionable insight generation. Educational implementations demonstrate\n78 Xu et al.\nadaptations for learning support, content development, and research skill training across systems like\nPerplexity/DeepResearch [209] and OpenManus [193]. These patterns highlight how general deep research\ncapabilitiestranslateintodomainvaluethroughspecializedadaptationaddressingfield-specificrequirements\nand workflows.\nEthical Consideration Approaches. Our analysis reveals both common patterns and implementation diver-\nsity in addressing crucial ethical dimensions including information accuracy, privacy protection, intellectual\nproperty respect, and accessibility. Commercial implementations typically demonstrate sophisticated ap-\nproachestofactualverification,withsystemslikeOpenAI/DeepResearch[197]andPerplexity/DeepResearch\n[209] implementing multi-level verification and explicit attribution, while open-source alternatives like\ngrapeot/deep_research_agent [263] and HKUDS/Auto-Deep-Research [112] demonstrate pragmatic ap-\nproaches within more constrained technical environments.\nPrivacy protection shows similar patterns, with commercial systems implementing comprehensive safe-\nguards appropriate to their cloud-based operation, while open-source alternatives like OpenManus [193]\nemphasize local deployment for sensitive applications. Attribution and intellectual property approaches\ndemonstrate consistent emphasis on source transparency and appropriate utilization boundaries, though\nwith varied implementation sophistication across the ecosystem.\nThese patterns highlight both shared ethical priorities across the ecosystem and implementation diversity\nreflectingdifferenttechnicalconstraints,deploymentmodels,anduserrequirements.Thisdiversityrepresents\na strength in addressing multi-faceted ethical challenges through complementary approaches and continuous\ninnovation.\n9.2 Limitations and Outlook\nWhile this survey provides comprehensive analysis of current Deep Research systems and emerging trends,\nseveral limitations warrant acknowledgment:\nRapidly Evolving Landscape. The accelerating pace of development in this domain presents inherent\nchallenges for comprehensive analysis. New systems and capabilities continue to emerge, with commercial\nofferingslikeOpenAI/DeepResearch[197],Gemini/DeepResearch[60],andPerplexity/DeepResearch[209]\nreceiving frequent updates, while the open-source ecosystem continuously expands through new projects\nandenhancementstoexistingframeworkslikedzhng/deep-research[321]andHKUDS/Auto-Deep-Research\n[112].\nThissurveycapturesthestateoftheartasofearly2025,butbothtechnicalcapabilitiesandimplementation\napproaches will continue to evolve rapidly. The classification framework and analysis methodology provided\nhere offer a structural foundation for continued assessment as the field progresses through subsequent\ndevelopment phases.\nImplementation Detail Limitations. Comprehensive technical analysis faces challenges due to limited\nimplementation transparency, particularly for commercial systems. While open-source implementations\nlikenickscamara/open-deep-research[42]andAgent-RL/ReSearch[2]enabledetailedarchitecturalexam-\nination, commercial systems like OpenAI/DeepResearch [197] and Gemini/DeepResearch [60] reveal limited\ninternal details, restricting comprehensive comparative analysis of certain technical dimensions.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 79\nOur approach addresses this limitation through behavioral analysis, publicly available documentation\nexamination, and consistent evaluation across standardized benchmarks and qualitative assessment frame-\nworks. These methods enable meaningful comparison despite transparency variations, though complete\narchitectural analysis remains challenging for proprietary implementations.\nApplication Impact Assessment. Evaluating real-world impact presents persistent challenges given the\nearly deployment stage of many Deep Research systems. While initial applications demonstrate promis-\ning capabilities across domains including academic research[17, 208, 225, 292], business intelligence, and\neducation[14,215,317],acomprehensivelong-termimpactassessmentrequiresextendedobservationbeyond\nthe scope of this survey. Potential transformative effects on research methodologies, knowledge work, and\ninformation access patterns remain partially speculative despite encouraging early indications.\nFuture research should incorporate longitudinal analysis of deployment patterns, usage evolution, and\norganizational integration to assess realized impact beyond technical capabilities and early applications.\nSuch analysis would complement the technical and architectural focus of the current survey with valuable\nperspectives on practical significance and societal implications.\n9.3 Broader Implications\nBeyondspecificfindings,thissurveyhighlightsseveralbroaderimplicationsforthefutureofknowledgework\nand information access:\nResearch Methodology Transformation. Deep Research systems demonstrate potential to fundamentally\ntransformresearchmethodologiesacrossdomains.Thecomprehensiveinformationaccess,advancedreasoning\ncapabilities, and efficient knowledge synthesis demonstrated by systems like OpenAI/DeepResearch [197],\nGemini/DeepResearch[60],andtheiropen-sourcealternativessuggestsignificantopportunitiestoaccelerate\ndiscovery,enhancecomprehensiveness,andenablenovelcross-domainconnectionsbeyondtraditionalresearch\napproaches.\nRather than simply automating existing processes, these systems enable fundamentally new research\napproaches leveraging capabilities exceeding human information processing in scale while complementing\nhuman insight, creativity, and contextual understanding. This complementarity suggests evolution toward\ncollaborative research models rather than replacement of human researchers, with significant potential\nfor productivity enhancement and discovery acceleration. However, Ashktorab et al. [15] highlight that in\nhuman-AI collaboration, users may exhibit overreliance behaviors, appending AI-generated responses even\nwhen conflicting, which can compromise data quality.\nKnowledge Access Democratization. The emergence of accessible Deep Research implementations across\ncommercial and open-source ecosystems demonstrates potential for broader knowledge democratization.\nSystems like Perplexity/DeepResearch [209] with free access tiers and open-source alternatives like\nnickscamara/open-deep-research[42]andHKUDS/Auto-Deep-Research[112]enablesophisticatedresearch\ncapabilitiespreviouslyrequiringspecializedexpertiseandsubstantialresources,potentiallyreducingbarriers\nto high-quality information access and analysis.\nThis democratization carries significant implications for education, entrepreneurship, civic participation,\nandindividualknowledgedevelopment.Whileaccessibilitychallengesremain,particularlyregardingtechnical\n80 Xu et al.\nexpertise requirements and computational resources, the overall trajectory suggests broadening access to\nadvanced research capabilities with potential positive impacts on knowledge equity across society.\nCollective Intelligence Enhancement. Beyondindividualapplications,DeepResearchsystemsdemonstrate\npotentialforcollectiveintelligenceenhancementthroughimprovedknowledgeintegration,insightsharing,and\ncollaborativediscovery.ThecapabilitiesdemonstratedbysystemslikeManus[164],Flowith/OracleMode[77],\nand smolagents/open_deep_research [115] suggest opportunities for enhanced knowledge synthesis across\norganizational and disciplinary boundaries, potentially addressing fragmentation challenges in increasingly\ncomplex knowledge domains.\nRather than viewing these systems as isolated tools, their integration into collaborative knowledge\necosystemshighlightspotentialforsystemicenhancementofcollectivesense-making,evidence-baseddecision\nmaking, and shared understanding development. This perspective emphasizes the social and organizational\ndimensions of Deep Research impact beyond technical capabilities and individual productivity enhancement.\n9.4 Final Thoughts\nThe rapid emergence and evolution of Deep Research systems represent a significant advancement in the\napplication of artificial intelligence to knowledge discovery and utilization. While technical implementations\nwillcontinuetoevolveandspecificsystemswillemergeandrecede,thefundamentalcapabilityshiftenabled\nby these technologies appears likely to persist and expand.\nThediverseecosystemspanningcommercialplatformslikeOpenAI/DeepResearch[197],Gemini/DeepResearch\n[60], and Perplexity/DeepResearch [209], alongside open-source alternatives like dzhng/deep-research\n[321], HKUDS/Auto-Deep-Research [112], and numerous specialized components, demonstrates innovation\nacross multiple technical dimensions, implementation approaches, and application domains. This diversity\nenhances overall ecosystem health through competition, specialization, and complementary development\ntrajectories.\nAsresearchcontinuesacrossadvancedreasoningarchitectures,multimodalcapabilities,domainspecializa-\ntion, human-AI collaboration, and ecosystem standardization, we anticipate continued rapid advancement\nbuilding on the foundation established by current implementations. This evolution will likely yield increas-\ningly sophisticated research capabilities with significant implications for knowledge work across domains,\npotentially transforming how information is discovered, validated, synthesized, and utilized throughout\nsociety.\nThe responsible development of these powerful capabilities requires continued attention to ethical consid-\nerations including information accuracy, privacy protection, intellectual property respect, and accessibility.\nByaddressingtheseconsiderationsalongsidetechnicaladvancement,theDeepResearchecosystemcanfulfill\nits potential for positive impact on knowledge discovery and utilization while minimizing potential harms or\nmisuse.\nIn conclusion, Deep Research represents both a fascinating technical domain for continued research and a\npotentially transformative capability for practical knowledge work across society. The frameworks, analysis,\nand directions presented in this survey provide a foundation for continued examination of this rapidly\nevolving field with significant implications for the future of information access, knowledge synthesis, and\ndiscovery processes.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 81\nReferences\n[1] AdilzhanAdilkhanov,AmirYelenov,AssylkhanSeitzhanov,AyanMazhitov,AzamatAbdikarimov,DanissaSandyk-\nbayeva,DarynKenzhebek,DinmukhammedMukashev,IlyasUmurbekov,JabrailChumakov,KamilaSpanova,Karina\nBurunchina,MadinaYergibay,MargulanIssa,MoldirZabirova,NurdauletZhuzbay,NurlanKabdyshev,NurlanZhani-\nyar,RasulYermagambet,RustamChibar,SaltanatSeitzhan,SoibkhonKhajikhanov,TasbolatTaunyazov,Temirlan\nGalimzhanov,TemirlanKaiyrbay,TleukhanMussin,TogzhanSyrymova,ValeriyaKostyukova,YerkebulanMassalim,\nYermakhan Kassym, Zerde Nurbayeva, and Zhanat Kappassov. 2025. Survey on Vision-Language-Action Models.\narXiv:2502.06851[cs.CL] https://arxiv.org/abs/2502.06851\n[2] Agent-RL.2024. ReSearch. https://github.com/Agent-RL/ReSearch.\n[3] Agno-AGI.2025. Agno. https://github.com/agno-agi/agno.\n[4] GarimaAgrawal,SashankGummuluri,andCosimoSpera.2024. Beyond-RAG:QuestionIdentificationandAnswer\nGenerationinReal-TimeConversations. arXiv:2410.10136[cs.CL] https://arxiv.org/abs/2410.10136\n[5] FlowiseAI.2023. Flowise:Low-codeLLMApplicationBuildingTool. https://flowiseai.com/.\n[6] NawafAlampara,MaraSchilling-Wilhelmi,MartiñoRíos-García,IndrajeetMandal,PranavKhetarpal,HargunSingh\nGrover,N.M.AnoopKrishnan,andKevinMaikJablonka.2025. Probingthelimitationsofmultimodallanguage\nmodelsforchemistryandmaterialsresearch. arXiv:2411.16955[cs.LG] https://arxiv.org/abs/2411.16955\n[7] AlphaProofandAlphaGeometryteams.2024. AIachievessilver-medalstandardsolvingInternationalMathematical\nOlympiadproblems. https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/.\n[8] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan\nJiang,ArdaKaz,WindsorNguyen,SewoongOh,HimanshuTyagi,andPramodViswanath.2025. OpenDeepSearch:\nDemocratizingSearchwithOpen-sourceReasoningAgents.arXiv:2503.20201[cs.LG] https://arxiv.org/abs/2503.20201\n[9] LucioAnderlini,MatteoBarbetti,GiulioBianchini,DiegoCiangottini,StefanoDalPra,DiegoMichelotto,Carmelo\nPellegrino, Rosa Petrini, Alessandro Pascolini, and Daniele Spiga. 2025. Supporting the development of Machine\nLearning for fundamental science in a federated Cloud with the AI_INFN platform. arXiv:2502.21266 [cs.DC]\nhttps://arxiv.org/abs/2502.21266\n[10] Mehrad Ansari and Seyed Mohamad Moosavi. 2023. Agent-based Learning of Materials Datasets from Scientific\nLiterature. https://github.com/AI4ChemS/Eunomia. arXiv:2312.11690[cs.AI] https://arxiv.org/abs/2312.11690\n[11] Anthropic.2024. Buildingeffectiveagents. https://www.anthropic.com/engineering/building-effective-agents.\n[12] Antropic.2024. ModelContextProtocol(MCP). https://docs.anthropic.com/en/docs/agents-and-tools/mcp.\n[13] Antropic.2025. Claudetakesresearchtonewplaces. https://www.anthropic.com/news/research.\n[14] PrakashAryan.2024. LLMsasDebatePartners:UtilizingGeneticAlgorithmsandAdversarialSearchforAdaptive\nArguments. arXiv:2412.06229[cs.AI] https://arxiv.org/abs/2412.06229\n[15] ZahraAshktorab,QianPan,WernerGeyer,MichaelDesmond,MarinaDanilevsky,JamesM.Johnson,CaseyDugan,\nand Michelle Bachman. 2024. Emerging Reliance Behaviors in Human-AI Text Generation: Hallucinations, Data\nQualityAssessment,andCognitiveForcingFunctions. arXiv:2409.08937[cs.HC] https://arxiv.org/abs/2409.08937\n[16] assafelovic.2023. GPT-Researcher. https://github.com/assafelovic/gpt-researcher/.\n[17] AhmetYasinAytar,KemalKilic,andKamerKaya.2024. ARetrieval-AugmentedGenerationFrameworkforAcademic\nLiteratureNavigationinDataScience. arXiv:2412.15404[cs.IR] https://arxiv.org/abs/2412.15404\n[18] JinheonBaek,SujayKumarJauhar,SilviuCucerzan,andSungJuHwang.2025. ResearchAgent:IterativeResearch\nIdeaGenerationoverScientificLiteraturewithLargeLanguageModels. arXiv:2404.07738[cs.CL] https://arxiv.org/\nabs/2404.07738\n[19] DzmitryBahdanau,NicolasGontier,GabrielHuang,EhsanKamalloo,RafaelPardinas,AlexPiché,TorstenScholak,\nOlehShliazhko,JordanPrinceTremblay,KaramGhanem,SohamParikh,MitulTiwari,andQuaizarVohra.2024.\nTapeAgents: a Holistic Framework for Agent Development and Optimization. arXiv:2412.08445 [cs.AI] https:\n//arxiv.org/abs/2412.08445\n[20] GalBakal,AliDasdan,YanivKatz,MichaelKaufman,andGuyLevin.2025. ExperiencewithGitHubCopilotfor\nDeveloperProductivityatZoominfo. arXiv:2501.13282[cs.SE] https://arxiv.org/abs/2501.13282\n[21] HowardBalshem,MarkHelfand,HolgerJSchünemann,AndrewDOxman,ReginaKunzandJanBrozek,GunnEVist,\nYngveFalck-Ytter,JoergMeerpohl,SusanNorris,andGordonHGuyatt.2011. GRADEguidelines:3.Ratingthe\nqualityofevidence. https://pubmed.ncbi.nlm.nih.gov/21208779/.\n[22] SamuelBarham,OrionWeller,MichelleYuan,KentonMurray,MahsaYarmohammadi,ZhengpingJiang,Siddharth\nVashishtha, Alexander Martin, Anqi Liu, Aaron Steven White, Jordan Boyd-Graber, and Benjamin Van Durme.\n2023. MegaWika: Millions of reports and their sources across 50 diverse languages. arXiv:2307.07049 [cs.CL]\nhttps://arxiv.org/abs/2307.07049\n82 Xu et al.\n[23] RheaBasappa,MustafaTekman,HongLu,BenjaminFaught,SandeepKakar,andAshokK.Goel.2024. SocialAI\nAgentsTooNeedtoExplainThemselves. SpringerNatureSwitzerland,351–360. doi:10.1007/978-3-031-63028-6_29\n[24] JoeranBeel,Min-YenKan,andMoritzBaumgart.2025. EvaluatingSakana’sAIScientistforAutonomousResearch:\nWishfulThinkingoranEmergingRealityTowards’ArtificialResearchIntelligence’(ARI)? arXiv:2502.14297[cs.IR]\nhttps://arxiv.org/abs/2502.14297\n[25] Morad Behandish, John Maxwell III, and Johan de Kleer. 2022. AI Research Associate for Early-Stage Scientific\nDiscovery. arXiv:2202.03199[cs.AI] https://arxiv.org/abs/2202.03199\n[26] Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt MacDermott, Sören\nMindermann,AdamOberman,JesseRichardson,OliverRichardson,Marc-AntoineRondeau,Pierre-LucSt-Charles,\nandDavidWilliams-King.2025. SuperintelligentAgentsPoseCatastrophicRisks:CanScientistAIOfferaSaferPath?\narXiv:2502.15657[cs.AI] https://arxiv.org/abs/2502.15657\n[27] KarimBenharrak,TimZindulka,andDanielBuschek.2024. DeceptivePatternsofIntelligentandInteractiveWriting\nAssistants. arXiv:2404.09375[cs.HC] https://arxiv.org/abs/2404.09375\n[28] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2024. OceanGPT:\nA Large Language Model for Ocean Science Tasks. http://oceangpt.zjukg.cn/. arXiv:2310.02031 [cs.CL] https:\n//arxiv.org/abs/2310.02031\n[29] StefanoBianchini,MoritzMüller,andPierrePelletier.2024. DriversandBarriersofAIAdoptionandUseinScientific\nResearch. arXiv:2312.09843[cs.CY] https://arxiv.org/abs/2312.09843\n[30] bindAI.2025. ChatGPTDeepResearchvsPerplexity–WhichOneIsBetter? https://blog.getbind.co/2025/02/03/\nchatgpt-deep-research-is-it-better-than-perplexity/.\n[31] FranciscoBolanos,AngeloSalatino,FrancescoOsborne,andEnricoMotta.2024. ArtificialIntelligenceforLiterature\nReviews:OpportunitiesandChallenges. arXiv:2402.08565[cs.AI] https://arxiv.org/abs/2402.08565\n[32] Bolt.2024. Bolt. https://bolt.new/.\n[33] bracai.2025.MMLUbenchmark:TestingLLMsmulti-taskcapabilities.https://www.bracai.eu/post/mmlu-benchmark.\n[34] AndresMBran,SamCox,OliverSchilter,CarloBaldassari,AndrewDWhite,andPhilippeSchwaller.2023.ChemCrow:\nAugmentinglarge-languagemodelswithchemistrytools. arXiv:2304.05376[physics.chem-ph] https://arxiv.org/abs/\n2304.05376\n[35] ChrisBrownandJasonCusati.2024. ExploringtheEvidence-BasedBeliefsandBehaviorsofLLM-BasedProgramming\nAssistants. arXiv:2407.13900[cs.SE] https://arxiv.org/abs/2407.13900\n[36] browserbase.2025. Open-operator. https://github.com/browserbase/open-operator.\n[37] btahir.2024. open_deep_research. https://github.com/btahir/open-deep-research.\n[38] ByteDance.2024. CozeSpace. https://www.coze.cn/space-preview.\n[39] ByteDance.2025. agent-tars. https://github.com/bytedance/UI-TARS-desktop/tree/main/apps/agent-tars.\n[40] BeatrizCabrero-Daniel,TomasHerda,VictoriaPichler,andMartinEder.2024. ExploringHuman-AICollaborationin\nAgile:CustomisedLLMMeetingAssistants. arXiv:2404.14871[cs.SE] https://arxiv.org/abs/2404.14871\n[41] FilipeCalegario,VanilsonBurégio,FranciscoErivaldo,DanielMoraesCostaAndrade,KailaneFelix,NathaliaBarbosa,\nPedro Lucas da Silva Lucena, and César França. 2023. Exploring the intersection of Generative AI and Software\nDevelopment. arXiv:2312.14262[cs.SE] https://arxiv.org/abs/2312.14262\n[42] NicholasCamara.2025. open-deep-research. https://github.com/nickscamara/open-deep-research.\n[43] CamelAI.2025. OWL. https://github.com/camel-ai/owl.\n[44] FranckCappello,SandeepMadireddy,RobertUnderwood,NeilGetty,NicholasLee-PingChia,NesarRamachandra,\nJoshNguyen,MuratKeceli,TanwiMallick,ZilinghanLi,MariemeNgom,ChenhuiZhang,AngelYanguas-Gil,Evan\nAntoniuk,BhavyaKailkhura,MinyangTian,YufengDu,Yuan-SenTing,AztonWells,BogdanNicolae,AvinashMaurya,\nM.MustafaRafique,EliuHuerta,BoLi,IanFoster,andRickStevens.2025. EAIRA:EstablishingaMethodologyfor\nEvaluatingAIModelsasScientificResearchAssistants. arXiv:2502.20309[cs.AI] https://arxiv.org/abs/2502.20309\n[45] PeterCardon,CarolinFleischmann,JolantaAritz,MinnaLogemann,andJeanetteHeidewald.2023. TheChallenges\nandOpportunitiesofAI-AssistedWriting:DevelopingAILiteracyfortheAIAge. https://journals.sagepub.com/doi/\nabs/10.1177/23294906231176517.\n[46] MertCemri,MelissaZ.Pan,ShuyiYang,LakshyaA.Agrawal,BhavyaChopra,RishabhTiwari,KurtKeutzer,Aditya\nParameswaran,DanKlein,KannanRamchandran,MateiZaharia,JosephE.Gonzalez,andIonStoica.2025. WhyDo\nMulti-AgentLLMSystemsFail? arXiv:2503.13657[cs.AI] https://arxiv.org/abs/2503.13657\n[47] Eric Chamoun, Michael Schlichktrull, and Andreas Vlachos. 2024. Automated Focused Feedback Generation for\nScientificWritingAssistance. arXiv:2405.20477[cs.CL] https://arxiv.org/abs/2405.20477\n[48] Chi-MinChan,WeizeChen,YushengSu,JianxuanYu,WeiXue,ShanghangZhang,JieFu,andZhiyuanLiu.2023.\nChatEval:TowardsBetterLLM-basedEvaluatorsthroughMulti-AgentDebate. https://github.com/thunlp/ChatEval.\narXiv:2308.07201[cs.CL] https://arxiv.org/abs/2308.07201\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 83\n[49] JiangjieChen,XintaoWang,RuiXu,SiyuYuan,YikaiZhang,WeiShi,JianXie,ShuangLi,RuihanYang,Tinghui\nZhu,AiliChen,NianqiLi,LidaChen,CaiyuHu,SiyeWu,ScottRen,ZiquanFu,andYanghuaXiao.2024. From\nPersonatoPersonalization:ASurveyonRole-PlayingLanguageAgents. arXiv:2404.18231[cs.CL] https://arxiv.org/\nabs/2404.18231\n[50] KexinChen,HanqunCao,JunyouLi,YuyangDu,MenghaoGuo,XinZeng,LanqingLi,JiezhongQiu,PhengAnn\nHeng,andGuangyongChen.2024. AnAutonomousLargeLanguageModelAgentforChemicalLiteratureDataMining.\narXiv:2402.12993[cs.IR] https://arxiv.org/abs/2402.12993\n[51] PengchengChen,JinYe,GuoanWang,YanjunLi,ZhongyingDeng,WeiLi,TianbinLi,HaodongDuan,ZiyanHuang,\nYanzhouSu,BenyouWang,ShaotingZhang,BinFu,JianfeiCai,BohanZhuang,EricJSeibel,JunjunHe,andYuQiao.\n2024. GMAI-MMBench:AComprehensiveMultimodalEvaluationBenchmarkTowardsGeneralMedicalAI. https:\n//uni-medical.github.io/GMAI-MMBench.github.io/. arXiv:2408.03361[eess.IV] https://arxiv.org/abs/2408.03361\n[52] Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, and Dianbo Liu. 2025. Auto-\nBench: An Automated Benchmark for Scientific Discovery in LLMs. https://github.com/AutoBench/AutoBench.\narXiv:2502.15224[cs.LG] https://arxiv.org/abs/2502.15224\n[53] ValerieChen,AlanZhu,SebastianZhao,HusseinMozannar,DavidSontag,andAmeetTalwalkar.2025. NeedHelp?\nDesigningProactiveAIAssistantsforProgramming. arXiv:2410.04596[cs.HC] https://arxiv.org/abs/2410.04596\n[54] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin\nHung,ChenQian,YujiaQin,XinCong,RuobingXie,ZhiyuanLiu,MaosongSun,andJieZhou.2023. AgentVerse:\nFacilitatingMulti-AgentCollaborationandExploringEmergentBehaviors. arXiv:2308.10848[cs.CL] https://arxiv.\norg/abs/2308.10848\n[55] QinyuanCheng,TianxiangSun,XiangyangLiu,WenweiZhang,ZhangyueYin,ShiminLi,LinyangLi,ZhengfuHe,\nKai Chen, and Xipeng Qiu. 2024. Can AI Assistants Know What They Don’t Know? arXiv:2401.13275 [cs.CL]\nhttps://arxiv.org/abs/2401.13275\n[56] ZhaoCheng,DianeWan,MatthewAbueg,SahraGhalebikesabi,RenYi,EugeneBagdasarian,BorjaBalle,StefanMellem,\nandShawnO’Banion.2024. CI-Bench:BenchmarkingContextualIntegrityofAIAssistantsonSyntheticData. https:\n//www.aimodels.fyi/papers/arxiv/ci-bench-benchmarking-contextual-integrity-ai-assistants. arXiv:2409.13903[cs.AI]\nhttps://arxiv.org/abs/2409.13903\n[57] Bhavya Chopra, Ananya Singha, Anna Fariha, Sumit Gulwani, Chris Parnin, Ashish Tiwari, and Austin Z. Hen-\nley. 2023. Conversational Challenges in AI-Powered Data Science: Obstacles, Needs, and Design Opportunities.\narXiv:2310.16164[cs.HC] https://arxiv.org/abs/2310.16164\n[58] Daniel J. H. Chung, Zhiqi Gao, Yurii Kvasiuk, Tianyi Li, Moritz Münchmeyer, Maja Rudolph, Frederic Sala, and\nSaiChaitanyaTadepalli.2025. TheoreticalPhysicsBenchmark(TPBench)–aDatasetandStudyofAIReasoning\nCapabilitiesinTheoreticalPhysics. https://tpbench.org/. arXiv:2502.15815[cs.LG] https://arxiv.org/abs/2502.15815\n[59] UmutCihan,VahidHaratian,Ardaİçöz,MertKaanGül,ÖmercanDevran,EmircanFurkanBayendur,BaykalMehmet\nUçar,andErayTüzün.2024. AutomatedCodeReviewInPractice. arXiv:2412.18531[cs.SE] https://arxiv.org/abs/\n2412.18531\n[60] DaveCitron.2025. DeepResearchisnowavailableonGemini2.5ProExperimental. https://blog.google/products/\ngemini/deep-research-gemini-2-5-pro-experimental/.\n[61] Cline.2024. Cline. https://github.com/cline/cline.\n[62] CognitionLabs.2025. Devin.ai. https://devin.ai\n[63] Consensus.2025. Consensus. https://consensus.app/.\n[64] crewAIInc.2023. CrewAI. https://github.com/crewAIInc/crewAI.\n[65] Cursor.2023. Cursor. https://www.cursor.com/.\n[66] MohamadH.Danesh,TuTrinh,BenjaminPlaut,andNguyenX.Khanh.2025. LearningtoCoordinatewithExperts.\nhttps://github.com/modanesh/YRC-Bench. arXiv:2502.09583[cs.LG] https://arxiv.org/abs/2502.09583\n[67] KristinM.dePayrebrune,KathrinFlaßkamp,TomStröhla,ThomasSattel,DieterBestle,BenedictRöder,Peter\nEberhard,SebastianPeitz,MarcusStoffel,GulakalaRutwik,BorseAditya,MeikeWohlleben,WalterSextro,Maximilian\nRaff, C. David Remy, Manish Yadav, Merten Stender, Jan van Delden, Timo Lüddecke, Sabine C. Langer, Julius\nSchultz, and Christopher Blech. 2024. The impact of AI on engineering design procedures for dynamical systems.\narXiv:2412.12230[eess.SY] https://arxiv.org/abs/2412.12230\n[68] DeepSeek-AI,DayaGuo,DejianYang,HaoweiZhang,JunxiaoSong,RuoyuZhang,RunxinXu,QihaoZhu,Shirong\nMa,PeiyiWang,XiaoBi,XiaokangZhang,XingkaiYu,YuWu,etal.2025. DeepSeek-R1:IncentivizingReasoning\nCapabilityinLLMsviaReinforcementLearning. arXiv:2501.12948[cs.CL] https://arxiv.org/abs/2501.12948\n[69] Akash Dhruv and Anshu Dubey. 2025. Leveraging Large Language Models for Code Translation and Software\nDevelopmentinScientificComputing. arXiv:2410.24119[cs.SE] https://arxiv.org/abs/2410.24119\n84 Xu et al.\n[70] TalissaDreossi.2025. BridgingLogicProgrammingandDeepLearningforExplainabilitythroughILASP. Electronic\nProceedingsinTheoreticalComputerScience416(Feb.2025),314–323. doi:10.4204/eptcs.416.31\n[71] IanDrosos,AdvaitSarkar,XiaotongXu,andNeilToronto.2025. \"Itmakesyouthink\":ProvocationsHelpRestore\nCriticalThinkingtoAI-AssistedKnowledgeWork. arXiv:2501.17247[cs.HC] https://arxiv.org/abs/2501.17247\n[72] OmerDunay,DanielCheng,AdamTait,ParthThakkar,PeterCRigby,AndyChiu,ImadAhmad,ArunGanesan,\nChandraMaddila,VijayaraghavanMurali,AliTayyebi,andNachiappanNagappan.2024. Multi-lineAI-assistedCode\nAuthoring. arXiv:2402.04141[cs.SE] https://arxiv.org/abs/2402.04141\n[73] Steffen Eger, Yong Cao, Jennifer D’Souza, Andreas Geiger, Christian Greisinger, Stephanie Gross, Yufang Hou,\nBrigitteKrenn,AnneLauscher,YizhiLi,ChenghuaLin,NafiseSadatMoosavi,WeiZhao,andTristanMiller.2025.\nTransformingSciencewithLargeLanguageModels:ASurveyonAI-assistedScientificDiscovery,Experimentation,\nContentGeneration,andEvaluation. arXiv:2502.05151[cs.CL] https://arxiv.org/abs/2502.05151\n[74] Elicit.2025. Elicit. https://elicit.com/?redirected=true.\n[75] MichaelD.Ernst.2017. NaturalLanguageisaProgrammingLanguage:ApplyingNaturalLanguageProcessingto\nSoftwareDevelopment. https://drops.dagstuhl.de/storage/00lipics/lipics-vol071-snapl2017/LIPIcs.SNAPL.2017.4/\nLIPIcs.SNAPL.2017.4.pdf.\n[76] WenqiFan,YujuanDing,LiangboNing,ShijieWang,HengyunLi,DaweiYin,Tat-SengChua,andQingLi.2024.\nASurveyonRAGMeetingLLMs:TowardsRetrieval-AugmentedLargeLanguageModels. arXiv:2405.06211[cs.CL]\nhttps://arxiv.org/abs/2405.06211\n[77] Flowith.2024. FlowithOracleMode. https://flowith.net/.\n[78] Forethought-Technologies.2023. AutoChain. https://github.com/Forethought-Technologies/AutoChain.\n[79] CésarFrança.2023. AIempoweringresearch:10wayshowsciencecanbenefitfromAI. arXiv:2307.10265[cs.GL]\nhttps://arxiv.org/abs/2307.10265\n[80] Future-House.2023. PaperQA. https://github.com/Future-House/paper-qa.\n[81] GAIR-NLP.2025. DeepResearcher. https://github.com/GAIR-NLP/DeepResearcher.\n[82] DifeiGao,LeiJi,LuoweiZhou,KevinQinghongLin,JoyaChen,ZihanFan,andMikeZhengShou.2023. AssistGPT:\nA General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn. arXiv:2306.08640 [cs.CV] https:\n//arxiv.org/abs/2306.08640\n[83] Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz,\nYasha Ektefaie, Jovana Kondic, and Marinka Zitnik. 2024. Empowering Biomedical Discovery with AI Agents.\narXiv:2404.02831[cs.AI] https://arxiv.org/abs/2404.02831\n[84] AlirezaGhafarollahiandMarkusJ.Buehler.2024. SciAgents:Automatingscientificdiscoverythroughmulti-agent\nintelligentgraphreasoning. arXiv:2409.05556[cs.AI] https://arxiv.org/abs/2409.05556\n[85] LucaGioacchini,MarcoMellia,IdilioDrago,AlexanderDelsanto,GiuseppeSiracusano,andRobertoBifulco.2024.\nAutoPenBench:BenchmarkingGenerativeAgentsforPenetrationTesting. https://github.com/lucagioacchini/auto-\npen-bench. arXiv:2410.03225[cs.CR] https://arxiv.org/abs/2410.03225\n[86] Github.2021. GithubCopilot. https://github.com/features/copilot?ref=nav.poetries.top.\n[87] AmrGomaa,MichaelSargious,andAntonioKrüger.2024. AdaptoML-UX:AnAdaptiveUser-centeredGUI-based\nAutoML Toolkit for Non-AI Experts and HCI Researchers. https://github.com/MichaelSargious/AdaptoML_UX.\narXiv:2410.17469[cs.HC] https://arxiv.org/abs/2410.17469\n[88] Google.2021. BIG-bench. https://github.com/google/BIG-bench.\n[89] Google.2024. TryDeepResearchandournewexperimentalmodelinGemini,yourAIassistant. https://blog.google/\nproducts/gemini/google-gemini-deep-research/.\n[90] Google.2025. A2A. https://github.com/google/A2A.\n[91] Google.2025. AgentDevelopmentKit. https://google.github.io/adk-docs/.\n[92] Google.2025. AnnouncingtheAgent2AgentProtocol(A2A). https://developers.googleblog.com/en/a2a-a-new-era-of-\nagent-interoperability/.\n[93] Google.2025. Gemini2.0Flash(Feb’25):Intelligence,PerformanceandPriceAnalysis. https://artificialanalysis.ai/\nmodels/gemini-2-0-flash.\n[94] Google.2025. gemini-fullstack-langgraph-quickstart. https://github.com/google-gemini/gemini-fullstack-langgraph-\nquickstart.\n[95] Google.2025. NotebookLm. https://notebooklm.google/.\n[96] KanikaGoswami,PuneetMathur,RyanRossi,andFranckDernoncourt.2025. ChartCitor:Multi-AgentFramework\nforFine-GrainedChartVisualAttribution. arXiv:2502.00989[cs.CL] https://arxiv.org/abs/2502.00989\n[97] JurajGottweis,Wei-HungWeng,AlexanderDaryin,TaoTu,AnilPalepu,PetarSirkovic,ArtiomMyaskovsky,Felix\nWeissenberger,KeranRong,RyutaroTanno,KhaledSaab,DanPopovici,JacobBlum,FanZhang,KatherineChou,\nAvinatanHassidim,BurakGokturk,AminVahdat,PushmeetKohli,YossiMatias,AndrewCarroll,KavitaKulkarni,\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 85\nNenadTomasev,VikramDhillon,EeshitDhavalVaishnav,ByronLee,TiagoRDCosta,JoséRPenadés,GaryPeltz,\nYunhan Xu, Annalisa Pawlosky, Alan Karthikesalingam, and Vivek Natarajan. 2025. Towards an AI co-scientist.\nhttps://storage.googleapis.com/coscientist_paper/ai_coscientist.pdf.\n[98] AlexanderH.Gower,KonstantinKorovin,DanielBrunnsåker,FilipKronström,GabrielK.Reder,IevgeniiaA.Tiukova,\nRonaldS.Reiserer,JohnP.Wikswo,andRossD.King.2024. TheUseofAI-RoboticSystemsforScientificDiscovery.\narXiv:2406.17835[cs.LG] https://arxiv.org/abs/2406.17835\n[99] Tianyang Gu, Jingjin Wang, Zhihao Zhang, and HaoHong Li. 2025. LLMs can Realize Combinatorial Creativity:\nGeneratingCreativeIdeasviaLLMsforScientificResearch. arXiv:2412.14141[cs.AI] https://arxiv.org/abs/2412.14141\n[100] YuzheGu,WenweiZhang,ChengqiLyu,DahuaLin,andKaiChen.2025. Mask-DPO:GeneralizableFine-grained\nFactualityAlignmentofLLMs. arXiv:2503.02846[cs.CL] https://arxiv.org/abs/2503.02846\n[101] Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, and Bo Li. 2024. Red-\nCode:RiskyCodeExecutionandGenerationBenchmarkforCodeAgents. https://github.com/AI-secure/RedCode.\narXiv:2411.07781[cs.SE] https://arxiv.org/abs/2411.07781\n[102] SiyuanGuo,ChengDeng,YingWen,HechangChen,YiChang,andJunWang.2024. DS-Agent:AutomatedData\nSciencebyEmpoweringLargeLanguageModelswithCase-BasedReasoning. https://github.com/guosyjlu/DS-Agent.\narXiv:2402.17453[cs.LG] https://arxiv.org/abs/2402.17453\n[103] Xin Guo, Haotian Xia, Zhaowei Liu, Hanyang Cao, Zhi Yang, Zhiqiang Liu, Sizhe Wang, Jinyi Niu, Chuqi Wang,\nYanhui Wang, Xiaolong Liang, Xiaoming Huang, Bing Zhu, Zhongyu Wei, Yun Chen, Weining Shen, and Liwen\nZhang.2024. FinEval:AChineseFinancialDomainKnowledgeEvaluationBenchmarkforLargeLanguageModels.\narXiv:2308.09975[cs.CL] https://arxiv.org/abs/2308.09975\n[104] HildaHadan,DerrickWang,RezaHadiMogavi,JosephTu,LeahZhang-Kennedy,andLennartE.Nacke.2024. The\nGreat AI Witch Hunt: Reviewers Perception and (Mis)Conception of Generative AI in Research Writing. https:\n//arxiv.org/abs/2407.12015.\n[105] Sukjin Han. 2024. Mining Causality: AI-Assisted Search for Instrumental Variables. arXiv:2409.14202 [econ.EM]\nhttps://arxiv.org/abs/2409.14202\n[106] Ebtesam Al Haque, Chris Brown, Thomas D. LaToza, and Brittany Johnson. 2025. Towards Decoding Developer\nCognitionintheAgeofAIAssistants. arXiv:2501.02684[cs.HC] https://arxiv.org/abs/2501.02684\n[107] GaoleHe,PatrickHemmer,MichaelVössing,MaxSchemmer,andUjwalGadiraju.2025. Fine-GrainedAppropriate\nReliance:Human-AICollaborationwithaMulti-StepTransparentDecisionWorkflowforComplexTaskDecomposition.\narXiv:2501.10909[cs.AI] https://arxiv.org/abs/2501.10909\n[108] KaveenHiniduma,SurenByna,JeanLucaBez,andRaviMadduri.2024. AIDataReadinessInspector(AIDRIN)for\nQuantitativeAssessmentofDataReadinessforAI.InProceedingsofthe36thInternationalConferenceonScientific\nandStatisticalDatabaseManagement (SSDBM2024).ACM,1–12. doi:10.1145/3676288.3676296\n[109] HKUDS.2025. AI-Researcher. https://github.com/HKUDS/AI-Researcher.\n[110] BrendanHogan,AnmolKabra,FelipeSiqueiraPacheco,LauraGreenstreet,JoshuaFan,AaronFerber,MartaUmmus,\nAlecsanderBrito,OliviaGraham,LillianAoki,DrewHarvell,AlexFlecker,andCarlaGomes.2024. AiSciVision:A\nFrameworkforSpecializingLargeMultimodalModelsinScientificImageClassification. arXiv:2410.21480[cs.LG]\nhttps://arxiv.org/abs/2410.21480\n[111] SiruiHong,MingchenZhuge,JiaqiChen,XiawuZheng,YuhengCheng,CeyaoZhang,JinlinWang,ZiliWang,Steven\nKaShingYau,ZijuanLin,LiyangZhou,ChenyuRan,LingfengXiao,ChenglinWu,andJürgenSchmidhuber.2024.\nMetaGPT:MetaProgrammingforAMulti-AgentCollaborativeFramework. https://github.com/geekan/MetaGPT.\narXiv:2308.00352[cs.AI] https://arxiv.org/abs/2308.00352\n[112] Hong Kong University Data Science Lab. 2024. Auto-Deep-Research. https://github.com/HKUDS/Auto-Deep-\nResearch.\n[113] BettyLiHou,KejianShi,JasonPhang,JamesAung,StevenAdler,andRosieCampbell.2024. LargeLanguageModels\nasMisleadingAssistantsinConversation. arXiv:2407.11789[cs.CL] https://arxiv.org/abs/2407.11789\n[114] Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, and Hai-Tao Zheng. 2024.\nLatEval:AnInteractiveLLMsEvaluationBenchmarkwithIncompleteInformationfromLateralThinkingPuzzles.\nhttps://github.com/THUKElab/LatEval. arXiv:2308.10855[cs.CL] https://arxiv.org/abs/2308.10855\n[115] HuggingFace. 2025. smolagents: open_deep_research. https://github.com/huggingface/smolagents/tree/main/\nexamples/open_deep_research.\n[116] FariaHuq,AbdusSamee,DavidChuan-EnLin,AliceXiaodiTang,andJeffreyPBigham.2025. NoTeeline:Supporting\nReal-Time, Personalized Notetaking with LLM-Enhanced Micronotes. In Proceedings of the 30th International\nConferenceonIntelligentUserInterfaces(IUI’25).ACM,1064–1081. doi:10.1145/3708359.3712086\n[117] KurandoIIDAandKenjiroMIMURA.2024. CATER:LeveragingLLMtoPioneeraMultidimensional,Reference-\nIndependentParadigminTranslationQualityEvaluation. arXiv:2412.11261[cs.CL] https://arxiv.org/abs/2412.11261\n86 Xu et al.\n[118] SeyedMohammadAliJafari.2024. StreamliningtheSelectionPhaseofSystematicLiteratureReviews(SLRs)Using\nAI-EnabledGPT-4AssistantAPI. arXiv:2402.18582[cs.DL] https://arxiv.org/abs/2402.18582\n[119] RishabJainandAdityaJain.2023. GenerativeAIinWritingResearchPapers:ANewTypeofAlgorithmicBiasand\nUncertaintyinScholarlyWork. arXiv:2312.10057[cs.CY] https://arxiv.org/abs/2312.10057\n[120] ZhengyaoJiang,DominikSchmidt,DhruvSrikanth,DixingXu,IanKaplan,DenissJacenko,andYuxiangWu.2025.\nAIDE:AI-DrivenExplorationintheSpaceofCode. arXiv:2502.13138[cs.AI] https://arxiv.org/abs/2502.13138\n[121] JinaAI.2025. node-DeepResearch. https://github.com/jina-ai/node-DeepResearch.\n[122] Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du,\nand Dong Yu. 2025. DSBench: How Far Are Data Science Agents from Becoming Data Science Experts? https:\n//github.com/LiqiangJing/DSBench. arXiv:2409.07703[cs.AI] https://arxiv.org/abs/2409.07703\n[123] NicolaJones.2025. OpenAI’s‘deepresearch’tool:isitusefulforscientists? https://www.nature.com/articles/d41586-\n025-00377-9.\n[124] VijayJoshiandIverBand.2024. DisruptingTestDevelopmentwithAIAssistants:BuildingtheBaseoftheTest\nPyramidwithThreeAICodingAssistants. (Oct.2024). doi:10.36227/techrxiv.173014488.82191966/v1\n[125] MajeedKazemitabaar,JackWilliams,IanDrosos,ToviGrossman,AustinZacharyHenley,CarinaNegreanu,andAdvait\nSarkar.2024. ImprovingSteeringandVerificationinAI-AssistedDataAnalysiswithInteractiveTaskDecomposition.\nInProceedingsofthe37thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(UIST’24).ACM,\n1–19. doi:10.1145/3654777.3676345\n[126] CTOLEditorsKen.2025. GeminiLaunchesDeepResearchon2.5ProAimingtoRedefineAI-PoweredAnalysiswith\nStrongLeadOverOpenAI. https://www.ctol.digital/news/gemini-deep-research-launch-2-5-pro-vs-openai/.\n[127] AnttiKeurulainen,IsakWesterlund,SamuelKaski,andAlexanderIlin.2021. LearningtoAssistAgentsbyObserving\nThem. arXiv:2110.01311[cs.AI] https://arxiv.org/abs/2110.01311\n[128] Abdullah Khalili and Abdelhamid Bouchachia. 2022. Toward Building Science Discovery Machines.\narXiv:2103.15551[cs.AI] https://arxiv.org/abs/2103.15551\n[129] StefanKramer,MattiaCerrato,SašoDžeroski,andRossKing.2023. AutomatedScientificDiscovery:FromEquation\nDiscoverytoAutonomousDiscoverySystems. arXiv:2305.02251[cs.AI] https://arxiv.org/abs/2305.02251\n[130] IliaKuznetsov,OsamaMohammedAfzal,KoenDercksen,NilsDycke,AlexanderGoldberg,TomHope,DirkHovy,\nJonathanK.Kummerfeld,AnneLauscher,KevinLeyton-Brown,ShengLu,Mausam,MargotMieskes,AurélieNévéol,\nDanish Pruthi, Lizhen Qu, Roy Schwartz, Noah A. Smith, Thamar Solorio, Jingyan Wang, Xiaodan Zhu, Anna\nRogers,NiharB.Shah,andIrynaGurevych.2024. WhatCanNaturalLanguageProcessingDoforPeerReview?\narXiv:2405.06563[cs.CL] https://arxiv.org/abs/2405.06563\n[131] MartinLance.2024. open_deep_research. https://github.com/langchain-ai/open_deep_research.\n[132] HaoLang,FeiHuang,andYongbinLi.2025. DebateHelpsWeak-to-StrongGeneralization. arXiv:2501.13124[cs.CL]\nhttps://arxiv.org/abs/2501.13124\n[133] LangChain. 2025. How to think about agent frameworks. https://blog.langchain.dev/how-to-think-about-agent-\nframeworks/. https://docs.google.com/spreadsheets/d/1B37VxTBuGLeTSPVWtz7UMsCdtXrqV5hCjWkbHN8tfAo/\n[134] langChainAI.2024. LangGraph. https://github.com/langchain-ai/langgraph.\n[135] AndrewLaverick,KristenSurrao,InigoZubeldia,BorisBolliet,MilesCranmer,AntonyLewis,BlakeSherwin,and\nJulienLesgourgues.2024. Multi-AgentSystemforCosmologicalParameterAnalysis. arXiv:2412.00431[astro-ph.IM]\nhttps://arxiv.org/abs/2412.00431\n[136] EunhaeLee.2024.TowardsEthicalPersonalAIApplications:PracticalConsiderationsforAIAssistantswithLong-Term\nMemory. arXiv:2409.11192[cs.CY] https://arxiv.org/abs/2409.11192\n[137] Yuho Lee, Taewon Yun, Jason Cai, Hang Su, and Hwanjun Song. 2024. UniSumEval: Towards Unified, Fine-\nGrained,Multi-DimensionalSummarizationEvaluationforLLMs. https://github.com/DISL-Lab/UniSumEval-v1.0.\narXiv:2409.19898[cs.CL] https://arxiv.org/abs/2409.19898\n[138] Letta-AI.2023. Letta. https://github.com/letta-ai/letta.\n[139] KylaLevin,NicolasvanKempen,EmeryD.Berger,andStephenN.Freund.2025.ChatDBG:AnAI-PoweredDebugging\nAssistant. arXiv:2403.16354[cs.SE] https://arxiv.org/abs/2403.16354\n[140] James R. Lewis. 2018. The System Usability Scale: Past, Present, and Future. International\nJournal of Human–Computer Interaction 34, 7 (2018), 577–590. doi:10.1080/10447318.2018.1455307\narXiv:https://doi.org/10.1080/10447318.2018.1455307\n[141] BelindaZ.Li,BeenKim,andZiWang.2025. QuestBench:CanLLMsasktherightquestiontoacquireinformationin\nreasoningtasks? arXiv:2503.22674[cs.AI] https://arxiv.org/abs/2503.22674\n[142] GuohaoLi,HasanAbedAlKaderHammoud,HaniItani,DmitriiKhizbullin,andBernardGhanem.2023. CAMEL:\nCommunicativeAgentsfor\"Mind\"ExplorationofLargeLanguageModelSociety. arXiv:2303.17760[cs.AI] https:\n//arxiv.org/abs/2303.17760\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 87\n[143] JiachenLi,XiwenLi,JustinSteinberg,AkshatChoube,BingshengYao,XuhaiXu,DakuoWang,ElizabethMynatt,\nand Varun Mishra. 2025. Vital Insight: Assisting Experts’ Context-Driven Sensemaking of Multi-modal Personal\nTrackingDataUsingVisualizationandHuman-In-The-LoopLLMAgents. arXiv:2410.14879[cs.HC] https://arxiv.\norg/abs/2410.14879\n[144] XuefengLi,HaoyangZou,andPengfeiLiu.2025. TORL:ScalingTool-IntegratedRL. https://github.com/GAIR-\nNLP/ToRL. https://arxiv.org/pdf/2503.23383\n[145] YuanLi,YixuanZhang,andLichaoSun.2023.MetaAgents:SimulatingInteractionsofHumanBehaviorsforLLM-based\nTask-orientedCoordinationviaCollaborativeGenerativeAgents. arXiv:2310.06500[cs.AI] https://arxiv.org/abs/2310.\n06500\n[146] Zhuoyan Li, Chen Liang, Jing Peng, and Ming Yin. 2024. How Does the Disclosure of AI Assistance Affect the\nPerceptionsofWriting? arXiv:2410.04545[cs.CL] https://arxiv.org/abs/2410.04545\n[147] Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk Lim, Sungyoung Ji,\nByungjuLee,XifengYan,LindaRuthPetzold,StephenD.Wilson,WoosangLim,andWilliamYangWang.2025.\nMMSci:ADatasetforGraduate-LevelMulti-DisciplineMultimodalScientificUnderstanding. https://github.com/\nLeezekun/MMSci. arXiv:2407.04903[cs.CL] https://arxiv.org/abs/2407.04903\n[148] JennyT.Liang,ChenyangYang,andBradA.Myers.2023. ALarge-ScaleSurveyontheUsabilityofAIProgramming\nAssistants:SuccessesandChallenges. arXiv:2303.17125[cs.SE] https://arxiv.org/abs/2303.17125\n[149] PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,YianZhang,Deepak\nNarayanan,YuhuaiWu,AnanyaKumar,BenjaminNewman,BinhangYuan,BobbyYan,CeZhang,ChristianCosgrove,\nChristopherD.Manning,ChristopherRé,DianaAcosta-Navas,DrewA.Hudson,EricZelikman,EsinDurmus,Faisal\nLadhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert\nYuksekgonul,MiracSuzgun,NathanKim,NeelGuha,NiladriChatterji,OmarKhattab,PeterHenderson,QianHuang,\nRyanChi,SangMichaelXie,ShibaniSanturkar,SuryaGanguli,TatsunoriHashimoto,ThomasIcard,TianyiZhang,\nVishravChaudhary,WilliamWang,XuechenLi,YifanMai,YuhuiZhang,andYutaKoreeda.2023. HolisticEvaluation\nofLanguageModels. arXiv:2211.09110[cs.CL] https://arxiv.org/abs/2211.09110\n[150] JialiangLin,JiaxinSong,ZhangpingZhou,YidongChen,andXiaodongShi.2023. Automatedscholarlypaperreview:\nConcepts,technologies,andchallenges. InformationFusion98(Oct.2023),101830. doi:10.1016/j.inffus.2023.101830\n[151] StephanieLin,JacobHilton,andOwainEvans.2022. TruthfulQA:MeasuringHowModelsMimicHumanFalsehoods.\narXiv:2109.07958[cs.CL] https://arxiv.org/abs/2109.07958\n[152] ShilongLiu,HaoCheng,HaotianLiu,HaoZhang,FengLi,TianheRen,XueyanZou,JianweiYang,HangSu,Jun\nZhu,LeiZhang,JianfengGao,andChunyuanLi.2023. LLaVA-Plus:LearningtoUseToolsforCreatingMultimodal\nAgents. arXiv:2311.05437[cs.CV] https://arxiv.org/abs/2311.05437\n[153] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai\nSun,JiaqiWang,JunjieGao,JunjunShan,KangningLiu,ShudanZhang,ShuntianYao,SiyiCheng,WentaoYao,\nWenyi Zhao, Xinghan Liu, Xinyi Liu, Xinying Chen, Xinyue Yang, Yang Yang, Yifan Xu, Yu Yang, Yujia Wang,\nYulin Xu, Zehan Qi, Yuxiao Dong, and Jie Tang. 2024. AutoGLM: Autonomous Foundation Agents for GUIs.\narXiv:2411.00820[cs.HC] https://arxiv.org/abs/2411.00820\n[154] Xiao Liu, Bo Qin, Dongzhu Liang, Guang Dong, Hanyu Lai, Hanchen Zhang, Hanlin Zhao, Iat Long Iong, Jiadai\nSun,JiaqiWang,JunjieGao,JunjunShan,KangningLiu,ShudanZhang,ShuntianYao,SiyiCheng,WentaoYao,\nWenyi Zhao, Xinghan Liu, Xinyi Liu, Xinying Chen, Xinyue Yang, Yang Yang, Yifan Xu, Yu Yang, Yujia Wang,\nYulin Xu, Zehan Qi, Yuxiao Dong, and Jie Tang. 2024. AutoGLM: Autonomous Foundation Agents for GUIs.\narXiv:2411.00820[cs.HC] https://arxiv.org/abs/2411.00820\n[155] ZijunLiu,KaimingLiu,YiqiZhu,XuanyuLei,ZonghanYang,ZhenheZhang,PengLi,andYangLiu.2024. AIGS:\nGeneratingSciencefromAI-PoweredAutomatedFalsification. arXiv:2411.11910[cs.LG] https://arxiv.org/abs/2411.\n11910\n[156] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen,\nJuanCarlosNiebles,DevanshArpit,RanXu,PhilMui,HuanWang,CaimingXiong,andSilvioSavarese.2023.BOLAA:\nBenchmarking and Orchestrating LLM-augmented Autonomous Agents. https://github.com/salesforce/BOLAA.\narXiv:2308.05960[cs.AI] https://arxiv.org/abs/2308.05960\n[157] Renze Lou, Hanzi Xu, Sijia Wang, Jiangshu Du, Ryo Kamoi, Xiaoxin Lu, Jian Xie, Yuxuan Sun, Yusen Zhang,\nJihyunJaniceAhn,HongchaoFang,ZhuoyangZou,WenchaoMa,XiLi,KaiZhang,CongyingXia,LifuHuang,and\nWenpengYin.2025. AAAR-1.0:AssessingAI’sPotentialtoAssistResearch. https://renzelou.github.io/AAAR-1.0/.\narXiv:2410.22394[cs.CL] https://arxiv.org/abs/2410.22394\n[158] Cong Lu, Shengran Hu, and Jeff Clune. 2025. Automated Capability Discovery via Model Self-Exploration.\narXiv:2502.07577[cs.LG] https://arxiv.org/abs/2502.07577\n88 Xu et al.\n[159] ChrisLu,CongLu,RobertTjarkoLange,JakobFoerster,JeffClune,andDavidHa.2024. TheAIScientist:Towards\nFullyAutomatedOpen-EndedScientificDiscovery. arXiv:2408.06292[cs.AI] https://arxiv.org/abs/2408.06292\n[160] PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,PeterClark,and\nAshwinKalyan.2022. LearntoExplain:MultimodalReasoningviaThoughtChainsforScienceQuestionAnswering.\nhttps://scienceqa.github.io/. arXiv:2209.09513[cs.CL] https://arxiv.org/abs/2209.09513\n[161] ChandraMaddila,NegarGhorbani,KosayJabre,VijayaraghavanMurali,EdwinKim,ParthThakkar,NikolayPavlovich\nLaptev,OliviaHarman,DianaHsu,RuiAbreu,andPeterC.Rigby.2024. AI-AssistedSQLAuthoringatIndustry\nScale. arXiv:2407.13280[cs.SE] https://arxiv.org/abs/2407.13280\n[162] SrijoniMajumdar,EdithElkind,andEvangelosPournaras.2025. GenerativeAIVoting:FairCollectiveChoiceis\nResilienttoLLMBiasesandInconsistencies. arXiv:2406.11871[cs.AI] https://arxiv.org/abs/2406.11871\n[163] DungNguyenManh,ThangPhanChau,NamLeHai,ThongT.Doan,NamV.Nguyen,QuangPham,andNghiD.Q.Bui.\n2025.CodeMMLU:AMulti-TaskBenchmarkforAssessingCodeUnderstanding&ReasoningCapabilitiesofCodeLLMs.\nhttps://github.com/FSoft-AI4Code/CodeMMLU. arXiv:2410.01999[cs.SE] https://arxiv.org/abs/2410.01999\n[164] Manus.2025. Manus. https://manus.im/.\n[165] RohinManvi,SamarKhanna,GengchenMai,MarshallBurke,DavidLobell,andStefanoErmon.2024. GeoLLM:\nExtractingGeospatialKnowledgefromLargeLanguageModels. arXiv:2310.06213[cs.CL] https://arxiv.org/abs/2310.\n06213\n[166] DavidM.Markowitz.2024. FromComplexitytoClarity:HowAIEnhancesPerceptionsofScientistsandthePublic’s\nUnderstandingofScience. arXiv:2405.00706[cs.CL] https://arxiv.org/abs/2405.00706\n[167] Jonathan Mast. 2025. ChatGPT’s Deep Research vs. Google’s Gemini 1.5 Pro with Deep Research: A Detailed\nComparison. https://whitebeardstrategies.com/ai-prompt-engineering/chatgpts-deep-research-vs-googles-gemini-1-5-\npro-with-deep-research-a-detailed-comparison/.\n[168] Mastra-AI.2025. Mastra. https://github.com/mastra-ai/mastra.\n[169] ShrayMathur,NoahvanderVleuten,KevinYager,andEstherTsai.2024.VISION:AModularAIAssistantforNatural\nHuman-InstrumentInteractionatScientificUserFacilities. arXiv:2412.18161[cs.AI] https://arxiv.org/abs/2412.18161\n[170] Gianmarco Mengaldo. 2025. Explain the Black Box for the Sake of Science: the Scientific Method in the Era of\nGenerativeArtificialIntelligence. arXiv:2406.10557[cs.AI] https://arxiv.org/abs/2406.10557\n[171] MGXTechnologies.2025. MGX.dev. https://mgx.dev\n[172] GregoireMialon,ClementineFourrier,CraigSwift,ThomasWolf,YannLeCun,andThomasScialom.2023. GAIA:A\nBenchmarkforGeneralAIAssistants. https://huggingface.co/gaia-benchmark. https://arxiv.org/pdf/2311.12983\n[173] Microsoft.2023. MicrosoftCopilot. https://www.microsoft.com/en-us/microsoft-copilot/organizations.\n[174] Microsoft.2023. Semantic-kernel. https://github.com/microsoft/semantic-kernel.\n[175] mirayayerdem.2022.Github-Copilot-Amazon-Whisperer-ChatGPT.https://github.com/mirayayerdem/Github-Copilot-\nAmazon-Whisperer-ChatGPT.\n[176] Mlc-ai.2023. web-llm. https://github.com/mlc-ai/web-llm.\n[177] ModelTC.2025. lightllm. https://github.com/ModelTC/lightllm.\n[178] DevamMondalandAtharvaInamdar.2024. SeqMate:ANovelLargeLanguageModelPipelineforAutomatingRNA\nSequencing. arXiv:2407.03381[q-bio.GN] https://arxiv.org/abs/2407.03381\n[179] PeyaMowar,Yi-HaoPeng,JasonWu,AaronSteinfeld,andJeffreyP.Bigham.2025. CodeA11y:MakingAICoding\nAssistantsUsefulforAccessibleWebDevelopment. arXiv:2502.10884[cs.HC] https://arxiv.org/abs/2502.10884\n[180] Dilxat Muhtar, Zhenshi Li, Feng Gu, Xueliang Zhang, and Pengfeng Xiao. 2024. LHRS-Bot: Empowering Re-\nmoteSensingwithVGI-EnhancedLargeMultimodalLanguageModel. https://github.com/NJU-LHRS/LHRS-Bot.\narXiv:2402.02544[cs.CV] https://arxiv.org/abs/2402.02544\n[181] Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, and Tung Mai. 2025. From Documents to\nDialogue:BuildingKG-RAGEnhancedAIAssistants. arXiv:2502.15237[cs.IR] https://arxiv.org/abs/2502.15237\n[182] ShesheraMysore,MahmoodJasim,HaoruSong,SarahAkbar,AndreKennethChaseRandall,andNargesMahyar.\n2023. How Data Scientists Review the Scholarly Literature. In Proceedings of the 2023 Conference on Human\nInformationInteractionandRetrieval (CHIIR’23).ACM,137–152. doi:10.1145/3576840.3578309\n[183] n8n.2023. n8n. https://github.com/n8n-io/n8n.\n[184] NanobrowserTeam.2024. Nanobrowser. https://github.com/nanobrowser/nanobrowser.\n[185] NathaliaNascimento,EvertonGuimaraes,SaiSanjnaChintakunta,andSanthoshAnithaBoominathan.2024.LLM4DS:\nEvaluatingLargeLanguageModelsforDataScienceCodeGeneration. https://github.com/DataForScience/LLM4DS.\narXiv:2411.11908[cs.SE] https://arxiv.org/abs/2411.11908\n[186] KhanhNghiem,AnhMinhNguyen,andNghiD.Q.Bui.2024. EnvisioningtheNext-GenerationAICodingAssistants:\nInsights&Proposals. arXiv:2403.14592[cs.SE] https://arxiv.org/abs/2403.14592\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 89\n[187] AlexNguyen,ZilongWang,JingboShang,andDheerajMekala.2024.DOCMASTER:AUnifiedPlatformforAnnotation,\nTraining,&InferenceinDocumentQuestion-Answering. arXiv:2404.00439[cs.CL] https://arxiv.org/abs/2404.00439\n[188] KienX.Nguyen,FengchunQiao,ArthurTrembanis,andXiPeng.2024. SeafloorAI:ALarge-scaleVision-Language\nDatasetforSeafloorGeologicalSurvey. https://github.com/deep-real/SeafloorAI. arXiv:2411.00172[cs.CV] https:\n//arxiv.org/abs/2411.00172\n[189] Ziqi Ni, Yahao Li, Kaijia Hu, Kunyuan Han, Ming Xu, Xingyu Chen, Fengqi Liu, Yicong Ye, and Shuxin Bai.\n2024. MatPilot: an LLM-enabled AI Materials Scientist under the Framework of Human-Machine Collaboration.\narXiv:2411.08063[physics.soc-ph] https://arxiv.org/abs/2411.08063\n[190] Alexander Novikov, Ngân Vu˜, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey\nShirobokov,BorislavKozlovskii,FranciscoJ.R.Ruiz,AbbasMehrabian,M.PawanKumar,AbigailSee,SwaratChaud-\nhuri,GeorgeHolland,AlexDavies,SebastianNowozin,PushmeetKohli,andMatejBalog.2025.AlphaEvolve:Acoding\nagentforscientificandalgorithmicdiscovery. https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-\ncoding-agent-for-designing-advanced-algorithms/. https://storage.googleapis.com/deepmind-media/DeepMind.com/\nBlog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/AlphaEvolve.pdf\n[191] KojiOchiai,YuyaTahara-Arai,AkariKato,KazunariKaizu,HirokazuKariyazaki,MakotoUmeno,KoichiTakahashi,\nGenkiN.Kanda,andHarukaOzaki.2025. AutomatingCarebySelf-maintainabilityforFullLaboratoryAutomation.\narXiv:2501.05789[q-bio.QM] https://arxiv.org/abs/2501.05789\n[192] Ollama.2023. Ollama. https://github.com/ollama/ollama.\n[193] OpenManusTeam.2025. OpenManus. https://github.com/mannaandpoem/OpenManus.\n[194] OpenAI.2025. codex. https://github.com/openai/codex.\n[195] OpenAI.2025. Comparemodels-OpenAIAPI. https://platform.openai.com/docs/models/compare?model=o3.\n[196] OpenAI.2025. DeepResearchSystemCard. https://cdn.openai.com/deep-research-system-card.pdf.\n[197] OpenAI.2025. IntroducingDeepResearch. https://openai.com/index/introducing-deep-research/.\n[198] OpenAI.2025. IntroducingOpenAIo3ando4-mini. https://openai.com/index/introducing-o3-and-o4-mini/.\n[199] OpenAI.2025. OpenAIAgentsSDK. https://github.com/openai/openai-agents-python.\n[200] OpenAI. 2025. OpenAI o3 and o4-mini System Card. https://cdn.openai.com/pdf/2221c875-02dc-4789-800b-\ne7758f3722c1/o3-and-o4-mini-system-card.pdf.\n[201] OpenAI.2025. Thinkingwithimages. https://openai.com/index/thinking-with-images/.\n[202] OpenBMB.2023. XAgent. https://github.com/OpenBMB/XAgent.\n[203] Orkes.2022. Orkes. https://orkes.io/use-cases/agentic-workflows.\n[204] Takauki Osogami. 2025. Position: AI agents should be regulated based on autonomous action sequences.\narXiv:2503.04750[cs.CY] https://arxiv.org/abs/2503.04750\n[205] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,ChongZhang,Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,\nAmandaAskell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe.2022. Traininglanguagemodelstofollow\ninstructionswithhumanfeedback. arXiv:2203.02155[cs.CL] https://arxiv.org/abs/2203.02155\n[206] MdSultanulIslamOvi,NafisaAnjum,TasminaHaqueBithe,Md.MahabuburRahman,andMst.ShahnajAkterSmrity.\n2024. BenchmarkingChatGPT,Codeium,andGitHubCopilot:AComparativeStudyofAI-DrivenProgrammingand\nDebuggingAssistants. arXiv:2409.19922[cs.SE] https://arxiv.org/abs/2409.19922\n[207] Carlos Alves Pereira, Tanay Komarlu, and Wael Mobeirek. 2023. The Future of AI-Assisted Writing.\narXiv:2306.16641[cs.HC] https://arxiv.org/abs/2306.16641\n[208] MikePerkinsandJasperRoe.2024. GenerativeAIToolsinAcademicResearch:ApplicationsandImplicationsfor\nQualitativeandQuantitativeResearchMethodologies. arXiv:2408.06872[cs.HC] https://arxiv.org/abs/2408.06872\n[209] Perplexity.2025. IntroducingPerplexityDeepResearch. https://www.perplexity.ai/hub/blog/introducing-perplexity-\ndeep-research.\n[210] Perplexity.2025. SonarbyPerplexity. https://docs.perplexity.ai/guides/model-cards#research-models.\n[211] TomasPetricek,GerritJ.J.vandenBurg,AlfredoNazábal,TahaCeritli,ErnestoJiménez-Ruiz,andChristopher\nK.I.Williams.2022. AIAssistants:AFrameworkforSemi-AutomatedDataWrangling. arXiv:2211.00192[cs.DB]\nhttps://arxiv.org/abs/2211.00192\n[212] LongPhan,AliceGatti,ZiwenHan,NathanielLi,JosephinaHu,HughZhang,ChenBoCalvinZhang,Mohamed\nShaaban,JohnLing,SeanShi,etal.2025. Humanity’sLastExam. arXiv:2501.14249[cs.LG] https://arxiv.org/abs/\n2501.14249\n[213] EvangelosPournaras.2023. ScienceintheEraofChatGPT,LargeLanguageModelsandGenerativeAI:Challenges\nforResearchEthicsandHowtoRespond. arXiv:2305.15299[cs.CY] https://arxiv.org/abs/2305.15299\n[214] RonakPradeep,NandanThakur,SahelSharifymoghaddam,EricZhang,RyanNguyen,DanielCampos,NickCraswell,\nandJimmyLin.2024. Ragnarök:AReusableRAGFrameworkandBaselinesforTREC2024Retrieval-Augmented\n90 Xu et al.\nGenerationTrack. arXiv:2406.16828[cs.IR] https://arxiv.org/abs/2406.16828\n[215] JamesPrather,JuhoLeinonen,NatalieKiesler,JamieGorsonBenario,SamLau,StephenMacNeil,NargesNorouzi,\nSimoneOpel,VeePettit,LeoPorter,BrentN.Reeves,JaromirSavelka,DavidH.SmithIV,SvenStrickroth,and\nDaniel Zingaro. 2024. Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research,\nTeachingPractices,andTools. arXiv:2412.14732[cs.CY] https://arxiv.org/abs/2412.14732\n[216] Pydantic.2024. Pydantic-AI. https://github.com/pydantic/pydantic-ai.\n[217] Pythagora-io.2024. gpt-pilot. https://github.com/Pythagora-io/gpt-pilot.\n[218] JingyuanQi,ZianJia,MinqianLiu,WangzhiZhan,JunkaiZhang,XiaofeiWen,JingruGan,JianpengChen,QinLiu,\nMingyuDerekMa,BangzhengLi,HaohuiWang,AdithyaKulkarni,MuhaoChen,DaweiZhou,LingLi,WeiWang,\nandLifuHuang.2024. MetaScientist:AHuman-AISynergisticFrameworkforAutomatedMechanicalMetamaterial\nDesign. arXiv:2412.16270[cs.AI] https://arxiv.org/abs/2412.16270\n[219] Laryn Qi, J. D. Zamfirescu-Pereira, Taehan Kim, Björn Hartmann, John DeNero, and Narges Norouzi. 2024. A\nKnowledge-Component-BasedMethodologyforEvaluatingAIAssistants. arXiv:2406.05603[cs.CY] https://arxiv.org/\nabs/2406.05603\n[220] ChenQian,WeiLiu,HongzhangLiu,NuoChen,YufanDang,JiahaoLi,ChengYang,WeizeChen,YushengSu,Xin\nCong,JuyuanXu,DahaiLi,ZhiyuanLiu,andMaosongSun.2024. ChatDev:CommunicativeAgentsforSoftware\nDevelopment. https://github.com/OpenBMB/ChatDev. https://aclanthology.org/2024.acl-long.810.pdf\n[221] Shuofei Qiao, Runnan Fang, Zhisong Qiu, Xiaobin Wang, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang,\nand Huajun Chen. 2025. Benchmarking Agentic Workflow Generation. https://github.com/zjunlp/WorfBench.\narXiv:2410.07869[cs.CL] https://arxiv.org/abs/2410.07869\n[222] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,XiangruTang,BillQian,\nSihanZhao,LaurenHong,RunchuTian,RuobingXie,JieZhou,MarkGerstein,DahaiLi,ZhiyuanLiu,andMaosong\nSun.2023. ToolLLM:FacilitatingLargeLanguageModelstoMaster16000+Real-worldAPIs. arXiv:2307.16789[cs.AI]\nhttps://arxiv.org/abs/2307.16789\n[223] YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,XiangruTang,BillQian,\nSihanZhao,LaurenHong,RunchuTian,RuobingXie,JieZhou,MarkGerstein,DahaiLi,ZhiyuanLiu,andMaosong\nSun.2023. ToolLLM:FacilitatingLargeLanguageModelstoMaster16000+Real-worldAPIs. arXiv:2307.16789[cs.AI]\nhttps://arxiv.org/abs/2307.16789\n[224] QwenLM.2024. Qwen-Agent. https://github.com/QwenLM/Qwen-Agent.\n[225] Joaquin Ramirez-Medina, Mohammadmehdi Ataei, and Alidad Amirfazli. 2025. Accelerating Scientific Research\nThroughaMulti-LLMFramework. arXiv:2502.07960[physics.app-ph] https://arxiv.org/abs/2502.07960\n[226] RuchitRawal,Victor-AlexandruPădurean,SvenApel,AdishSingla,andMariyaToneva.2024. HintsHelpFinding\nandFixingBugsDifferentlyinPythonandText-basedProgramRepresentations. arXiv:2412.12471[cs.SE] https:\n//arxiv.org/abs/2412.12471\n[227] RuntaoRen,JianMa,andJianxiLuo.2025.Largelanguagemodelforpatentconceptgeneration.AdvancedEngineering\nInformatics65(May2025),103301. doi:10.1016/j.aei.2025.103301\n[228] ResearchRabbit.2025. ResearchRabbit. https://www.researchrabbit.ai/.\n[229] Restate.2024. Restate. https://restate.dev/.\n[230] reworkd.2023. AgentGPT. https://github.com/reworkd/AgentGPT.\n[231] FilippoRicca,AlessandroMarchetto,andAndreaStocco.2025. AMulti-YearGreyLiteratureReviewonAI-assisted\nTestAutomation. https://arxiv.org/pdf/2408.06224.\n[232] NathalieRiche,AnnaOffenwanger,FredericGmeiner,DavidBrown,HugoRomat,MichelPahud,NicolaiMarquardt,\nKoriInkpen,andKenHinckley.2025. AI-Instruments:EmbodyingPromptsasInstrumentstoAbstract&Reflect\nGraphicalInterfaceCommandsasGeneral-PurposeTools. https://arxiv.org/abs/2502.18736.\n[233] AnthonyCintronRoman,JenniferWortmanVaughan,ValerieSee,StephBallard,JehuTorres,CalebRobinson,and\nJuanM.LavistaFerres.2024. OpenDatasheets:Machine-readableDocumentationforOpenDatasetsandResponsible\nAIAssessments. arXiv:2312.06153[cs.LG] https://arxiv.org/abs/2312.06153\n[234] KaushikRoy,VedantKhandelwal,HarshulSurana,ValerieVera,AmitSheth,andHeatherHeckman.2023. GEAR-Up:\nGenerativeAIandExternalKnowledge-basedRetrievalUpgradingScholarlyArticleSearchesforSystematicReviews.\narXiv:2312.09948[cs.IR] https://arxiv.org/abs/2312.09948\n[235] Run-llama.2023. LlamaIndex. https://github.com/run-llama/llama_index.\n[236] SergeyVSamsonau,AzizaKurbonova,LuJiang,HazemLashen,JiamuBai,TheresaMerchant,RuoxiWang,Laiba\nMehnaz, Zecheng Wang, and Ishita Patil. 2024. Artificial Intelligence for Scientific Research: Authentic Research\nEducationFramework. arXiv:2210.08966[cs.CY] https://arxiv.org/abs/2210.08966\n[237] SamuelSchmidgall.2025. AgentLaboratory. https://github.com/SamuelSchmidgall/AgentLaboratory.\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 91\n[238] ThomasSandholm,SarahDong,SayandevMukherjee,JohnFeland,andBernardoA.Huberman.2024. Semantic\nNavigationforAI-assistedIdeation. arXiv:2411.03575[cs.HC] https://arxiv.org/abs/2411.03575\n[239] LindsaySannemanandJulieShah.2021. ExplainingRewardFunctionstoHumansforBetterHuman-RobotCollabo-\nration. arXiv:2110.04192[cs.RO] https://arxiv.org/abs/2110.04192\n[240] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and\nEmad Barsoum. 2025. Agent Laboratory: Using LLM Agents as Research Assistants. arXiv:2501.04227 [cs.HC]\nhttps://arxiv.org/abs/2501.04227\n[241] MartijnJ.Schuemie,M.SoledadCepeda,MarcA.Suchard,JianxiaoYang,YuxiTian,AlejandroSchuler,PatrickB.\nRyan,DavidMadigan,andGeorgeHripcsak.2020.HowConfidentAreWeAboutObservationalFindingsinHealthcare:\nABenchmarkStudy. HarvardDataScienceReview2,1(2020). doi:10.1162/99608f92.147cc28e\n[242] Scispace.2024. Scispace. https://scispace.com/.\n[243] Scite.2025. Scite. https://scite.ai/.\n[244] AgniaSergeyuk,YaroslavGolubev,TimofeyBryksin,andIftekharAhmed.2025. UsingAI-basedcodingassistants\ninpractice:Stateofaffairs,perceptions,andwaysforward. InformationandSoftwareTechnology178(Feb.2025),\n107610. doi:10.1016/j.infsof.2024.107610\n[245] Mahsa Shamsabadi and Jennifer D’Souza. 2024. A FAIR and Free Prompt-based Research Assistant.\narXiv:2405.14601[cs.CL] https://arxiv.org/abs/2405.14601\n[246] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT:\nSolvingAITaskswithChatGPTanditsFriendsinHuggingFace. https://github.com/microsoft/JARVIS. https:\n//arxiv.org/pdf/2303.17580\n[247] ZejiangShen,TalAugust,PaoSiangliulue,KyleLo,JonathanBragg,JeffHammerbacher,DougDowney,JosephChee\nChang,andDavidSontag.2023. BeyondSummarization:DesigningAISupportforReal-WorldExpositoryWriting\nTasks. arXiv:2304.02623[cs.CL] https://arxiv.org/abs/2304.02623\n[248] ShumingShi,EnboZhao,DuyuTang,YanWang,PijiLi,WeiBi,HaiyunJiang,GuopingHuang,LeyangCui,Xinting\nHuang,CongZhou,YongDai,andDongyangMa.2022. Effidit:YourAIWritingAssistant. arXiv:2208.01815[cs.CL]\nhttps://arxiv.org/abs/2208.01815\n[249] MichaelShumer.2025. OpenDeepResearcher. https://github.com/mshumer/OpenDeepResearcher.\n[250] Significant-Gravitas.2023. AutoGPT. https://github.com/Significant-Gravitas/AutoGPT.\n[251] DavidSilverandRichardSutton.2025. WelcometotheEraofExperience. https://storage.googleapis.com/deepmind-\nmedia/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf.\n[252] AusteSimkute,EwaLuger,MichaelEvans,andRhianneJones.2024. \"Itisthere,andyouneedit,sowhydoyounot\nuseit?\"AchievingbetteradoptionofAIsystemsbydomainexperts,inthecasestudyofnaturalscienceresearch.\narXiv:2403.16895[cs.HC] https://arxiv.org/abs/2403.16895\n[253] MichaelSkarlinski,TylerNadolski,JamesBraza,RemoStorni,MaykCaldas,LudovicoMitchener,MichaelaHinks,\nAndrewWhite,andSamRodriques.2025. FutureHousePlatform:SuperintelligentAIAgentsforScientificDiscovery.\nhttps://www.futurehouse.org/research-announcements/launching-futurehouse-platform-ai-agents.\n[254] XinyiSong,KexinXie,LinaLee,RuizheChen,JaredM.Clark,HaoHe,HaoranHe,JieMin,XinleiZhang,Simin\nZheng,ZhiyangZhang,XinweiDeng,andYiliHong.2025. PerformanceEvaluationofLargeLanguageModelsin\nStatisticalProgramming. arXiv:2502.13117[stat.AP] https://arxiv.org/abs/2502.13117\n[255] Jamshid Sourati and James Evans. 2021. Accelerating science with human versus alien artificial intelligences.\narXiv:2104.05188[cs.AI] https://arxiv.org/abs/2104.05188\n[256] Jamshid Sourati and James Evans. 2023. Accelerating science with human-aware artificial intelligence.\narXiv:2306.01495[cs.AI] https://arxiv.org/abs/2306.01495\n[257] StanfordNLP.2024. DSPy. https://github.com/stanfordnlp/dspy.\n[258] HaoyangSu,RenqiChen,ShixiangTang,ZhenfeiYin,XinzheZheng,JinzheLi,BiqingQi,QiWu,HuiLi,Wanli\nOuyang,PhilipTorr,BowenZhou,andNanqingDong.2025. ManyHeadsAreBetterThanOne:ImprovedScientific\nIdeaGenerationbyALLM-BasedMulti-AgentSystem. arXiv:2410.09403[cs.AI] https://arxiv.org/abs/2410.09403\n[259] Ltd.SuzhouYulingArtificialIntelligenceTechnologyCo.2023. Dify:Open-sourceLLMApplicationDevelopment\nPlatform. https://dify.ai/.\n[260] XinTan,XiaoLong,XianjunNi,YinghaoZhu,JingJiang,andLiZhang.2024. HowfarareAI-poweredprogramming\nassistantsfrommeetingdevelopers’needs? arXiv:2404.12000[cs.SE] https://arxiv.org/abs/2404.12000\n[261] Brian Tang and Kang G. Shin. 2024. Steward: Natural Language Web Automation. arXiv:2409.15441 [cs.AI]\nhttps://arxiv.org/abs/2409.15441\n[262] JiabinTang,TianyuFan,andChaoHuang.2025. AutoAgent:AFully-AutomatedandZero-CodeFrameworkforLLM\nAgents. arXiv:2502.05957[cs.AI] https://arxiv.org/abs/2502.05957\n[263] YanTang.2025. deep_research_agent. https://github.com/grapeot/deep_research_agent.\n92 Xu et al.\n[264] TadahiroTaniguchi,ShiroTakagi,JunOtsuka,YusukeHayashi,andHiroTaiyoHamada.2024. CollectivePredictive\nCodingasModelofScience:FormalizingScientificActivitiesTowardsGenerativeScience.arXiv:2409.00102[physics.soc-\nph] https://arxiv.org/abs/2409.00102\n[265] Temporalio.2020. Temporal. https://github.com/temporalio/temporal.\n[266] EnkeledaThaqi,MohamedOmarMantawy,andEnkelejdaKasneci.2024. SARA:SmartAIReadingAssistantfor\nReadingComprehension.InProceedingsofthe2024SymposiumonEyeTrackingResearchandApplications(ETRA\n’24).ACM,1–3. doi:10.1145/3649902.3655661\n[267] TheBlewish. 2024. Automated-AI-Web-Researcher-Ollama. https://github.com/TheBlewish/Automated-AI-Web-\nResearcher-Ollama.\n[268] MinyangTian,LuyuGao,ShizhuoDylanZhang,XinanChen,CunweiFan,XuefeiGuo,RolandHaas,PanJi,Kittithat\nKrongchon,YaoLi,ShengyanLiu,DiLuo,YutaoMa,HaoTong,KhaTrinh,ChenyuTian,ZihanWang,BohaoWu,\nYanyuXiong,ShengzhuYin,MinhuiZhu,KilianLieret,YanxinLu,GenglinLiu,YufengDu,TianhuaTao,OfirPress,\nJamie Callan, Eliu Huerta, and Hao Peng. 2024. SciCode: A Research Coding Benchmark Curated by Scientists.\nhttps://scicode-bench.github.io/. arXiv:2407.13168[cs.AI] https://arxiv.org/abs/2407.13168\n[269] IevgeniiaA.Tiukova,DanielBrunnsåker,ErikY.Bjurström,AlexanderH.Gower,FilipKronström,GabrielK.Reder,\nRonald S. Reiserer, Konstantin Korovin, Larisa B. Soldatova, John P. Wikswo, and Ross D. King. 2024. Genesis:\nTowardstheAutomationofSystemsBiologyResearch. arXiv:2408.10689[cs.AI] https://arxiv.org/abs/2408.10689\n[270] IrinaTolstykh,AleksandraTsybina,SergeyYakubson,AleksandrGordeev,VladimirDokholyan,andMaksimKuprashe-\nvich.2024. GigaCheck:DetectingLLM-generatedContent. arXiv:2410.23728[cs.CL] https://arxiv.org/abs/2410.23728\n[271] Benjamin Towle and Ke Zhou. 2024. Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback.\narXiv:2410.11009[cs.CL] https://arxiv.org/abs/2410.11009\n[272] Thanh-DatTruong,Hoang-QuanNguyen,Xuan-BacNguyen,AshleyDowling,XinLi,andKhoaLuu.2025. Insect-\nFoundation:AFoundationModelandLargeMultimodalDatasetforVision-LanguageInsectUnderstanding. https:\n//uark-cviu.github.io/projects/insect-foundation/. arXiv:2502.09906[cs.CV] https://arxiv.org/abs/2502.09906\n[273] JosephTu,HildaHadan,DerrickM.Wang,SabrinaASgandurra,RezaHadiMogavi,andLennartE.Nacke.2024.\nAugmentingtheAuthor:ExploringthePotentialofAICollaborationinAcademicWriting. arXiv:2404.16071[cs.HC]\nhttps://arxiv.org/abs/2404.16071\n[274] XinmingTu,JamesZou,WeijieJ.Su,andLinjunZhang.2023. WhatShouldDataScienceEducationDowithLarge\nLanguageModels? arXiv:2307.02792[cs.CY] https://arxiv.org/abs/2307.02792\n[275] MicheleTufano,AnishaAgarwal,JinuJang,RoshanakZilouchianMoghaddam,andNeelSundaresan.2024. AutoDev:\nAutomatedAI-DrivenDevelopment. arXiv:2403.08299[cs.SE] https://arxiv.org/abs/2403.08299\n[276] Aleksei Turobov, Diane Coyle, and Verity Harding. 2024. Using ChatGPT for Thematic Analysis.\narXiv:2405.08828[cs.HC] https://arxiv.org/abs/2405.08828\n[277] RasmusUlfsnes,NilsBredeMoe,ViktoriaStray,andMarianneSkarpen.2024. TransformingSoftwareDevelopment\nwithGenerativeAI:EmpiricalInsightsonCollaborationandWorkflow. arXiv:2405.01543[cs.SE] https://arxiv.org/\nabs/2405.01543\n[278] StanfordUniversity.2025. STORM. https://storm.genie.stanford.edu/.\n[279] Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate E. Jones, Oisin Mac Aodha, Sara\nBeery,andGrantVanHorn.2024. INQUIRE:ANaturalWorldText-to-ImageRetrievalBenchmark. https://inquire-\nbenchmark.github.io/. arXiv:2411.02537[cs.CV] https://arxiv.org/abs/2411.02537\n[280] Vercel.2020. Vercel. https://vercel.com/.\n[281] Vllm-project.2023. vllm. https://github.com/vllm-project/vllm.\n[282] ThiemoWambsganss,XiaotianSu,VinitraSwamy,SeyedParsaNeshaei,RomanRietsche,andTanjaKäser.2023.\nUnravelingDownstreamGenderBiasfromLargeLanguageModels:AStudyonAIEducationalWritingAssistance.\narXiv:2311.03311[cs.CL] https://arxiv.org/abs/2311.03311\n[283] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D. Weisz, Xuye Liu, Lingfei\nWu, and Casey Dugan. 2022. Documentation Matters: Human-Centered AI System to Assist Data Science Code\nDocumentationinComputationalNotebooks. ACMTransactionsonComputer-HumanInteraction29,2(Jan.2022),\n1–33. doi:10.1145/3489465\n[284] DakuoWang,Q.VeraLiao,YunfengZhang,UdayanKhurana,HorstSamulowitz,SoyaPark,MichaelMuller,andLisa\nAmini.2021. HowMuchAutomationDoesaDataScientistWant? arXiv:2101.03970[cs.LG] https://arxiv.org/abs/\n2101.03970\n[285] SuyuanWang,XueqianYin,MenghaoWang,RuofengGuo,andKaiNan.2024. EvoPat:AMulti-LLM-basedPatents\nSummarizationandAnalysisAgent. arXiv:2412.18100[cs.DL] https://arxiv.org/abs/2412.18100\n[286] TiannanWang,JiaminChen,QingruiJia,ShuaiWang,RuoyuFang,HuilinWang,ZhaoweiGao,ChunzhaoXie,Chuou\nXu,JihongDai,YibinLiu,JialongWu,ShengweiDing,LongLi,ZhiweiHuang,XinleDeng,TengYu,GanganMa,Han\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 93\nXiao,ZixinChen,DanjunXiang,YunxiaWang,YuanyuanZhu,YiXiao,JingWang,YiruWang,SiranDing,Jiayang\nHuang,JiayiXu,YilihamuTayier,ZhenyuHu,YuanGao,ChengfengZheng,YueshuYe,YihangLi,LeiWan,Xinyue\nJiang,YujieWang,SiyuCheng,ZhuleSong,XiangruTang,XiaohuaXu,NingyuZhang,HuajunChen,YuchenEleanor\nJiang,andWangchunshuZhou.2024. Weaver:FoundationModelsforCreativeWriting. arXiv:2401.17268[cs.CL]\nhttps://arxiv.org/abs/2401.17268\n[287] XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdhery,andDenny\nZhou.2023. Self-ConsistencyImprovesChainofThoughtReasoninginLanguageModels. arXiv:2203.11171[cs.CL]\nhttps://arxiv.org/abs/2203.11171\n[288] YaoWang,MingxuanCui,andArthurJiang.2025.EnablingAIScientiststoRecognizeInnovation:ADomain-Agnostic\nAlgorithmforAssessingNovelty. arXiv:2503.01508[cs.AI] https://arxiv.org/abs/2503.01508\n[289] Ying-MeiWangandTzeng-JChen.2025. AI’sdeepresearchrevolution:Transformingbiomedicalliteratureanalysis.\nhttps://journals.lww.com/jcma/citation/9900/ai_s_deep_research_revolution__transforming.508.aspx.\n[290] JasonWei,NguyenKarina,HyungWonChung,YunxinJoyJiao,SpencerPapay,AmeliaGlaese,JohnSchulman,\nandWilliamFedus.2024. Measuringshort-formfactualityinlargelanguagemodels. https://cdn.openai.com/papers/\nsimpleqa.pdf.\n[291] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,BrianIchter,FeiXia,EdChi,QuocLe,andDenny\nZhou.2023. Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguageModels. arXiv:2201.11903[cs.CL]\nhttps://arxiv.org/abs/2201.11903\n[292] ShufaWei,XiaolongXu,XianbiaoQi,XiYin,JunXia,JingyiRen,PeijunTang,YuxiangZhong,YihaoChen,Xiaoqin\nRen, Yuxin Liang, Liankai Huang, Kai Xie, Weikang Gui, Wei Tan, Shuanglong Sun, Yongquan Hu, Qinxian Liu,\nNanjin Li, Chihao Dai, Lihua Wang, Xiaohui Liu, Lei Zhang, and Yutao Xie. 2023. AcademicGPT: Empowering\nAcademicResearch. arXiv:2311.12315[cs.CL] https://arxiv.org/abs/2311.12315\n[293] SarahWelsh.2025. AIBenchmarkDeepDive:Gemini2.5andHumanity’sLastExam. https://arize.com/blog/ai-\nbenchmark-deep-dive-gemini-humanitys-last-exam/.\n[294] Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. 2025.\nCycleResearcher:ImprovingAutomatedResearchviaAutomatedReview. arXiv:2411.00816[cs.CL] https://arxiv.org/\nabs/2411.00816\n[295] ManFaiWong,ShangxinGuo,ChingNamHang,SiuWaiHo,andCheeWeiTan.2023. NaturalLanguageGeneration\nandUnderstandingofBigCodeforAI-AssistedProgramming:AReview. Entropy 25,6(June2023),888. doi:10.\n3390/e25060888\n[296] JunchaoWu,ShuYang,RunzheZhan,YulinYuan,DerekF.Wong,andLidiaS.Chao.2024.ASurveyonLLM-Generated\nTextDetection:Necessity,Methods,andFutureDirections.arXiv:2310.14724[cs.CL] https://arxiv.org/abs/2310.14724\n[297] JundeWu,JiayuanZhu,andYuyuanLiu.2025. AgenticReasoning:ReasoningLLMswithToolsfortheDeepResearch.\narXiv:2502.04644[cs.AI] https://arxiv.org/abs/2502.04644\n[298] QingyunWu,GaganBansal,JieyuZhang,YiranWu,BeibinLi,ErkangZhu,LiJiang,XiaoyunZhang,ShaokunZhang,\nJialeLiu,AhmedHassanAwadallah,RyenWWhite,DougBurger,andChiWang.2023. AutoGen:EnablingNext-Gen\nLLMApplicationsviaMulti-AgentConversation. https://github.com/microsoft/autogen. arXiv:2308.08155[cs.AI]\nhttps://arxiv.org/abs/2308.08155\n[299] xAI.2025. Grok3Beta-Theageofreasoningagents. https://x.ai/news/grok-3.\n[300] MenglinXia,VictorRuehle,SaravanRajmohan,andRezaShokri.2025.Minerva:AProgrammableMemoryTestBench-\nmarkforLanguageModels. https://github.com/gkamradt/LLMTest_NeedleInAHaystack. arXiv:2502.03358[cs.CL]\nhttps://arxiv.org/abs/2502.03358\n[301] TianbaoXie,FanZhou,ZhoujunCheng,PengShi,LuoxuanWeng,YitaoLiu,TohJingHua,JunningZhao,QianLiu,\nCheLiu,LeoZ.Liu,YihengXu,HongjinSu,DongchanShin,CaimingXiong,andTaoYu.2023. OpenAgents:An\nOpenPlatformforLanguageAgentsintheWild. arXiv:2310.10634[cs.CL] https://arxiv.org/abs/2310.10634\n[302] YujiaXie,XunWang,Si-QingChen,WayneXiong,andPengchengHe.2023.InteractiveEditingforTextSummarization.\narXiv:2306.03067[cs.CL] https://arxiv.org/abs/2306.03067\n[303] FengXiong,XinguoYu,andHonWaiLeong.2024. AI-EmpoweredHumanResearchIntegratingBrainScienceand\nSocialSciencesInsights. arXiv:2411.12761[cs.HC] https://arxiv.org/abs/2411.12761\n[304] FrankF.Xu,YufanSong,BoxuanLi,YuxuanTang,KritanjaliJain,MengxueBao,ZoraZ.Wang,XuhuiZhou,Zhitong\nGuo,MurongCao,MingyangYang,HaoYangLu,AmaadMartin,ZheSu,LeanderMaben,RajMehta,WayneChi,\nLawrenceJang,YiqingXie,ShuyanZhou,andGrahamNeubig.2024. TheAgentCompany:BenchmarkingLLMAgents\nonConsequentialRealWorldTasks. arXiv:2412.14161[cs.CL] https://arxiv.org/abs/2412.14161\n[305] XinXu,QiyunXu,TongXiao,TianhaoChen,YuchenYan,JiaxinZhang,ShizheDiao,CanYang,andYangWang.\n2025. UGPhysics:AComprehensiveBenchmarkforUndergraduatePhysicsReasoningwithLargeLanguageModels.\nhttps://github.com/YangLabHKUST/UGPhysics. arXiv:2502.00334[cs.CL] https://arxiv.org/abs/2502.00334\n94 Xu et al.\n[306] Te-LunYang,Jyi-ShaneLiu,Yuen-HsienTseng,andJyh-ShingRogerJang.2025. KnowledgeRetrievalBasedon\nGenerativeAI. arXiv:2501.04635[cs.IR] https://arxiv.org/abs/2501.04635\n[307] ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamW.Cohen,RuslanSalakhutdinov,andChristopherD.\nManning.2018.HotpotQA:ADatasetforDiverse,ExplainableMulti-hopQuestionAnswering.arXiv:1809.09600[cs.CL]\nhttps://arxiv.org/abs/1809.09600\n[308] YiYao,JunWang,YabaiHu,LifengWang,YiZhou,JackChen,XumingGai,ZhenmingWang,andWenjunLiu.\n2024. BugBlitz-AI:AnIntelligentQAAssistant. arXiv:2406.04356[cs.SE] https://arxiv.org/abs/2406.04356\n[309] Burak Yetiştiren, Işık Özsoy, Miray Ayerdem, and Eray Tüzün. 2023. Evaluating the Code Quality of AI-\nAssistedCodeGenerationTools:AnEmpiricalStudyonGitHubCopilot,AmazonCodeWhisperer,andChatGPT.\narXiv:2304.10778[cs.SE] https://arxiv.org/abs/2304.10778\n[310] Xiaoxin Yin. 2024. \"Turing Tests\" For An AI Scientist. https://github.com/MatthewFilipovich/pycharge.\narXiv:2405.13352[cs.AI] https://arxiv.org/abs/2405.13352\n[311] yoheinakajima.2024. BabyAGI. https://github.com/yoheinakajima/babyagi.\n[312] Seungri Yoon, Woosang Jeon, Sanghyeok Choi, Taehyeong Kim, and Tae In Ahn. 2025. Knowledge Synthesis of\nPhotosynthesisResearchUsingaLargeLanguageModel. arXiv:2502.01059[cs.CL] https://arxiv.org/abs/2502.01059\n[313] You.com.2023. You.com. https://you.com/about.\n[314] HengjieYuandYaochuJin.2025. UnlockingthePotentialofAIResearchersinScientificDiscovery:WhatIsMissing?\narXiv:2503.05822[cs.CY] https://arxiv.org/abs/2503.05822\n[315] HengjieYuandYaochuJin.2025. UnlockingthePotentialofAIResearchersinScientificDiscovery:WhatIsMissing?\narXiv:2503.05822[cs.CY] https://arxiv.org/abs/2503.05822\n[316] JiakangYuan,XiangchaoYan,ShiyangFeng,BoZhang,TaoChen,BotianShi,WanliOuyang,YuQiao,LeiBai,and\nBowenZhou.2025. Dolphin:MovingTowardsClosed-loopAuto-researchthroughThinking,Practice,andFeedback.\narXiv:2501.03916[cs.AI] https://arxiv.org/abs/2501.03916\n[317] SiyuYuan,ChengJiayang,LinQiu,andDeqingYang.2024. BoostingScientificConceptsUnderstanding:CanAnalogy\nfromTeacherModelsEmpowerStudentModels? arXiv:2406.11375[cs.CL] https://arxiv.org/abs/2406.11375\n[318] Hector Zenil, Jesper Tegnér, Felipe S. Abrahão, Alexander Lavin, Vipin Kumar, Jeremy G. Frey, Adrian Weller,\nLarisaSoldatova,AlanR.Bundy,NicholasR.Jennings,KoichiTakahashi,LawrenceHunter,SasoDzeroski,Andrew\nBriggs,FrederickD.Gregory,CarlaP.Gomes,JonRowe,JamesEvans,HiroakiKitano,andRossKing.2023. The\nFuture of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. arXiv:2307.07522 [cs.AI]\nhttps://arxiv.org/abs/2307.07522\n[319] BeiqZhang,PengLiang,XiyuZhou,AakashAhmad,andMuhammadWaseem.2023.PracticesandChallengesofUsing\nGitHubCopilot:AnEmpiricalStudy.InProceedingsofthe35thInternationalConferenceonSoftwareEngineering\nandKnowledgeEngineering(SEKE2023,Vol.2023).KSIResearchInc.,124–129. doi:10.18293/seke2023-077\n[320] CedegaoE.Zhang,KatherineM.Collins,AdrianWeller,andJoshuaB.Tenenbaum.2023. AIforMathematics:A\nCognitiveSciencePerspective. arXiv:2310.13021[q-bio.NC] https://arxiv.org/abs/2310.13021\n[321] DavidZhang.2025. deep-research. https://github.com/dzhng/deep-research.\n[322] KevinZhangandHodLipson.2024. AligningAI-drivendiscoverywithhumanintuition. arXiv:2410.07397[cs.LG]\nhttps://arxiv.org/abs/2410.07397\n[323] XingjianZhang,YutongXie,JinHuang,JingeMa,ZhaoyingPan,QijiaLiu,ZiyangXiong,TolgaErgen,Dongsub\nShim,HonglakLee,andQiaozhuMei.2024. MASSW:ANewDatasetandBenchmarkTasksforAI-AssistedScientific\nWorkflows. https://github.com/xingjian-zhang/massw. arXiv:2406.06357[cs.CL] https://arxiv.org/abs/2406.06357\n[324] ZhengZhang,JieGao,RanjodhSinghDhaliwal,andTobyJia-JunLi.2023. VISAR:AHuman-AIArgumentative\nWritingAssistantwithVisualProgrammingandRapidDraftPrototyping.InProceedingsofthe36thAnnualACM\nSymposiumonUserInterfaceSoftwareandTechnology(UIST’23).ACM,1–30. doi:10.1145/3586183.3606800\n[325] Jinjin Zhao, Avidgor Gal, and Sanjay Krishnan. 2024. A System for Quantifying Data Science Workflows with\nFine-GrainedProceduralLoggingandaPilotStudy. arXiv:2405.17845[cs.HC] https://arxiv.org/abs/2405.17845\n[326] Zihan Zhao, Bo Chen, Jingpiao Li, Lu Chen, Liyang Wen, Pengyu Wang, Zichen Zhu, Danyang Zhang, Yansi\nLi, Zhongyang Dai, Xin Chen, and Kai Yu. 2024. ChemDFM-X: towards large multimodal model for chemistry.\nhttps://github.com/OpenDFM/ChemDFM-X. ScienceChinaInformationSciences67,12(Dec.2024). doi:10.1007/\ns11432-024-4243-0\n[327] RaigulZheldibayeva.2025. TheimpactofAIandpeerfeedbackonresearchwritingskills:astudyusingtheCGScholar\nplatformamongKazakhstanischolars. arXiv:2503.05820[cs.CY] https://arxiv.org/abs/2503.05820\n[328] DewuZheng,YanlinWang,EnshengShi,XilinLiu,YuchiMa,HongyuZhang,andZibinZheng.2025. TopGeneral\nPerformance=TopDomainPerformance?DomainCodeBench:AMulti-domainCodeGenerationBenchmark. https:\n//github.com/DeepSoftwareAnalytics/MultiCodeBench. arXiv:2412.18573[cs.SE] https://arxiv.org/abs/2412.18573\nA Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications 95\n[329] YuxiangZheng,DayuanFu,XiangkunHu,XiaojieCai,LyumanshanYe,PengruiLu,andPengfeiLiu.2025. DeepRe-\nsearcher:ScalingDeepResearchviaReinforcementLearninginReal-worldEnvironments. arXiv:2504.03160[cs.AI]\nhttps://arxiv.org/abs/2504.03160\n[330] ZhipuAI.2025. AutoGLM-Research. https://autoglm-research.zhipuai.cn/.\n[331] WanjunZhong,RuixiangCui,YiduoGuo,YaoboLiang,ShuaiLu,YanlinWang,AminSaied,WeizhuChen,andNan\nDuan.2023. AGIEval:AHuman-CentricBenchmarkforEvaluatingFoundationModels. arXiv:2304.06364[cs.CL]\nhttps://arxiv.org/abs/2304.06364\n[332] ShuyanZhou,ChengrunChi,CeyaoZheng,BailinZhang,YonatanBisk,DanielFried,IshanMisra,KarthikRaghunathan,\nTongshuangZhao,BaianZhou,etal.2024. WebArena:ABenchmarkforWebAgents. https://github.com/web-arena-\nx/webarena.\n[333] MinjunZhu,YixuanWeng,LinyiYang,andYueZhang.2025. DeepReview:ImprovingLLM-basedPaperReviewwith\nHuman-likeDeepThinkingProcess. arXiv:2503.08569[cs.CL] https://arxiv.org/abs/2503.08569\n[334] Tonghe Zhuang and Zhicheng Lin. 2024. The why, what, and how of AI-based coding in scientific research.\narXiv:2410.02156[cs.CY] https://arxiv.org/abs/2410.02156\n[335] DennisZyska,NilsDycke,JanBuchmann,IliaKuznetsov,andIrynaGurevych.2023.CARE:CollaborativeAI-Assisted\nReadingEnvironment. arXiv:2302.12611[cs.CL] https://arxiv.org/abs/2302.12611\n[336] Tolga Çöplü, Arto Bendiken, Andrii Skomorokhov, Eduard Bateiko, Stephen Cobb, and Joshua J. Bouw. 2024.\nPrompt-TimeSymbolicKnowledgeCapturewithLargeLanguageModels. https://github.com/HaltiaAI/paper-PTSKC.\narXiv:2402.00414[cs.CL] https://arxiv.org/abs/2402.00414",
  "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
  "description": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications RENJUN XU∗ and JINGWEN PENG, Zhejiang University, China",
  "links": []
}