import asyncio
import re
import json
import logging
from datetime import datetime
from typing import List, Any, Dict, Optional, Type
from pydantic import BaseModel, Field
from introlix.agents.baseclass import Tool, AgentInput, PromptTemplate, BaseAgent
from introlix.agents.explorer_agent import ExplorerAgent


class VerifyReportAgentOutput(BaseModel):
    rejected_sections: List[str] = Field(
        description="List of sections that were rejected with explanations"
    )


class ExplorerToolInput(BaseModel):
    """Input schema for explorer agent tool"""
    queries: List[str] = Field(description="List of queries to verify")
    user_id: str = Field(description="User ID for tracking")
    max_results: int = Field(default=5, description="Maximum number of results per query")


INSTRUCTIONS = f"""
You are the Verify Report Agent. Your task is to review the research output generated by the Researcher Agent
and identify any sections that contain unverified, inaccurate, or misleading information. You have access to
an explorer agent tool to verify facts.

Today's date is {datetime.now().strftime("%Y-%m-%d")}

WORKFLOW:
1. Carefully read through the research output and extract facts and statements that need verification.
2. Use the 'explorer_agent' tool to verify filtered facts and statements.
3. Based on the verification results, identify sections with unverified, inaccurate, or misleading information.
4. Provide rejected sections with brief explanations of why they were rejected.

CRITICAL OUTPUT FORMAT RULES:
- You MUST respond with ONLY valid JSON, no explanatory text before or after
- Do NOT wrap JSON in markdown code blocks (no ```json```)
- Do NOT include any conversational text
- Do NOT include any text outside the JSON object

For tool calls, respond with EXACTLY this JSON structure (nothing else):
{{
    "type": "tool",
    "name": "explorer_agent",
    "input": {{
        "queries": ["fact to verify 1", "fact to verify 2"],
        "user_id": "USER_ID_PLACEHOLDER",
        "max_results": 5
    }}
}}

For final answers, respond with EXACTLY this JSON structure (nothing else):
{{
    "type": "final",
    "answer": {{
        "rejected_sections": [
            "Section X: Reason for rejection based on verification",
            "Section Y: Reason for rejection based on verification"
        ]
    }}
}}

REMEMBER:
- NO text outside the JSON structure
- NO markdown formatting
- NO explanations or commentary
- ONLY pure JSON output
- Always verify claims using the tool before making final decisions
- Use USER_ID_PLACEHOLDER as the user_id value (it will be replaced automatically)
"""


class ToolEnabledAgent(BaseAgent):
    """Enhanced Agent that can handle tool calls properly"""
    
    def __init__(self, model, instruction, output_model_class, config: AgentInput, max_iterations: int = 5):
        super().__init__(config=config, model=model, max_iterations=max_iterations)
        self.logger = logging.getLogger(__name__)
        self.row_instruction = instruction
        self.output_model_class = output_model_class

    async def _call_llm(self, prompt: str, cloud: bool = True) -> str:
        """Override to add better error handling"""
        try:
            result = await super()._call_llm(prompt, cloud)
            
            # Check if result is an error dict
            if isinstance(result, dict) and 'error' in result:
                error_msg = result.get('error', {}).get('message', 'Unknown API error')
                self.logger.error(f"LLM API Error: {error_msg}")
                raise ValueError(f"LLM API Error: {error_msg}")
            
            return result
        except Exception as e:
            self.logger.error(f"LLM call failed: {e}")
            raise

    def _build_prompt(self, user_prompt: str, state: Dict[str, Any]) -> PromptTemplate:
        """Build context-specific prompt with tool results if available."""
        
        # Include tool results in the prompt if any
        tool_context = ""
        if state.get("tool_results"):
            tool_context = "\n\n=== PREVIOUS TOOL RESULTS ===\n"
            for tool_name, result in state["tool_results"].items():
                tool_context += f"\nTool: {tool_name}\n"
                tool_context += f"Result: {json.dumps(result, indent=2)}\n"
            tool_context += "\nUse these results to make your final decision.\n"
        
        instruction = f"{self.row_instruction}\n{tool_context}"
        
        return PromptTemplate(user_prompt=user_prompt, system_prompt=instruction)

    async def _parse_output(self, raw_output: Any) -> Any:
        """
        Parse LLM output handling:
        1. Tool calls with {"type": "tool", ...}
        2. Final answers with {"type": "final", "answer": {...}}
        3. Direct output model format
        """
        try:
            # Handle case where raw_output is already a dict (API error or direct response)
            if isinstance(raw_output, dict):
                # Check if it's an API error
                if 'error' in raw_output:
                    error_msg = raw_output.get('error', {}).get('message', 'Unknown API error')
                    self.logger.error(f"API Error: {error_msg}")
                    raise ValueError(f"API Error: {error_msg}")
                
                # If it's already a dict, treat it as parsed JSON
                parsed_json = raw_output
                
                # Skip to step 3
                response_type = parsed_json.get("type", "unknown")
                
                if response_type in ["tool", "agent"]:
                    self.logger.info(f"Detected {response_type} call")
                    return parsed_json
                
                if response_type == "final":
                    self.logger.info("Detected final answer")
                    answer_data = parsed_json.get("answer", parsed_json)
                    if isinstance(answer_data, dict):
                        return self.output_model_class(**answer_data)
                    return answer_data
                
                # No type field - assume it's direct output model data
                self.logger.info("No type field, treating as direct output")
                return self.output_model_class(**parsed_json)
            
            # Handle string output
            if not isinstance(raw_output, str):
                self.logger.error(f"Unexpected raw_output type: {type(raw_output)}")
                raise ValueError(f"Expected string or dict, got {type(raw_output)}")
            
            # Step 1: Clean the output
            cleaned = raw_output.strip()
            
            # Remove any thinking tags
            if '<think>' in cleaned:
                cleaned = re.sub(r'<think>.*?</think>', '', cleaned, flags=re.DOTALL).strip()
            
            # Remove markdown code blocks
            if '```' in cleaned:
                # Extract content between ```json and ``` or just ``` and ```
                match = re.search(r'```(?:json)?\s*\n(.*?)\n```', cleaned, re.DOTALL)
                if match:
                    cleaned = match.group(1).strip()
                else:
                    # Try to remove any ``` markers
                    cleaned = re.sub(r'```(?:json)?', '', cleaned).strip()
            
            # Remove any leading/trailing text that's not JSON
            # Find first { and last }
            first_brace = cleaned.find('{')
            last_brace = cleaned.rfind('}')
            
            if first_brace != -1 and last_brace != -1:
                cleaned = cleaned[first_brace:last_brace+1]
            
            self.logger.info(f"Cleaned output: {cleaned[:200]}...")
            
            # Step 2: Try to parse as JSON
            try:
                parsed_json = json.loads(cleaned)
            except json.JSONDecodeError as e:
                self.logger.error(f"JSON decode error: {e}")
                self.logger.error(f"Attempted to parse: {cleaned[:500]}...")
                raise ValueError(f"Invalid JSON: {e}")
            
            # Step 3: Handle different response types
            if isinstance(parsed_json, dict):
                response_type = parsed_json.get("type", "unknown")
                
                # Tool or agent calls - return as-is for _decide_action to handle
                if response_type in ["tool", "agent"]:
                    self.logger.info(f"Detected {response_type} call")
                    return parsed_json
                
                # Final answer - extract and validate
                if response_type == "final":
                    self.logger.info("Detected final answer")
                    answer_data = parsed_json.get("answer", parsed_json)
                    
                    # If answer is a dict, validate it with output_model_class
                    if isinstance(answer_data, dict):
                        return self.output_model_class(**answer_data)
                    return answer_data
                
                # No type field - assume it's direct output model data
                self.logger.info("No type field, treating as direct output")
                return self.output_model_class(**parsed_json)
            
            # Shouldn't reach here, but handle it
            self.logger.warning("Unexpected parsed_json type")
            return parsed_json
            
        except Exception as e:
            self.logger.error(f"Failed to parse output: {e}")
            if isinstance(raw_output, str):
                self.logger.error(f"Raw output: {raw_output[:500]}...")
            else:
                self.logger.error(f"Raw output: {str(raw_output)[:500]}...")
            raise

    def _decide_action(self, parsed_output: Any) -> Dict[str, Any]:
        """Decide what action to take based on parsed output."""
        
        # If it's already a dict with type (tool/agent/final), return it
        if isinstance(parsed_output, dict):
            if "type" in parsed_output:
                return parsed_output
            else:
                # No type means it's final answer data
                return {"type": "final", "answer": parsed_output}
        
        # If it's our expected output model, wrap it as final
        if isinstance(parsed_output, self.output_model_class):
            return {"type": "final", "answer": parsed_output}
        
        # Fallback - treat as final answer
        self.logger.warning(f"Fallback: treating {type(parsed_output)} as final answer")
        return {"type": "final", "answer": parsed_output}


class VerifyReportAgent:
    def __init__(self):
        self.INSTRUCTIONS = INSTRUCTIONS
        self.logger = logging.getLogger(__name__)
        
        # Define the explorer agent tool
        self.explorer_tool = Tool(
            name="explorer_agent",
            description=(
                "Verifies facts and statements by searching and retrieving information. "
                "Takes a list of queries, user_id, and max_results. Returns verification results."
            ),
            function=self._run_explorer,
            input_schema=ExplorerToolInput.model_json_schema()
        )
        
        # Configure agent with the tool
        self.agent_config = AgentInput(
            name="Verify Report Agent",
            description="Reviews research outputs to identify and flag unverified or inaccurate information.",
            tools=[self.explorer_tool],
            output_type=VerifyReportAgentOutput
        )

        # Initialize the agent with loop support for tool usage
        self.verify_report_agent = ToolEnabledAgent(
            model="meta-llama/llama-3.3-70b-instruct:free",
            instruction=self.INSTRUCTIONS,
            output_model_class=VerifyReportAgentOutput,
            config=self.agent_config,
            max_iterations=5  # Allow multiple iterations for tool usage
        )
    
    async def _run_explorer(self, queries: List[str], user_id: str, max_results: int = 5) -> dict:
        """
        Wrapper function to run the explorer agent.
        This is called by the Tool when the LLM decides to use it.
        """
        try:
            self.logger.info(f"Running explorer agent with {len(queries)} queries for user {user_id}")
            
            # Create a new ExplorerAgent instance with the required parameters
            explorer_agent = ExplorerAgent(
                queries=queries,
                user_id=user_id,
                get_answer=True,
                get_multiple_answer=True,
                max_results=max_results
            )
            
            # Run the explorer agent
            results = await explorer_agent.run()
            
            self.logger.info(f"Explorer agent completed successfully")
            
            # Format results for the agent to understand
            return {
                "status": "success",
                "queries_verified": len(queries),
                "results": results,
                "summary": f"Verified {len(queries)} queries with {max_results} max results each"
            }
        except Exception as e:
            self.logger.error(f"Explorer agent error: {e}")
            return {
                "status": "error",
                "error": str(e),
                "queries": queries
            }
    
    async def verify_report(
        self, 
        enriched_prompt: str, 
        research_output: str,
        user_id: str
    ) -> VerifyReportAgentOutput:
        """
        Verify the research report by checking facts and identifying rejected sections.
        
        Args:
            enriched_prompt: The original enriched prompt with research objectives
            research_output: The research output to verify
            user_id: User ID for tracking explorer queries
        
        Returns:
            VerifyReportAgentOutput with rejected sections
        """
        # Replace USER_ID_PLACEHOLDER in instructions
        self.verify_report_agent.row_instruction = self.INSTRUCTIONS.replace(
            "USER_ID_PLACEHOLDER", user_id
        )
        
        # Prepare the input
        user_prompt = f"""
ENRICHED PROMPT:
{enriched_prompt}

RESEARCH OUTPUT TO VERIFY:
{research_output}

Please verify the research output and identify any sections that should be rejected.
Use the explorer_agent tool to verify factual claims before making decisions.
Remember to output ONLY valid JSON with no extra text.
        """
        
        try:
            # Use run_loop to allow tool execution
            response = await self.verify_report_agent.run_loop(user_prompt)
            
            # Return the result
            if isinstance(response.result, VerifyReportAgentOutput):
                return response.result
            elif isinstance(response.result, dict):
                # Check if it's a valid output
                if 'rejected_sections' in response.result:
                    return VerifyReportAgentOutput(**response.result)
                else:
                    self.logger.warning(f"Result missing rejected_sections: {response.result}")
                    return VerifyReportAgentOutput(rejected_sections=[])
            else:
                self.logger.warning(f"Unexpected result type: {type(response.result)}")
                return VerifyReportAgentOutput(rejected_sections=[])
                
        except ValueError as e:
            # Handle API errors
            self.logger.error(f"API or validation error: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Verification failed: {e}")
            # Return empty result instead of crashing
            return VerifyReportAgentOutput(rejected_sections=[])


# Example usage
async def main():
    # Set up logging
    logging.basicConfig(level=logging.INFO)
    
    agent = VerifyReportAgent()
    
    enriched_prompt = "Research the impact of AI on healthcare in 2024"
    research_output = """
    Section 1: AI has improved diagnostic accuracy by 95% in 2024.
    Section 2: The global AI healthcare market reached $500 billion.
    Section 3: All hospitals now use AI-powered systems.
    """
    user_id = "user123"
    
    result = await agent.verify_report(enriched_prompt, research_output, user_id)
    print(f"\nRejected sections: {result.rejected_sections}")


if __name__ == "__main__":
    asyncio.run(main())

# Todo: Make this work